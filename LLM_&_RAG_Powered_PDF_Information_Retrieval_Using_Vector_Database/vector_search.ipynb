{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Databases Leveraged LLM & RAG powered Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import OpenAI,HuggingFaceHub,HuggingFaceEndpoint,Ollama\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings,HuggingFaceEmbeddings,OllamaEmbeddings\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# from langchain_core.vectorstores import\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader,PyPDFLoader\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os,numpy as np, pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1). Loading & splitting the Docs \n",
    " - **NOTE: Combined both loading & splitting using PyPDFLoader & RecursiveCharacterTextSplitter to break it down into chunks of desired Size & overlaps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_doc(path,chunk_size1=10000,chunk_overlap1=50):\n",
    "    load_file=PyPDFLoader(path)\n",
    "    \"\"\"combined both load & split in one function using load_and_split(where it contains optional parameters to explicitly set chunk size & overlap using RecursiveCharacterTextSplitter )\"\"\"\n",
    "    docs_in_chunks_fmt=load_file.load_and_split(RecursiveCharacterTextSplitter(chunk_size=chunk_size1,chunk_overlap=chunk_overlap1))\n",
    "    return docs_in_chunks_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 3 0 (offset 0)\n",
      "Ignoring wrong pointing object 5 0 (offset 0)\n",
      "Ignoring wrong pointing object 279 0 (offset 0)\n",
      "Ignoring wrong pointing object 388 0 (offset 0)\n",
      "Ignoring wrong pointing object 541 0 (offset 0)\n",
      "Ignoring wrong pointing object 808 0 (offset 0)\n",
      "Ignoring wrong pointing object 968 0 (offset 0)\n",
      "Ignoring wrong pointing object 1126 0 (offset 0)\n",
      "Ignoring wrong pointing object 1175 0 (offset 0)\n",
      "Ignoring wrong pointing object 1176 0 (offset 0)\n",
      "Ignoring wrong pointing object 1177 0 (offset 0)\n",
      "Ignoring wrong pointing object 1178 0 (offset 0)\n",
      "Ignoring wrong pointing object 1208 0 (offset 0)\n",
      "Ignoring wrong pointing object 1218 0 (offset 0)\n",
      "Ignoring wrong pointing object 1219 0 (offset 0)\n",
      "Ignoring wrong pointing object 1220 0 (offset 0)\n",
      "Ignoring wrong pointing object 1221 0 (offset 0)\n",
      "Ignoring wrong pointing object 1222 0 (offset 0)\n",
      "Ignoring wrong pointing object 1223 0 (offset 0)\n",
      "Ignoring wrong pointing object 1224 0 (offset 0)\n",
      "Ignoring wrong pointing object 1225 0 (offset 0)\n",
      "Ignoring wrong pointing object 1226 0 (offset 0)\n",
      "Ignoring wrong pointing object 1227 0 (offset 0)\n",
      "Ignoring wrong pointing object 1228 0 (offset 0)\n",
      "Ignoring wrong pointing object 1229 0 (offset 0)\n",
      "Ignoring wrong pointing object 1230 0 (offset 0)\n",
      "Ignoring wrong pointing object 1231 0 (offset 0)\n",
      "Ignoring wrong pointing object 1232 0 (offset 0)\n",
      "Ignoring wrong pointing object 1233 0 (offset 0)\n",
      "Ignoring wrong pointing object 1234 0 (offset 0)\n",
      "Ignoring wrong pointing object 1235 0 (offset 0)\n",
      "Ignoring wrong pointing object 1236 0 (offset 0)\n",
      "Ignoring wrong pointing object 1237 0 (offset 0)\n",
      "Ignoring wrong pointing object 1238 0 (offset 0)\n",
      "Ignoring wrong pointing object 1239 0 (offset 0)\n",
      "Ignoring wrong pointing object 1240 0 (offset 0)\n",
      "Ignoring wrong pointing object 1241 0 (offset 0)\n",
      "Ignoring wrong pointing object 1242 0 (offset 0)\n",
      "Ignoring wrong pointing object 1243 0 (offset 0)\n",
      "Ignoring wrong pointing object 1244 0 (offset 0)\n",
      "Ignoring wrong pointing object 1245 0 (offset 0)\n",
      "Ignoring wrong pointing object 1246 0 (offset 0)\n",
      "Ignoring wrong pointing object 1247 0 (offset 0)\n",
      "Ignoring wrong pointing object 1248 0 (offset 0)\n",
      "Ignoring wrong pointing object 1249 0 (offset 0)\n",
      "Ignoring wrong pointing object 1250 0 (offset 0)\n",
      "Ignoring wrong pointing object 1251 0 (offset 0)\n",
      "Ignoring wrong pointing object 1252 0 (offset 0)\n",
      "Ignoring wrong pointing object 1253 0 (offset 0)\n",
      "Ignoring wrong pointing object 1254 0 (offset 0)\n",
      "Ignoring wrong pointing object 1255 0 (offset 0)\n",
      "Ignoring wrong pointing object 1256 0 (offset 0)\n",
      "Ignoring wrong pointing object 1257 0 (offset 0)\n",
      "Ignoring wrong pointing object 1258 0 (offset 0)\n",
      "Ignoring wrong pointing object 1259 0 (offset 0)\n",
      "Ignoring wrong pointing object 1260 0 (offset 0)\n",
      "Ignoring wrong pointing object 1261 0 (offset 0)\n",
      "Ignoring wrong pointing object 1262 0 (offset 0)\n",
      "Ignoring wrong pointing object 1263 0 (offset 0)\n",
      "Ignoring wrong pointing object 1264 0 (offset 0)\n",
      "Ignoring wrong pointing object 1265 0 (offset 0)\n",
      "Ignoring wrong pointing object 1266 0 (offset 0)\n",
      "Ignoring wrong pointing object 1267 0 (offset 0)\n",
      "Ignoring wrong pointing object 1268 0 (offset 0)\n",
      "Ignoring wrong pointing object 1269 0 (offset 0)\n",
      "Ignoring wrong pointing object 1270 0 (offset 0)\n",
      "Ignoring wrong pointing object 1271 0 (offset 0)\n",
      "Ignoring wrong pointing object 1272 0 (offset 0)\n",
      "Ignoring wrong pointing object 1273 0 (offset 0)\n",
      "Ignoring wrong pointing object 1274 0 (offset 0)\n",
      "Ignoring wrong pointing object 1275 0 (offset 0)\n",
      "Ignoring wrong pointing object 1276 0 (offset 0)\n",
      "Ignoring wrong pointing object 1277 0 (offset 0)\n",
      "Ignoring wrong pointing object 1278 0 (offset 0)\n",
      "Ignoring wrong pointing object 1279 0 (offset 0)\n",
      "Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "Ignoring wrong pointing object 1288 0 (offset 0)\n",
      "Ignoring wrong pointing object 1290 0 (offset 0)\n",
      "Ignoring wrong pointing object 1291 0 (offset 0)\n",
      "Ignoring wrong pointing object 1292 0 (offset 0)\n",
      "Ignoring wrong pointing object 1293 0 (offset 0)\n",
      "Ignoring wrong pointing object 1294 0 (offset 0)\n",
      "Ignoring wrong pointing object 1295 0 (offset 0)\n",
      "Ignoring wrong pointing object 1299 0 (offset 0)\n",
      "Ignoring wrong pointing object 1300 0 (offset 0)\n",
      "Ignoring wrong pointing object 1301 0 (offset 0)\n",
      "Ignoring wrong pointing object 1302 0 (offset 0)\n",
      "Ignoring wrong pointing object 1303 0 (offset 0)\n",
      "Ignoring wrong pointing object 1305 0 (offset 0)\n",
      "Ignoring wrong pointing object 1310 0 (offset 0)\n",
      "Ignoring wrong pointing object 1314 0 (offset 0)\n",
      "Ignoring wrong pointing object 1318 0 (offset 0)\n",
      "Ignoring wrong pointing object 1328 0 (offset 0)\n",
      "Ignoring wrong pointing object 1333 0 (offset 0)\n",
      "Ignoring wrong pointing object 1334 0 (offset 0)\n",
      "Ignoring wrong pointing object 1338 0 (offset 0)\n",
      "Ignoring wrong pointing object 1343 0 (offset 0)\n",
      "Ignoring wrong pointing object 1344 0 (offset 0)\n",
      "Ignoring wrong pointing object 1351 0 (offset 0)\n",
      "Ignoring wrong pointing object 1352 0 (offset 0)\n",
      "Ignoring wrong pointing object 1353 0 (offset 0)\n",
      "Ignoring wrong pointing object 1354 0 (offset 0)\n",
      "Ignoring wrong pointing object 1355 0 (offset 0)\n",
      "Ignoring wrong pointing object 1356 0 (offset 0)\n",
      "Ignoring wrong pointing object 1357 0 (offset 0)\n",
      "Ignoring wrong pointing object 1358 0 (offset 0)\n",
      "Ignoring wrong pointing object 1362 0 (offset 0)\n",
      "Ignoring wrong pointing object 1363 0 (offset 0)\n",
      "Ignoring wrong pointing object 1364 0 (offset 0)\n",
      "Ignoring wrong pointing object 1365 0 (offset 0)\n",
      "Ignoring wrong pointing object 1366 0 (offset 0)\n",
      "Ignoring wrong pointing object 1367 0 (offset 0)\n",
      "Ignoring wrong pointing object 1377 0 (offset 0)\n",
      "Ignoring wrong pointing object 1387 0 (offset 0)\n",
      "Ignoring wrong pointing object 1388 0 (offset 0)\n",
      "Ignoring wrong pointing object 1389 0 (offset 0)\n",
      "Ignoring wrong pointing object 1390 0 (offset 0)\n",
      "Ignoring wrong pointing object 1391 0 (offset 0)\n",
      "Ignoring wrong pointing object 1392 0 (offset 0)\n",
      "Ignoring wrong pointing object 1393 0 (offset 0)\n",
      "Ignoring wrong pointing object 1394 0 (offset 0)\n",
      "Ignoring wrong pointing object 1449 0 (offset 0)\n",
      "Ignoring wrong pointing object 1450 0 (offset 0)\n",
      "Ignoring wrong pointing object 1452 0 (offset 0)\n",
      "Ignoring wrong pointing object 1453 0 (offset 0)\n",
      "Ignoring wrong pointing object 1454 0 (offset 0)\n",
      "Ignoring wrong pointing object 1455 0 (offset 0)\n",
      "Ignoring wrong pointing object 1456 0 (offset 0)\n",
      "Ignoring wrong pointing object 1457 0 (offset 0)\n",
      "Ignoring wrong pointing object 1458 0 (offset 0)\n",
      "Ignoring wrong pointing object 1459 0 (offset 0)\n",
      "Ignoring wrong pointing object 1460 0 (offset 0)\n",
      "Ignoring wrong pointing object 1461 0 (offset 0)\n",
      "Ignoring wrong pointing object 1465 0 (offset 0)\n",
      "Ignoring wrong pointing object 1466 0 (offset 0)\n",
      "Ignoring wrong pointing object 1467 0 (offset 0)\n",
      "Ignoring wrong pointing object 1468 0 (offset 0)\n",
      "Ignoring wrong pointing object 1473 0 (offset 0)\n",
      "Ignoring wrong pointing object 1475 0 (offset 0)\n",
      "Ignoring wrong pointing object 1476 0 (offset 0)\n",
      "Ignoring wrong pointing object 1477 0 (offset 0)\n",
      "Ignoring wrong pointing object 1478 0 (offset 0)\n",
      "Ignoring wrong pointing object 1479 0 (offset 0)\n",
      "Ignoring wrong pointing object 1480 0 (offset 0)\n",
      "Ignoring wrong pointing object 1481 0 (offset 0)\n",
      "Ignoring wrong pointing object 1482 0 (offset 0)\n",
      "Ignoring wrong pointing object 1483 0 (offset 0)\n",
      "Ignoring wrong pointing object 1484 0 (offset 0)\n",
      "Ignoring wrong pointing object 1485 0 (offset 0)\n",
      "Ignoring wrong pointing object 1486 0 (offset 0)\n",
      "Ignoring wrong pointing object 1487 0 (offset 0)\n",
      "Ignoring wrong pointing object 1488 0 (offset 0)\n",
      "Ignoring wrong pointing object 1489 0 (offset 0)\n",
      "Ignoring wrong pointing object 1490 0 (offset 0)\n",
      "Ignoring wrong pointing object 1491 0 (offset 0)\n",
      "Ignoring wrong pointing object 1492 0 (offset 0)\n",
      "Ignoring wrong pointing object 1493 0 (offset 0)\n",
      "Ignoring wrong pointing object 1494 0 (offset 0)\n",
      "Ignoring wrong pointing object 1495 0 (offset 0)\n",
      "Ignoring wrong pointing object 1496 0 (offset 0)\n",
      "Ignoring wrong pointing object 1500 0 (offset 0)\n",
      "Ignoring wrong pointing object 1505 0 (offset 0)\n",
      "Ignoring wrong pointing object 1514 0 (offset 0)\n",
      "Ignoring wrong pointing object 1524 0 (offset 0)\n",
      "Ignoring wrong pointing object 1525 0 (offset 0)\n",
      "Ignoring wrong pointing object 1526 0 (offset 0)\n",
      "Ignoring wrong pointing object 1527 0 (offset 0)\n",
      "Ignoring wrong pointing object 1528 0 (offset 0)\n",
      "Ignoring wrong pointing object 1529 0 (offset 0)\n",
      "Ignoring wrong pointing object 1530 0 (offset 0)\n",
      "Ignoring wrong pointing object 1531 0 (offset 0)\n",
      "Ignoring wrong pointing object 1535 0 (offset 0)\n",
      "Ignoring wrong pointing object 1537 0 (offset 0)\n",
      "Ignoring wrong pointing object 1538 0 (offset 0)\n",
      "Ignoring wrong pointing object 1539 0 (offset 0)\n",
      "Ignoring wrong pointing object 1540 0 (offset 0)\n",
      "Ignoring wrong pointing object 1541 0 (offset 0)\n",
      "Ignoring wrong pointing object 1542 0 (offset 0)\n",
      "Ignoring wrong pointing object 1543 0 (offset 0)\n",
      "Ignoring wrong pointing object 1544 0 (offset 0)\n",
      "Ignoring wrong pointing object 1545 0 (offset 0)\n",
      "Ignoring wrong pointing object 1546 0 (offset 0)\n",
      "Ignoring wrong pointing object 1547 0 (offset 0)\n",
      "Ignoring wrong pointing object 1548 0 (offset 0)\n",
      "Ignoring wrong pointing object 1549 0 (offset 0)\n",
      "Ignoring wrong pointing object 1550 0 (offset 0)\n",
      "Ignoring wrong pointing object 1551 0 (offset 0)\n",
      "Ignoring wrong pointing object 1552 0 (offset 0)\n",
      "Ignoring wrong pointing object 1553 0 (offset 0)\n",
      "Ignoring wrong pointing object 1554 0 (offset 0)\n",
      "Ignoring wrong pointing object 1562 0 (offset 0)\n",
      "Ignoring wrong pointing object 1563 0 (offset 0)\n",
      "Ignoring wrong pointing object 1564 0 (offset 0)\n",
      "Ignoring wrong pointing object 1565 0 (offset 0)\n",
      "Ignoring wrong pointing object 1570 0 (offset 0)\n",
      "Ignoring wrong pointing object 1575 0 (offset 0)\n",
      "Ignoring wrong pointing object 1579 0 (offset 0)\n",
      "Ignoring wrong pointing object 1583 0 (offset 0)\n",
      "Ignoring wrong pointing object 1590 0 (offset 0)\n",
      "Ignoring wrong pointing object 1603 0 (offset 0)\n",
      "Ignoring wrong pointing object 1604 0 (offset 0)\n",
      "Ignoring wrong pointing object 1617 0 (offset 0)\n",
      "Ignoring wrong pointing object 1618 0 (offset 0)\n",
      "Ignoring wrong pointing object 1619 0 (offset 0)\n",
      "Ignoring wrong pointing object 1620 0 (offset 0)\n",
      "Ignoring wrong pointing object 1638 0 (offset 0)\n",
      "Ignoring wrong pointing object 1639 0 (offset 0)\n",
      "Ignoring wrong pointing object 1640 0 (offset 0)\n",
      "Ignoring wrong pointing object 1641 0 (offset 0)\n",
      "Ignoring wrong pointing object 1642 0 (offset 0)\n",
      "Ignoring wrong pointing object 1643 0 (offset 0)\n",
      "Ignoring wrong pointing object 1644 0 (offset 0)\n",
      "Ignoring wrong pointing object 1645 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhuâˆ—\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Chengâ€ \\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such\\nframeworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.\\nâˆ—Work done during internship at Bytedance Inc.\\nâ€ Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nÂ©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\\nZhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a userâ€™s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuserâ€™s portrait and make predictions of userâ€™s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural\\nfit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical\\nfeatures, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 0}),\n",
       " Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 1}),\n",
       " Document(page_content='Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our serversâ€™ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate\\ndesign details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlowâ€™s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two\\nsets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlowâ€™s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlowâ€™s limitation that arises from separation of training and in-\\nference, Monolithâ€™s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation\\nis to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case ğ‘‚(1)time complexity\\nfor lookups and deletions, and an expected amortized ğ‘‚(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables ğ‘‡0,ğ‘‡1\\nwith different hash functions â„0(ğ‘¥),â„1(ğ‘¥), and an element would\\nbe stored in either one of them. When trying to insert an element\\nğ´intoğ‘‡0, it first attempts to place ğ´atâ„0(ğ´); Ifâ„0(ğ´)is occupied\\nby another element ğµ, it would evict ğµfromğ‘‡0and try inserting ğµ\\nintoğ‘‡1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;\\n(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 2}),\n",
       " Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nUser \\n Log Kafka \\nActions \\n(Click, Like, \\nConvert â€¦) \\nFeature Kafka \\n Model Server \\n Training Worker \\nTraining PS \\n Serving PS \\nJoiner \\nFlink Job \\nTraining Example \\nKafka \\nBatch Training Data \\nHDFS \\nGenerate \\nFeatures \\nParameter Synchronization \\nDump Batch Data \\nOnline \\nTraining \\nBatch Training \\nFigure 4: Streaming Engine.\\nThe information feedback loop from [User â†’Model Serverâ†’Training Workerâ†’Model Serverâ†’User] would spend a long time when taking the Batch Training\\npath, while the Online Training will close the loop more instantly.\\n(2)IDs are timed and set to expire after being inactive for a\\npredefined period of time. The expire time is also tunable for\\neach embedding table to allow for distinguishing features\\nwith different sensitivity to historical information.\\nIn our implementation, HashTable is implemented as a Tensor-\\nFlow resource operation. Similar to Variable , look-ups and updates\\nare also implemented as native TensorFlow operations for easier\\nintegration and better compatibility.\\n2.2 Online Training\\nIn Monolith, training is divided into two stages (Figure 1):\\n(1)Batch training stage. This stage works as an ordinary Tensor-\\nFlow training loop: In each training step, a training worker\\nreads one mini-batch of training examples from the stor-\\nage, requests parameters from PS, computes a forward and\\na backward pass, and finally push updated parameters to\\nthe training PS. Slightly different from other common deep\\nlearning tasks, we only train our dataset for one pass. Batch\\ntraining is useful for training historical data when we modify\\nour model architecture and retrain the model;\\n(2)Online training stage. After a model is deployed to online\\nserving, the training does not stop but enters the online train-\\ning stage. Instead of reading mini-batch examples from the\\nstorage, a training worker consumes realtime data on-the-fly\\nand updates the training PS. The training PS periodically syn-\\nchronizes its parameters to the serving PS, which will take\\neffect on the user side immediately. This enables our model\\nto interactively adapt itself according to a userâ€™s feedback in\\nrealtime.2.2.1 Streaming Engine. Monolith is built with the capability of\\nseamlessly switching between batch training and online training.\\nThis is enabled by our design of streaming engine as illustrated by\\nFigure 4.\\nIn our design, we use one Kafka [ 13] queue to log actions of users\\n(E.g. Click on an item or like an item etc.) and another Kafka queue\\nfor features. At the core of the engine is a Flink [ 4] streaming job for\\nonline feature Joiner. The online joiner concatenates features with\\nlabels from user actions and produces training examples, which are\\nthen written to a Kafka queue. The queue for training examples is\\nconsumed by both online training and batch training:\\nâ€¢For online training, the training worker directly reads data\\nfrom the Kafka queue;\\nâ€¢For batch training, a data dumping job will first dump data\\nto HDFS [ 18]; After data in HDFS accumulated to certain\\namount, training worker will retrieve data from HDFS and\\nperform batch training.\\nUpdated parameters in training PS will be pushed to serving PS\\naccording to the parameter synchronization schedule.\\n2.2.2 Online Joiner. In real-world applications, user actions log\\nand features are streamed into the online joiner (Figure 5) without\\nguarantee in time order. Therefore we use a unique key for each\\nrequest so that user action and features could correctly pair up.\\nThe lag of user action could also be a problem. For example, a\\nuser may take a few days before they decide to buy an item they\\nwere presented days ago. This is a challenge for the joiner because\\nif all features are kept in cache, it would simply not fit in memory.\\nIn our system, an on-disk key-value storage is utilized to store\\nfeatures that are waiting for over certain time period. When a user\\naction log arrives, it first looks up the in-memory cache, and then\\nlooks up the key-value storage in case of a missing cache.', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 3}),\n",
       " Document(page_content='Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nLog Kafka \\n(User Actions) \\nFeature Kafka \\nIn-memory \\nCache \\nOn-disk \\nKV-Store \\nJoin \\nNegative \\nSampling \\nTraining Example \\nKafka \\nFound in Cache \\nRead from KV-Store \\nFigure 5: Online Joiner.\\nAnother problem that arise in real-world application is that\\nthe distribution of negative and positive examples are highly un-\\neven, where number of the former could be magnitudes of order\\nhigher than the latter. To prevent positive examples from being\\noverwhelmed by negative ones, a common strategy is to do negative\\nsampling. This would certainly change the underlying distribution\\nof the trained model, tweaking it towards higher probability of\\nmaking positive predictions. As a remedy, we apply log odds cor-\\nrection [ 19] during serving, making sure that the online model is\\nan unbiased estimator of the original distribution.\\n2.2.3 Parameter Synchronization. During online training, the Mono-\\nlith training cluster keeps receiving data from the online serving\\nmodule and updates parameters on the training PS. A crucial step\\nto enable the online serving PS to benefit from these newly trained\\nparameters is the synchronization of updated model parameters. In\\nproduction environment, we are encountered by several challenges:\\nâ€¢Models on the online serving PS must not stop serving when\\nupdating. Our models in production is usually several ter-\\nabytes in size, and as a result replacing all parameters takes a\\nwhile. It would be intolerable to stop an online PS from serv-\\ning the model during the replacement process, and updates\\nmust be made on-the-fly;\\nâ€¢Transferring a multi-terabyte model of its entirety from train-\\ning PS to the online serving PS would pose huge pressure\\nto both the network bandwidth and memory on PS, since it\\nrequires doubled model size of memory to accept the newly\\narriving model.\\nFor the online training to scale up to the size of our business\\nscenario, we designed an incremental on-the-fly periodic parameter\\nsynchronization mechanism in Monolith based on several notice-\\nable characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\\ntion models;\\n(2)Given a short range of time window, only a small subset of\\nIDs gets trained and their embeddings updated;\\n(3)Dense variables move much slower than sparse embeddings.\\nThis is because in momentum-based optimizers, the accumu-\\nlation of momentum for dense variables is magnified by the\\ngigantic size of recommendation training data, while only\\na few sparse embeddings receives updates in a single data\\nbatch.\\n(1) and (2) allows us to exploit the sparse updates across all feature\\nIDs. In Monolith, we maintain a hash set of touched keys, repre-\\nsenting IDs whose embeddings get trained since the last parameter\\nsynchronization. We push the subset of sparse parameters whose\\nkeys are in the touched-keys set with a minute-level time interval\\nfrom the training PS to the online serving PS. This relatively small\\npack of incremental parameter update is lightweight for network\\ntransmission and will not cause a sharp memory spike during the\\nsynchronization.\\nWe also exploit (3) to further reduce network I/O and memory\\nusage by setting a more aggressive sync schedule for sparse pa-\\nrameters, while updating dense parameters less frequently. This\\ncould render us a situation where the dense parameters we serve is\\na relatively stale version compared to sparse part. However, such\\ninconsistency could be tolerated due to the reason mentioned in (3)\\nas no conspicuous loss has been observed.\\n2.3 Fault Tolerance\\nAs a system in production, Monolith is designed with the ability to\\nrecover a PS in case it fails. A common choice for fault tolerance is\\nto snapshot the state of a model periodically, and recover from the', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 4}),\n",
       " Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nlatest snapshot when PS failure is detected. The choice of snapshot\\nfrequency has two major impacts:\\n(1)Model quality. Intuitively, model quality suffers less from\\nloss of recent history with increased snapshot frequency.\\n(2)Computation overhead. Snapshotting a multi-terabyte model\\nis not free. It incurs large chunks of memory copy and disk\\nI/O.\\nAs a trade-off between model quality and computation overhead,\\nMonolith snapshots all training PS every day. Though a PS will lose\\none dayâ€™s worth of update in case of a failure, we discover that the\\nperformance degradation is tolerable through our experiments. We\\nwill analyze the effect of PS reliability in the next section.\\n. . .\\n. . .\\nIDsEmbeddings FMMLP Output \\nFigure 6: DeepFM model architecture.\\n3 EVALUATION\\nFor a better understanding of benefits and trade-offs brought about\\nby our proposed design, we conducted several experiments at pro-\\nduction scale and A/B test with live serving traffic to evaluate and\\nverify Monolith from different aspects. We aim to answer the fol-\\nlowing questions by our experiments:\\n(1) How much can we benefit from a collisionless HashTable ?\\n(2) How important is realtime online training?\\n(3)Is Monolithâ€™s design of parameter synchronization robust\\nenough in a large-scale production scenario?\\nIn this section, we first present our experimental settings and\\nthen discuss results and our findings in detail.\\n3.1 Experimental Setup\\n3.1.1 Embedding Table. As described in Section 2.1, embedding\\ntables in Monolith are implemented as collisionless HashTable s.\\nTo prove the necessity of avoiding collisions in embedding tables\\nand to quantify gains from our collisionless implementation, we\\nperformed two groups of experiments on the Movielens dataset\\nand on our internal production dataset respectively:\\n(1)MovieLens ml-25m dataset [ 11]. This is a standard public\\ndataset for movie ratings, containing 25 million ratings that\\ninvolves approximately 162000 users and 62000 movies.\\nâ€¢Preprocessing of labels . The original labels are ratings from\\n0.5 to 5.0, while in production our tasks are mostly re-\\nceiving binary signals from users. To better simulate our\\nproduction models, we convert scale labels to binary labelsby treating scoresâ‰¥3.5as positive samples and the rest\\nas negative samples.\\nâ€¢Model and metrics . We implemented a standard DeepFM\\n[9] model, a commonly used model architecture for recom-\\nmendation problems. It consist of an FM component and a\\ndense component (Figure 6). Predictions are evaluated by\\nAUC [ 2] as this is the major measurement for real models.\\nâ€¢Embedding collisions . This dataset contains approximately\\n160K user IDs and 60K movie IDs. To compare with the\\ncollisionless version of embedding table implementation,\\nwe performed another group of experiment where IDs are\\npreprocessed with MD5 hashing and then mapped to a\\nsmaller ID space. As a result, some IDs will share their\\nembedding with others. Table 1 shows detailed statistics\\nof user and movie IDs before and after hashing.\\nUser IDs Movie IDs\\n# Before Hashing 162541 59047\\n# After Hashing 149970 57361\\nCollision rate 7.73% 2 .86%\\nTable 1: Statistics of IDs Before and After Hashing.\\n(2)Internal Recommendation dataset .\\nWe also performed experiments on a recommendation model\\nin production environment. This model generally follows a\\nmulti-tower architecture, with each tower responsible for\\nlearning to predict a specialized kind of user behavior.\\nâ€¢Each model has around 1000 embedding tables, and distri-\\nbution of size of embedding tables are very uneven;\\nâ€¢The original ID space of embedding table was 248. In our\\nbaseline, we applied a hashing trick by decomposing to\\ncurb the size of embedding table. To be more specific, we\\nuse two smaller embedding tables instead of a gigantic\\none to generate a unique embedding for each ID by vector\\ncombination:\\nğ¼ğ·ğ‘Ÿ=ğ¼ğ·%224\\nğ¼ğ·ğ‘=ğ¼ğ·Ã·224\\nğ¸=ğ¸ğ‘Ÿ+ğ¸ğ‘,\\nwhereğ¸ğ‘Ÿ,ğ¸ğ‘are embeddings corresponding to ğ¼ğ·ğ‘Ÿ,ğ¼ğ·ğ‘.\\nThis effectively reduces embedding table sizes from 248\\nto225;\\nâ€¢This model is serving in real production, and the perfor-\\nmance of this experiment is measured by online AUC with\\nreal serving traffic.\\n3.1.2 Online Training. During online training, we update our on-\\nline serving PS with the latest set of parameters with minute-level\\nintervals. We designed two groups of experiments to verify model\\nquality and system robustness.\\n(1)Update frequency . To investigate the necessity of minute-\\nlevel update frequency, we conducted experiments that syn-\\nchronize parameters from training model to prediction model\\nwith different intervals.', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 5}),\n",
       " Document(page_content='Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nAlgorithm 1 Simulated Online Training.\\n1:Input:ğ·ğ‘ğ‘ğ‘¡ğ‘â„; /* Data for batch training. */\\n2:Input:ğ·ğ‘œğ‘›ğ‘™ğ‘–ğ‘›ğ‘’\\nğ‘–=1Â·Â·Â·ğ‘; /* Data for online training, split into ğ‘shards. */\\n3:ğœƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›â†ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ·ğ‘ğ‘ğ‘¡ğ‘â„,ğœƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›); /* Batch training. */\\n/* Online training. */\\n4:forğ‘–=1Â·Â·Â·ğ‘do\\n5:ğœƒğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘’â†ğœƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ; /* Sync training parameters to serving model. */\\n6:ğ´ğ‘ˆğ¶ğ‘–=Evaluate(ğœƒğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘’,ğ·ğ‘œğ‘›ğ‘™ğ‘–ğ‘›ğ‘’\\nğ‘–); /* Evaluate online prediction on new data. */\\n7:ğœƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›â†ğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›(ğ·ğ‘œğ‘›ğ‘™ğ‘–ğ‘›ğ‘’\\nğ‘–,ğœƒğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›); /* Train with new data. */\\n8:end for\\nThe dataset we use is the Criteo Display Ads Challenge\\ndataset3, a large-scale standard dataset for benchmarking\\nCTR models. It contains 7 days of chronologically ordered\\ndata recording features and click actions. For this experiment,\\nwe use a standard DeepFM [9] model as described in 6.\\nTo simulate online training, we did the following preprocess-\\ning for the dataset. We take 7 days of data from the dataset,\\nand split it to two parts: 5 days of data for batch training, and\\n2 days for online training. We further split the 2 days of data\\nintoğ‘shards chronologically. Online training is simulated\\nby algorithm 1.\\nAs such, we simulate synchronizing trained parameters to\\nonline serving PS with an interval determined by number of\\ndata shards. We experimented with ğ‘=10,50,100, which\\nroughly correspond to update interval of 5â„ğ‘Ÿ,1â„ğ‘Ÿ, and 30ğ‘šğ‘–ğ‘›.\\n(2)Live experiment . In addition, we also performed a live ex-\\nperiment with real serving traffic to further demonstrate the\\nimportance of online training in real-world application. This\\nA/B experiment compares online training to batch training\\none one of our Ads model in production.\\n3.2 Results and Analysis\\n3.2.1 The Effect of Embedding Collision. Results from MovieLens\\ndataset and the Internal recommedation dataset both show that\\nembedding collisions will jeopardize model quality.\\n(1)Models with collisionless HashTable consistently outper-\\nforms those with collision. This conclusion holds true re-\\ngardless of\\nâ€¢Increase of number of training epochs. As shown in Figure\\n7, the model with collisionless embedding table has higher\\nAUC from the first epoch and converges at higher value;\\nâ€¢Change of distribution with passage of time due to Con-\\ncept Drift. As shown in Figure 8, models with collision-\\nless embedding table is also robust as time passes by and\\nusers/items context changes.\\n(2)Data sparsity caused by collisionless embedding table will\\nnot lead to model overfitting. As shown in Figure 7, a model\\nwith collisionless embedding table does not overfit after it\\nconverges.\\n3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\\n1 2 3 4 5 6 7 8 9 10\\nepoch0.8050.8100.8150.8200.825test auccollision-free hash\\nhash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\\nMovieLens\\n2 4 6 8 10 12\\nDay0.7700.7750.7800.7850.790Online serving AUChash w/ collision\\ncollisionless hash table\\nFigure 8: Effect of Embedding Collision On A\\nRecommendation Model In Production\\nWe measure performance of this recommendation model by online serving AUC,\\nwhich is fluctuating across different days due to concept-drift.', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 6}),\n",
       " Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\nâ€¢Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;â€¢Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5â„ğ‘Ÿ,1â„ğ‘Ÿ, and 30ğ‘šğ‘–ğ‘› respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66Â±0.020 79 .42Â±0.026\\n1 hr 79.78Â±0.005 79 .44Â±0.030\\n30 min 79.80Â±0.008 79 .43Â±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 7}),\n",
       " Document(page_content='Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nDay 1 2 3 4 5 6 7\\nAUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\\nTable 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\\nSync Interval 1 hr\\nSync Interval 30 min\\nFigure 10: Comparison of different sync intervals for\\nonline training.\\nThe live A/B experiment between online training and batch\\ntraining on an Ads model in production also show that there\\nis a significant bump in online serving AUC (Table 3).\\nInspired by this observation, we synchronize sparse parame-\\nters to serving PS of our production models as frequent as\\npossible (currently at minute-level), to the extent that the\\ncomputation overhead and system reliability could endure.\\nRecall that dense variables requires a less frequent update as\\ndiscussed in 2.2.3, we update them at day-level. By doing so,\\nwe can bring down our computation overhead to a very low\\nlevel. Suppose 100,000 IDs gets updated in a minute, and the\\ndimension of embedding is 1024, the total size of data need\\nto be transferred is 4ğ¾ğµÃ—100,000â‰ˆ400ğ‘€ğµper minute.\\nFor dense parameters, since they are synchronized daily, we\\nchoose to schedule the synchronization when the traffic is\\nlowest (e.g. midnight).\\n(2)The Effect of PS reliability.\\nWith a minute-level parameter synchronization, we initially\\nexpect a more frequent snapshot of training PS to match the\\nrealtime update. To our surprise, we enlarged the snapshot\\ninterval to 1 day and still observed nearly no loss of model\\nquality.\\nFinding the right trade-off between model quality and com-\\nputation overhead is difficult for personalized ranking sys-\\ntems since users are extremely sensitive on recommendation\\nquality. Traditionally, large-scale systems tend to set a fre-\\nquent snapshot schedule for their models, which sacrifices\\ncomputation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\\nregard and to our surprise, model quality is more robust than\\nexpected. With a 0.01% failure rate of PS machine per day,\\nwe find a model from the previous day works embarrassingly\\nwell. This is explicable by the following calculation: Suppose\\na modelâ€™s parameters are sharded across 1000 PS, and they\\nsnapshot every day. Given 0.01% failure rate, one of them\\nwill go down every 10 days and we lose all updates on this\\nPS for 1 day. Assuming a DAU of 15 Million and an even\\ndistribution of user IDs on each PS, we lose 1 dayâ€™s feedback\\nfrom 15000 users every 10 days. This is acceptable because\\n(a) For sparse features which is user-specific, this is equiv-\\nalent to losing a tiny fraction of 0.01% DAU; (b) For dense\\nvariables, since they are updated slowly as we discussed in\\n2.2.3, losing 1 dayâ€™s update out of 1000 PS is negligible.\\nBased on the above observation and calculation, we radically\\nlowered our snapshot frequency and thereby saved quite a\\nbit in computation overhead.\\n4 RELATED WORK\\nEver since some earliest successful application of deep learning to\\nindustry-level recommendation systems [ 6,10], researchers and\\nengineers have been employing various techniques to ameliorate\\nissues mentioned in Section 1.\\nTo tackle the issue of sparse feature representation, [ 3,6] uses\\nfixed-size embedding table with hash-trick. There are also attempts\\nin improving hashing to reduce collision [ 3,7]. Other works directly\\nutilize native key-value hash table to allow dynamic growth of table\\nsize [ 12,15,20,21]. These implementations builds upon TensorFlow\\nbut relies either on specially designed software mechanism [ 14,\\n15,20] or hardware [ 21] to access and manage their hash-tables.\\nCompared to these solutions, Monolithâ€™s hash-table is yet another\\nnative TensorFlow operation. It is developer friendly and has higher\\ncross-platform interoperability, which is suitable for ToB scenarios.\\nAn organic and tight integration with TensorFlow also enables\\neasier optimizations of computation performance.\\nBridging the gap between training and serving and alleviation\\nof Concept Drift [ 8] is another topic of interest. To support online\\nupdate and avoid memory issues, both [ 12] and [ 20] designed fea-\\nture eviction mechanisms to flexibly adjust the size of embedding\\ntables. Both [ 12] and [ 14] support some form of online training,\\nwhere learned parameters are synced to serving with a relatively\\nshort interval compared to traditional batch training, with fault tol-\\nerance mechanisms. Monolith took similar approach to elastically\\nadmit and evict features, while it has a more lightweight parameter\\nsynchronization mechanism to guarantee model quality.', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 8}),\n",
       " Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n5 CONCLUSION\\nIn this work, we reviewed several most important challenges for\\nindustrial-level recommendation systems and present our system\\nin production, Monolith, to address them and achieved best perfor-\\nmance compared to existing solutions.\\nWe proved that a collisionless embedding table is essential for\\nmodel quality, and demonstrated that our implementation of Cuckoo\\nHashMap based embedding table is both memory efficient and help-\\nful for improving online serving metrics.\\nWe also proved that realtime serving is crucial in recommenda-\\ntion systems, and that parameter synchronization interval should\\nbe as short as possible for an ultimate model performance. Our\\nsolution for online realtime serving in Monolith has a delicately\\ndesigned parameter synchronization and a fault tolerance mecha-\\nnism: In our parameter synchronization algorithm, we showed that\\nconsistency of version across different parts of parameters could be\\ntraded-off for reducing network bandwidth consumption; In fault\\ntolerance design, we demonstrated that our strategy of trading-off\\nPS reliability for realtime-ness is a robust solution.\\nTo conclude, Monolith succeeded in providing a general solution\\nfor production scale recommendation systems.\\nACKNOWLEDGMENTS\\nHanzhi Zhou provided useful suggestions on revision of this paper.\\nREFERENCES\\n[1]MartÃ­n Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\\nYuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\\nmachine learning. ArXiv abs/1605.08695 (2016).\\n[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145â€“\\n1159.\\n[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\\nengineering.com/core-modeling-at-instagram-a51e0158aa48\\n[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\\nand Kostas Tzoumas. 2015. Apache Flink â„¢: Stream and Batch Processing in a\\nSingle Engine. IEEE Data Eng. Bull. 38 (2015), 28â€“38.\\n[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\\nIspir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\\nHemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\\nof the 1st Workshop on Deep Learning for Recommender Systems (2016).\\n[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\\nfor YouTube Recommendations. Proceedings of the 10th ACM Conference on\\nRecommender Systems (2016).\\n[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\\nteenth ACM Conference on Recommender Systems (2021).\\n[8]JoÃ£o Gama, Indr Ë™e Å½liobait Ë™e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\\n2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\\n(2014), 1 â€“ 37.\\n[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\\nIJCAI .\\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\\nDavid M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\\nAndrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\\nXuan Zhang. 2020. The Architectural Implications of Facebookâ€™s DNN-Based\\nPersonalized Recommendation. 2020 IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA) (2020), 488â€“501.\\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\\nand Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1â€“19:19.\\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\\nHuang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\\nSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\\nProceedings of the 1st International Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data (2019).\\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\\nWu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan\\nLuo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\\nLin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\\nbo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\\nScaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\\nabs/2111.05897 (2021).\\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\\nmender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\\ntensorflow-in-recommender-systems.html\\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS .\\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\\n2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) (2010), 1â€“10.\\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\\nSampling and Log Odds Correction with Rare Events Data. In Advances in Neural\\nInformation Processing Systems .\\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\\nLin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\\nContinual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\\nnational Conference for High Performance Computing, Networking, Storage and\\nAnalysis (2020), 1â€“17.\\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\\n2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\\n28th ACM International Conference on Information and Knowledge Management\\n(2019).', metadata={'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_in_chunks_fmt=Read_doc(path=\"G:\\CORE-3\\OPEN_AI_1\\VECTOR_DB_SEARCH\\Monolith_Real Time Recommendation System_BD.pdf\")\n",
    "docs_in_chunks_fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note- AS Chunk Size increases, No of Doc_chunks decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_in_chunks_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Embedding techniques using HuggingFace & Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iNTRINSICALLY IT CALLS SENTENCES TRANSFRMER TO CONVERT THE GIVEN TNPUTS INTO EMBEDDING VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "\n",
    "# model = SentenceTransformer(\"Salesforce/SFR-Embedding-Mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) USing Hugginface_embeddings-default-If using models from SBERT.net, then import sentencetransformer package as pre-requisite |||ar to (importing transformers for huggingface local pipeline for invoking & finetuning the LLM EMBEDDING MODEL FORlocal applications.) as Hugginface_embeddings-default have restrictionsto support only for models that only support text2text/text,conversation & translational useacses ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_embeddings_1=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\",model_kwargs={\"token\":os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={'token': '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'}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_embeddings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=HF_embeddings_1.embed_query(text=\"HI hpw are you\")\n",
    "# X2=HF_embeddings_1.embed_query(text=\"HI hpw are you\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,\n",
       " [0.012999637983739376,\n",
       "  -0.017129454761743546,\n",
       "  0.018512491136789322,\n",
       "  0.002367186127230525,\n",
       "  0.00745107838883996,\n",
       "  0.020457208156585693,\n",
       "  -0.010868463665246964,\n",
       "  0.016595248132944107,\n",
       "  0.009312111884355545,\n",
       "  -0.02195834182202816,\n",
       "  -0.003944918047636747,\n",
       "  -0.07091318070888519,\n",
       "  0.05666534975171089,\n",
       "  0.015442346222698689,\n",
       "  0.07004604488611221,\n",
       "  -0.08592944592237473,\n",
       "  0.00975573156028986,\n",
       "  0.016392813995480537,\n",
       "  -0.03093331679701805,\n",
       "  -0.004492770880460739,\n",
       "  -0.013158243149518967,\n",
       "  -0.005811539012938738,\n",
       "  -0.027268720790743828,\n",
       "  0.036309417337179184,\n",
       "  -0.004804563242942095,\n",
       "  0.008175290189683437,\n",
       "  -0.02284267172217369,\n",
       "  0.013835953548550606,\n",
       "  0.010306678712368011,\n",
       "  0.06863735616207123,\n",
       "  0.02921111136674881,\n",
       "  -0.02883000299334526,\n",
       "  0.027904920279979706,\n",
       "  0.030461473390460014,\n",
       "  1.863529291767918e-06,\n",
       "  -0.05150868743658066,\n",
       "  -0.00878837425261736,\n",
       "  0.0055794911459088326,\n",
       "  0.011600010097026825,\n",
       "  -0.020395997911691666,\n",
       "  -0.0372794084250927,\n",
       "  0.01286633126437664,\n",
       "  -0.012013011611998081,\n",
       "  0.03504011034965515,\n",
       "  0.01612309366464615,\n",
       "  -0.05282271280884743,\n",
       "  0.002291571581736207,\n",
       "  0.042870763689279556,\n",
       "  -0.021420065313577652,\n",
       "  0.011580429971218109,\n",
       "  0.001630605198442936,\n",
       "  0.013083933852612972,\n",
       "  0.03984680026769638,\n",
       "  -0.03901384398341179,\n",
       "  0.020796673372387886,\n",
       "  -0.020119260996580124,\n",
       "  0.013287500478327274,\n",
       "  0.002471166430041194,\n",
       "  -0.005152493249624968,\n",
       "  -0.017009282484650612,\n",
       "  0.08322297781705856,\n",
       "  0.03005884401500225,\n",
       "  -0.029941771179437637,\n",
       "  -0.035644158720970154,\n",
       "  0.015219750814139843,\n",
       "  0.0330568328499794,\n",
       "  0.018643170595169067,\n",
       "  0.00659651355817914,\n",
       "  0.039046674966812134,\n",
       "  0.053307708352804184,\n",
       "  0.019051603972911835,\n",
       "  -0.038706425577402115,\n",
       "  0.023968849331140518,\n",
       "  0.05904141440987587,\n",
       "  -0.014165894128382206,\n",
       "  0.003953031729906797,\n",
       "  -0.021977590397000313,\n",
       "  -0.008228217251598835,\n",
       "  -0.0200898889452219,\n",
       "  -0.020436851307749748,\n",
       "  0.017035366967320442,\n",
       "  0.01046250481158495,\n",
       "  0.02378055825829506,\n",
       "  0.005161608569324017,\n",
       "  0.0023396711330860853,\n",
       "  -0.004769525025039911,\n",
       "  0.021409297361969948,\n",
       "  -0.03676695004105568,\n",
       "  -0.027711208909749985,\n",
       "  0.0009340737597085536,\n",
       "  0.015109316445887089,\n",
       "  0.02360702119767666,\n",
       "  0.02127539925277233,\n",
       "  0.050609324127435684,\n",
       "  0.056094691157341,\n",
       "  -0.03752812743186951,\n",
       "  0.019149737432599068,\n",
       "  0.02369603142142296,\n",
       "  -0.023072753101587296,\n",
       "  -0.08691245317459106,\n",
       "  -0.050090491771698,\n",
       "  0.01833251863718033,\n",
       "  0.04681029170751572,\n",
       "  0.020073888823390007,\n",
       "  0.03734811767935753,\n",
       "  -0.029363583773374557,\n",
       "  0.04014241695404053,\n",
       "  -0.06451929360628128,\n",
       "  0.011008906178176403,\n",
       "  -0.015850182622671127,\n",
       "  -0.03658255189657211,\n",
       "  -0.017502624541521072,\n",
       "  -0.0267682783305645,\n",
       "  0.04344160109758377,\n",
       "  -0.009447048418223858,\n",
       "  0.015042486600577831,\n",
       "  -0.02656541019678116,\n",
       "  0.01697271317243576,\n",
       "  0.005445689894258976,\n",
       "  0.007153452839702368,\n",
       "  -0.010027448646724224,\n",
       "  -0.024672577157616615,\n",
       "  0.022823721170425415,\n",
       "  0.053233709186315536,\n",
       "  0.04761515557765961,\n",
       "  0.057602472603321075,\n",
       "  0.0015924657927826047,\n",
       "  0.0008863187395036221,\n",
       "  -0.030327264219522476,\n",
       "  -0.07617579400539398,\n",
       "  0.01823272369801998,\n",
       "  0.018525393679738045,\n",
       "  -0.008369080722332,\n",
       "  -0.018808813765645027,\n",
       "  0.02844928577542305,\n",
       "  0.1389879733324051,\n",
       "  -0.06510218232870102,\n",
       "  0.013127539306879044,\n",
       "  0.024995217099785805,\n",
       "  -0.03454781323671341,\n",
       "  -0.03244229033589363,\n",
       "  0.0027042177971452475,\n",
       "  -0.004812892060726881,\n",
       "  -0.02344345860183239,\n",
       "  0.0328148789703846,\n",
       "  -0.057044047862291336,\n",
       "  -0.005303144454956055,\n",
       "  0.01693553477525711,\n",
       "  0.012294887565076351,\n",
       "  -0.013154848478734493,\n",
       "  0.015405439771711826,\n",
       "  0.0247407928109169,\n",
       "  -0.0820104256272316,\n",
       "  -0.005808339919894934,\n",
       "  0.06579609960317612,\n",
       "  0.0006702970713376999,\n",
       "  0.001699297223240137,\n",
       "  -0.06559974700212479,\n",
       "  -0.05894491821527481,\n",
       "  0.022883422672748566,\n",
       "  0.005283147096633911,\n",
       "  -0.022340161725878716,\n",
       "  0.0563732348382473,\n",
       "  0.045589249581098557,\n",
       "  -0.017769960686564445,\n",
       "  0.007316553499549627,\n",
       "  0.045976411551237106,\n",
       "  -0.029755687341094017,\n",
       "  0.010002950206398964,\n",
       "  -0.0057303085923194885,\n",
       "  -0.01232444029301405,\n",
       "  0.04614457115530968,\n",
       "  -0.020681673660874367,\n",
       "  0.029946429654955864,\n",
       "  0.030989812687039375,\n",
       "  -0.04276760295033455,\n",
       "  0.022902049124240875,\n",
       "  0.02288569137454033,\n",
       "  0.024359209463000298,\n",
       "  -0.00894821435213089,\n",
       "  0.0010259982664138079,\n",
       "  -0.08558090776205063,\n",
       "  -0.019497809931635857,\n",
       "  -0.04451390355825424,\n",
       "  -0.054408907890319824,\n",
       "  0.017531968653202057,\n",
       "  -0.02962224744260311,\n",
       "  -0.010501866228878498,\n",
       "  0.008663676679134369,\n",
       "  -0.019217615947127342,\n",
       "  0.02635986916720867,\n",
       "  0.0445239394903183,\n",
       "  -0.0850415900349617,\n",
       "  0.02618383802473545,\n",
       "  0.038857609033584595,\n",
       "  0.009776901453733444,\n",
       "  0.0026459505315870047,\n",
       "  0.02550593391060829,\n",
       "  0.0510890856385231,\n",
       "  -0.06899691373109818,\n",
       "  0.00958398636430502,\n",
       "  0.011867859400808811,\n",
       "  0.007879078388214111,\n",
       "  -0.01501878909766674,\n",
       "  -0.009653780609369278,\n",
       "  -0.0009139979374594986,\n",
       "  0.08270979672670364,\n",
       "  -0.029003316536545753,\n",
       "  -0.03284075856208801,\n",
       "  -0.021486395969986916,\n",
       "  0.015048839151859283,\n",
       "  -0.022147007286548615,\n",
       "  0.03778078779578209,\n",
       "  0.04654066264629364,\n",
       "  0.033300623297691345,\n",
       "  0.021133527159690857,\n",
       "  -0.010255943983793259,\n",
       "  0.0038959523662924767,\n",
       "  -0.011484202928841114,\n",
       "  -0.005843000020831823,\n",
       "  0.023859411478042603,\n",
       "  0.05789503827691078,\n",
       "  0.032153964042663574,\n",
       "  -0.010768563486635685,\n",
       "  0.05134320259094238,\n",
       "  0.0011455133790150285,\n",
       "  0.05821071192622185,\n",
       "  0.0088571235537529,\n",
       "  -0.004831152502447367,\n",
       "  0.062404826283454895,\n",
       "  2.8329057386144996e-05,\n",
       "  0.007489391136914492,\n",
       "  -0.019386278465390205,\n",
       "  -0.012963731773197651,\n",
       "  -0.006963374093174934,\n",
       "  0.00391144584864378,\n",
       "  0.06344790011644363,\n",
       "  -0.08949469774961472,\n",
       "  -0.019031992182135582,\n",
       "  -0.05778677389025688,\n",
       "  0.06162209063768387,\n",
       "  -0.02029365859925747,\n",
       "  0.03396877273917198,\n",
       "  0.0009933726396411657,\n",
       "  0.0343594029545784,\n",
       "  0.02475764788687229,\n",
       "  0.04695630446076393,\n",
       "  -0.012485911138355732,\n",
       "  0.01333817932754755,\n",
       "  0.007671775761991739,\n",
       "  -0.020786654204130173,\n",
       "  -0.03349690139293671,\n",
       "  0.012563527561724186,\n",
       "  0.0037832348607480526,\n",
       "  0.014136345125734806,\n",
       "  -0.0902421697974205,\n",
       "  -0.022907959297299385,\n",
       "  -0.007452358957380056,\n",
       "  -0.09074703603982925,\n",
       "  -0.0032915708143264055,\n",
       "  0.022571127861738205,\n",
       "  -0.04342715069651604,\n",
       "  -0.010498801246285439,\n",
       "  0.0326361320912838,\n",
       "  0.052584461867809296,\n",
       "  -0.0009436520631425083,\n",
       "  -0.010786193422973156,\n",
       "  -0.033765148371458054,\n",
       "  -0.013661493547260761,\n",
       "  0.003807118395343423,\n",
       "  -0.0020680767484009266,\n",
       "  0.0336359366774559,\n",
       "  0.02850254811346531,\n",
       "  -0.0026256467681378126,\n",
       "  -0.045894693583250046,\n",
       "  -0.01254607830196619,\n",
       "  0.024415137246251106,\n",
       "  -0.024208638817071915,\n",
       "  -0.023289749398827553,\n",
       "  0.05000516027212143,\n",
       "  0.028073040768504143,\n",
       "  0.02583678625524044,\n",
       "  -0.05246929079294205,\n",
       "  -0.03574181720614433,\n",
       "  0.019913651049137115,\n",
       "  0.005996150895953178,\n",
       "  0.022762252017855644,\n",
       "  -0.1066851019859314,\n",
       "  0.026292163878679276,\n",
       "  0.05490866303443909,\n",
       "  -0.04085636883974075,\n",
       "  0.015492387115955353,\n",
       "  -0.031999215483665466,\n",
       "  -0.03848510608077049,\n",
       "  0.02842896804213524,\n",
       "  -0.032119110226631165,\n",
       "  -0.03519182652235031,\n",
       "  -0.06389673799276352,\n",
       "  -0.010374253615736961,\n",
       "  -0.022810377180576324,\n",
       "  -0.035885751247406006,\n",
       "  0.0074547771364450455,\n",
       "  -0.011318389326334,\n",
       "  -0.006723492871969938,\n",
       "  -0.03092411532998085,\n",
       "  0.0053014978766441345,\n",
       "  0.050282109528779984,\n",
       "  -0.030443985015153885,\n",
       "  0.017069483175873756,\n",
       "  0.012016511522233486,\n",
       "  0.03130228444933891,\n",
       "  -0.020969785749912262,\n",
       "  0.021942807361483574,\n",
       "  -0.039449263364076614,\n",
       "  0.03734762221574783,\n",
       "  0.03771994635462761,\n",
       "  -0.052264679223299026,\n",
       "  0.004800600465387106,\n",
       "  0.06689111888408661,\n",
       "  -0.04084528237581253,\n",
       "  0.025750616565346718,\n",
       "  -0.01920340768992901,\n",
       "  0.007977316156029701,\n",
       "  -0.013167114928364754,\n",
       "  -0.016111338511109352,\n",
       "  0.01098512951284647,\n",
       "  -0.052480220794677734,\n",
       "  0.06401722878217697,\n",
       "  -0.03513157740235329,\n",
       "  0.01402351725846529,\n",
       "  0.013764286413788795,\n",
       "  -0.08369718492031097,\n",
       "  -0.03563510254025459,\n",
       "  0.0026025567203760147,\n",
       "  -0.004678500350564718,\n",
       "  -0.07646910846233368,\n",
       "  0.04271804913878441,\n",
       "  -0.05762726813554764,\n",
       "  -0.016963819041848183,\n",
       "  -0.01857658103108406,\n",
       "  -0.01413749624043703,\n",
       "  -0.03640083968639374,\n",
       "  -0.06296119093894958,\n",
       "  -0.03274617716670036,\n",
       "  -0.03282701224088669,\n",
       "  -0.019055962562561035,\n",
       "  0.0006133917486295104,\n",
       "  -0.015010559000074863,\n",
       "  -0.01367245614528656,\n",
       "  0.007132526021450758,\n",
       "  -0.02092031016945839,\n",
       "  0.020704345777630806,\n",
       "  -0.018752677366137505,\n",
       "  -0.040584009140729904,\n",
       "  -0.027547046542167664,\n",
       "  0.04514696076512337,\n",
       "  0.030557289719581604,\n",
       "  -0.011933701112866402,\n",
       "  0.03597408905625343,\n",
       "  0.010083063505589962,\n",
       "  -0.0002877057413570583,\n",
       "  -0.0004788573132827878,\n",
       "  0.023176882416009903,\n",
       "  0.023308468982577324,\n",
       "  -0.0026107877492904663,\n",
       "  0.0018977095605805516,\n",
       "  0.019999785348773003,\n",
       "  -0.037110064178705215,\n",
       "  -0.03967622295022011,\n",
       "  0.00387975643388927,\n",
       "  -0.049973201006650925,\n",
       "  -0.0010772454552352428,\n",
       "  0.00791215244680643,\n",
       "  0.023580141365528107,\n",
       "  -0.004786644130945206,\n",
       "  -0.04760264232754707,\n",
       "  -0.03685900196433067,\n",
       "  0.03783272206783295,\n",
       "  0.031366750597953796,\n",
       "  -0.04411963000893593,\n",
       "  0.0741594135761261,\n",
       "  -0.0038505929987877607,\n",
       "  0.013326835818588734,\n",
       "  0.007925854995846748,\n",
       "  0.025081411004066467,\n",
       "  -0.044039320200681686,\n",
       "  0.008435609750449657,\n",
       "  0.00921558029949665,\n",
       "  -0.05278719961643219,\n",
       "  -0.045557159930467606,\n",
       "  0.053848788142204285,\n",
       "  -0.021018508821725845,\n",
       "  0.027172226458787918,\n",
       "  0.010853745974600315,\n",
       "  0.006913543678820133,\n",
       "  0.03855746239423752,\n",
       "  0.0018912142841145396,\n",
       "  0.018027734011411667,\n",
       "  -0.013911700807511806,\n",
       "  0.050547122955322266,\n",
       "  -0.046061377972364426,\n",
       "  0.014988119713962078,\n",
       "  0.03326421603560448,\n",
       "  0.020331429317593575,\n",
       "  0.036984335631132126,\n",
       "  -0.0317947119474411,\n",
       "  0.07429588586091995,\n",
       "  0.05291271209716797,\n",
       "  0.023769162595272064,\n",
       "  -0.06511691957712173,\n",
       "  0.005130297038704157,\n",
       "  -0.00474075973033905,\n",
       "  0.015120149590075016,\n",
       "  -0.04903396591544151,\n",
       "  -0.06856129318475723,\n",
       "  -0.06851539760828018,\n",
       "  0.025062410160899162,\n",
       "  -0.10588806122541428,\n",
       "  0.09256253391504288,\n",
       "  0.0055729541927576065,\n",
       "  -0.003532140515744686,\n",
       "  0.013739173300564289,\n",
       "  0.00193587108515203,\n",
       "  -0.019315505400300026,\n",
       "  -0.08798830956220627,\n",
       "  0.021249426528811455,\n",
       "  0.021978924050927162,\n",
       "  0.06997806578874588,\n",
       "  0.021469293162226677,\n",
       "  -0.026550889015197754,\n",
       "  -0.001852598274126649,\n",
       "  0.0005617402493953705,\n",
       "  0.058356743305921555,\n",
       "  0.07055022567510605,\n",
       "  0.059242356568574905,\n",
       "  0.041461281478405,\n",
       "  -0.03291408345103264,\n",
       "  0.036951567977666855,\n",
       "  -0.015617814846336842,\n",
       "  -0.029058028012514114,\n",
       "  0.010944150388240814,\n",
       "  -0.02272084541618824,\n",
       "  0.07414496690034866,\n",
       "  0.019433068111538887,\n",
       "  0.011850608512759209,\n",
       "  0.0714365541934967,\n",
       "  0.01263298001140356,\n",
       "  -0.014422927051782608,\n",
       "  0.01604372076690197,\n",
       "  -0.03377698361873627,\n",
       "  0.04902036115527153,\n",
       "  -0.033136263489723206,\n",
       "  -0.03112524002790451,\n",
       "  0.01941436529159546,\n",
       "  -0.004876554943621159,\n",
       "  -0.04427129030227661,\n",
       "  -0.028810154646635056,\n",
       "  -0.018174288794398308,\n",
       "  0.02043328993022442,\n",
       "  0.006318078842014074,\n",
       "  0.07013282179832458,\n",
       "  0.013791513629257679,\n",
       "  -0.005739178042858839,\n",
       "  0.0011631728848442435,\n",
       "  -0.06709200888872147,\n",
       "  -0.08026710152626038,\n",
       "  -0.03405291587114334,\n",
       "  -0.014922919683158398,\n",
       "  0.03425868600606918,\n",
       "  -0.05453537404537201,\n",
       "  -0.027073925361037254,\n",
       "  -0.02900816686451435,\n",
       "  -0.011476635001599789,\n",
       "  0.0001508310524513945,\n",
       "  -0.028684241697192192,\n",
       "  0.03308316692709923,\n",
       "  -0.018423525616526604,\n",
       "  0.0007097729248926044,\n",
       "  -0.08087760955095291,\n",
       "  -0.028161242604255676,\n",
       "  0.021162288263440132,\n",
       "  -0.003056034678593278,\n",
       "  -0.022555872797966003,\n",
       "  -0.031904224306344986,\n",
       "  0.023745236918330193,\n",
       "  -0.031033333390951157,\n",
       "  0.06482385843992233,\n",
       "  -0.023301562294363976,\n",
       "  -0.02126469835639,\n",
       "  -0.02696927823126316,\n",
       "  -0.01577676087617874,\n",
       "  0.01969231106340885,\n",
       "  0.007376945111900568,\n",
       "  -0.09317053109407425,\n",
       "  -0.019314877688884735,\n",
       "  0.0640728548169136,\n",
       "  -0.025266582146286964,\n",
       "  0.001976379659026861,\n",
       "  -0.057207364588975906,\n",
       "  -0.004599021747708321,\n",
       "  -0.03162116929888725,\n",
       "  0.0316104032099247,\n",
       "  0.035827118903398514,\n",
       "  0.03616020455956459,\n",
       "  0.05593128874897957,\n",
       "  -0.019368357956409454,\n",
       "  -0.037434469908475876,\n",
       "  0.021378908306360245,\n",
       "  0.025757014751434326,\n",
       "  -0.03109464794397354,\n",
       "  0.026038046926259995,\n",
       "  0.012404631823301315,\n",
       "  -0.008290277794003487,\n",
       "  -0.01809503696858883,\n",
       "  0.04027285799384117,\n",
       "  0.06289487332105637,\n",
       "  0.0025871009565889835,\n",
       "  0.035652101039886475,\n",
       "  -0.018316052854061127,\n",
       "  -0.01448897272348404,\n",
       "  0.019810223951935768,\n",
       "  0.003945631440728903,\n",
       "  0.04054900258779526,\n",
       "  0.03702442720532417,\n",
       "  -0.03777556121349335,\n",
       "  0.08422229439020157,\n",
       "  -0.018984399735927582,\n",
       "  -0.05164032801985741,\n",
       "  0.03070819564163685,\n",
       "  -0.01276211068034172,\n",
       "  -0.006201630458235741,\n",
       "  -0.022140515968203545,\n",
       "  0.05437147989869118,\n",
       "  0.006002671085298061,\n",
       "  -0.029194584116339684,\n",
       "  0.08091852813959122,\n",
       "  0.03900020569562912,\n",
       "  0.10309095680713654,\n",
       "  -0.02905886434018612,\n",
       "  0.07097718119621277,\n",
       "  -0.02578001655638218,\n",
       "  -0.025518562644720078,\n",
       "  -0.059300899505615234,\n",
       "  0.0032402737997472286,\n",
       "  0.021998217329382896,\n",
       "  0.052254755049943924,\n",
       "  0.004742362070828676,\n",
       "  -0.005895677022635937,\n",
       "  -0.00994841381907463,\n",
       "  -0.05619708448648453,\n",
       "  4.225599695928395e-05,\n",
       "  0.060636166483163834,\n",
       "  0.030379869043827057,\n",
       "  -0.0009429012425243855,\n",
       "  -0.004844737704843283,\n",
       "  -6.016112496583295e-33,\n",
       "  -0.04618079960346222,\n",
       "  0.007374955806881189,\n",
       "  -0.03644149750471115,\n",
       "  0.019437305629253387,\n",
       "  -0.07379095256328583,\n",
       "  -0.060941584408283234,\n",
       "  0.027698412537574768,\n",
       "  0.017582355067133904,\n",
       "  -0.0034900952596217394,\n",
       "  0.020589683204889297,\n",
       "  -0.026953397318720818,\n",
       "  -0.013083433732390404,\n",
       "  0.02505055069923401,\n",
       "  0.03527083620429039,\n",
       "  -0.03566139563918114,\n",
       "  -0.00720626674592495,\n",
       "  0.05960851162672043,\n",
       "  0.046305008232593536,\n",
       "  -0.008711792528629303,\n",
       "  0.01502207200974226,\n",
       "  -0.004772905260324478,\n",
       "  -0.002906292211264372,\n",
       "  -0.014634906314313412,\n",
       "  0.05044667050242424,\n",
       "  -0.017108742147684097,\n",
       "  -0.05158542841672897,\n",
       "  -0.006009818986058235,\n",
       "  0.04022005945444107,\n",
       "  0.023837508633732796,\n",
       "  -0.023624228313565254,\n",
       "  -0.033358220010995865,\n",
       "  0.0407700315117836,\n",
       "  0.019346686080098152,\n",
       "  0.10285577923059464,\n",
       "  0.014395886100828648,\n",
       "  0.0996248722076416,\n",
       "  -0.08461800962686539,\n",
       "  -0.055308207869529724,\n",
       "  0.03020126186311245,\n",
       "  -0.012127917259931564,\n",
       "  -0.003446824150159955,\n",
       "  -0.024852650240063667,\n",
       "  -0.001699505839496851,\n",
       "  -0.02335594780743122,\n",
       "  -0.040092382580041885,\n",
       "  0.027659473940730095,\n",
       "  0.002931649098172784,\n",
       "  0.02257867343723774,\n",
       "  0.03393525257706642,\n",
       "  -0.025710662826895714,\n",
       "  -0.04812821000814438,\n",
       "  -0.009335159324109554,\n",
       "  -0.013027475215494633,\n",
       "  -0.01544686034321785,\n",
       "  -0.04845479503273964,\n",
       "  0.007959377951920033,\n",
       "  -0.02542577125132084,\n",
       "  -0.015012616291642189,\n",
       "  -0.08556946367025375,\n",
       "  -0.0362936370074749,\n",
       "  0.011572222225368023,\n",
       "  -0.05121671408414841,\n",
       "  -0.003136245533823967,\n",
       "  0.029842276126146317,\n",
       "  0.031912870705127716,\n",
       "  -0.009959541261196136,\n",
       "  0.09549463540315628,\n",
       "  0.02111556939780712,\n",
       "  -0.03823930770158768,\n",
       "  0.02165547013282776,\n",
       "  0.05592730641365051,\n",
       "  -0.052965305745601654,\n",
       "  -0.047586023807525635,\n",
       "  -0.009961234405636787,\n",
       "  -0.032406870275735855,\n",
       "  -0.0037534593138843775,\n",
       "  -0.0381183847784996,\n",
       "  0.011707177385687828,\n",
       "  0.013741450384259224,\n",
       "  0.0073096430860459805,\n",
       "  -0.05251497030258179,\n",
       "  -0.027591358870267868,\n",
       "  -0.007767823990434408,\n",
       "  -0.015935439616441727,\n",
       "  -0.03256463259458542,\n",
       "  0.0668601244688034,\n",
       "  -0.018894348293542862,\n",
       "  -0.009979898110032082,\n",
       "  -0.05177565664052963,\n",
       "  -0.01325721014291048,\n",
       "  0.008795679546892643,\n",
       "  0.006315896287560463,\n",
       "  0.008098485879600048,\n",
       "  -0.0175518449395895,\n",
       "  -0.02911350131034851,\n",
       "  -0.026219531893730164,\n",
       "  -0.007063419558107853,\n",
       "  -0.05520569533109665,\n",
       "  -0.014855630695819855,\n",
       "  -0.009312598034739494,\n",
       "  -0.024489838629961014,\n",
       "  0.06745757907629013,\n",
       "  -0.05033618584275246,\n",
       "  0.020219657570123672,\n",
       "  0.011306765489280224,\n",
       "  0.02080908976495266,\n",
       "  -0.020326538011431694,\n",
       "  -0.036287903785705566,\n",
       "  -0.030891835689544678,\n",
       "  -0.022973710671067238,\n",
       "  -0.003629399463534355,\n",
       "  0.03330838307738304,\n",
       "  0.0372493639588356,\n",
       "  0.09754011780023575,\n",
       "  -0.008156408555805683,\n",
       "  0.03405601531267166,\n",
       "  0.008312300778925419,\n",
       "  0.007984940893948078,\n",
       "  -0.055183619260787964,\n",
       "  0.033824387937784195,\n",
       "  -0.015346409752964973,\n",
       "  0.07565582543611526,\n",
       "  -0.03801552951335907,\n",
       "  -0.008094160817563534,\n",
       "  -0.0195719376206398,\n",
       "  0.03028901107609272,\n",
       "  0.025195084512233734,\n",
       "  0.09978008270263672,\n",
       "  0.019655944779515266,\n",
       "  -0.03360908851027489,\n",
       "  0.010887408629059792,\n",
       "  0.001647931057959795,\n",
       "  2.407277293059451e-07,\n",
       "  -0.03820173069834709,\n",
       "  -0.048087041825056076,\n",
       "  0.0063595762476325035,\n",
       "  -0.005658369045704603,\n",
       "  0.003916775342077017,\n",
       "  0.017315804958343506,\n",
       "  0.016944818198680878,\n",
       "  0.011756560765206814,\n",
       "  -0.06787029653787613,\n",
       "  0.013556656427681446,\n",
       "  -0.03369634225964546,\n",
       "  -0.036422889679670334,\n",
       "  0.030186744406819344,\n",
       "  -0.06949097663164139,\n",
       "  -0.009950419887900352,\n",
       "  -0.08705012500286102,\n",
       "  0.01513849850744009,\n",
       "  -0.03343057632446289,\n",
       "  -0.016749845817685127,\n",
       "  -0.001868868712335825,\n",
       "  0.01925959251821041,\n",
       "  0.008008732460439205,\n",
       "  -0.015274678356945515,\n",
       "  -0.05661456286907196,\n",
       "  0.01818612962961197,\n",
       "  0.061188265681266785,\n",
       "  0.03015058860182762,\n",
       "  -0.012920758686959743,\n",
       "  -0.016754189506173134,\n",
       "  0.05434548854827881,\n",
       "  -0.01060460414737463,\n",
       "  -0.04356399551033974,\n",
       "  0.019560927525162697,\n",
       "  0.028159314766526222,\n",
       "  0.003815226024016738,\n",
       "  -0.014815130271017551,\n",
       "  0.017312971875071526,\n",
       "  0.0030454350635409355,\n",
       "  -0.030537467449903488,\n",
       "  0.03040619008243084,\n",
       "  -0.040871936827898026,\n",
       "  0.0021369303576648235,\n",
       "  0.042149100452661514,\n",
       "  -0.0912618488073349,\n",
       "  0.05142631754279137,\n",
       "  0.0011028744047507644,\n",
       "  0.0024711531586945057,\n",
       "  0.03778006508946419,\n",
       "  0.03187756985425949,\n",
       "  -0.030424833297729492,\n",
       "  -0.02317078784108162,\n",
       "  -0.00039487346657551825,\n",
       "  -0.00703700864687562,\n",
       "  0.02223513275384903,\n",
       "  0.03192141652107239,\n",
       "  -0.008790923282504082,\n",
       "  0.03109089657664299,\n",
       "  -0.008639027364552021,\n",
       "  0.031256891787052155,\n",
       "  -0.003703097580000758,\n",
       "  -0.008102773688733578,\n",
       "  -0.09655620157718658,\n",
       "  0.011179918423295021,\n",
       "  0.03379802405834198,\n",
       "  -0.0047256844118237495,\n",
       "  0.09322289377450943,\n",
       "  -0.01831728406250477,\n",
       "  1.378832029434874e-34,\n",
       "  -0.006051255390048027,\n",
       "  0.04392144829034805,\n",
       "  -0.029040854424238205,\n",
       "  0.02120848186314106,\n",
       "  0.025822175666689873,\n",
       "  -0.015275879763066769,\n",
       "  -0.00219386862590909,\n",
       "  -0.010607033036649227,\n",
       "  0.05139895901083946,\n",
       "  -0.05211133882403374,\n",
       "  0.02241469733417034])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X1),X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Using HuggingFace Local Pipeline through HuggingFacePipeline for models(inclusive of Embedding models) downloaded & run locally) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2 = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    device=1,\n",
    "    # model_kwargs={'device': 'cuda:1 '},\n",
    "    pipeline_kwargs={\"max_new_tokens\": 256},\n",
    "    # is_decoder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# from torch import Tensor\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# def last_token_pool(last_hidden_states: Tensor,\n",
    "#                  attention_mask: Tensor) -> Tensor:\n",
    "#     left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "#     if left_padding:\n",
    "#         return last_hidden_states[:, -1]\n",
    "#     else:\n",
    "#         sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "#         batch_size = last_hidden_states.shape[0]\n",
    "#         return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "# def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "#     return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "\n",
    "# # Each query must come with a one-sentence instruction that describes the task\n",
    "# task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "# queries = [\n",
    "#     get_detailed_instruct(task, 'how much protein should a female eat'),\n",
    "#     get_detailed_instruct(task, 'summit define')\n",
    "# ]\n",
    "# # No need to add instruction for retrieval documents\n",
    "# documents = [\n",
    "#     \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "#     \"Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n",
    "# ]\n",
    "# input_texts = queries + documents\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "# model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct')\n",
    "\n",
    "# max_length = 4096\n",
    "# # Tokenize the input texts\n",
    "# batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# outputs = model(**batch_dict)\n",
    "# embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# # normalize embeddings\n",
    "# embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "# scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "# print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- halted b)now --test it out later \n",
    "- & move forward with Pinecone vector databasecreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Vector_Search_Db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference-:https://python.langchain.com/v0.1/docs/integrations/vectorstores/pinecone/\n",
    "- Note - Assign appropriate embedding vector dimension in Pinecone based on Hf1_embedding model's word_embedding_dimension initialized in the Sentencetransformers sections incorporated(currently by defualt sets to 768 but can be modified at users interest.) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key='33b70e92-395b-4766-866e-ccdd9fd3e58c')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"langchain-vectordb\"\n",
    "# if index_name not in pc.list_indexes().names():\n",
    "#     pc.create_index(\n",
    "#         name=index_name,\n",
    "#         dimension=768,\n",
    "#         metric=\"cosine\",\n",
    "#         spec=ServerlessSpec(\n",
    "#             cloud='aws', \n",
    "#             region='us-east-1'\n",
    "#         ) \n",
    "#     ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***IMP _STEP IN initializing VectorStore containing Vectors/Embeddings from Documents by applying Hf1_embedding model(Docs_inchunks_fmt)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search=PineconeVectorStore.from_documents(documents=docs_in_chunks_fmt,embedding=HF_embeddings_1,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternate way to Converting given document chunks into DATAFRAME with apprpriate Headers(COLUMNS EXPECTED BY PINECONE I.E \n",
    "id' & 'values')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.011030139401555061, 0.026699990034103394, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.03252822160720825, 0.0005839192308485508, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.007412641774863005, 0.02444685623049736, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.012067509815096855, 0.01388515718281269, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.0021829206962138414, -0.012214593589305878...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             values\n",
       "0  0  [0.011030139401555061, 0.026699990034103394, 0...\n",
       "1  1  [0.03252822160720825, 0.0005839192308485508, -...\n",
       "2  2  [0.007412641774863005, 0.02444685623049736, -0...\n",
       "3  3  [-0.012067509815096855, 0.01388515718281269, -...\n",
       "4  4  [-0.0021829206962138414, -0.012214593589305878..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arr1_vector_data,arr1_vector_index=[],[]\n",
    "# for index1,each_chunk in enumerate(docs_in_chunks_fmt):\n",
    "#     arr1_vector_data.append(HF_embeddings_1.embed_query(each_chunk.page_content))\n",
    "#     \"\"\"MAKE SURE INDEXS ARE IN STRING SINCE KEYS MUST BE UNIQUE to be queried from vector database\"\"\"\n",
    "#     arr1_vector_index.append(str(index1))\n",
    "# data1={\"id\":arr1_vector_index,\"values\":arr1_vector_data}\n",
    "# df1=pd.DataFrame(data = data1)\n",
    "\n",
    "# print(df1.head())\n",
    "# \"\"\"4) Inserting DATAFRAME DF1 into Pincone Vector Database using the HF1_embedding_MODEL for the current document.\"\"\"\n",
    "# index1_obj.upsert_from_dataframe(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) RETRIEVE RESULTS BY QUERYING(based on users' Questionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Similarity Search over the Vector_Document_store Based on The Query_in Original_fmt Passed\n",
    "    - where the Similarity search employs Cosine |||arity to map query_inOriginal_fmt to its assocaited Vectore embeddings in Embedding Space.\n",
    "    -Which inturn returns the corresponding Embedding_doc chunk  in original fmt containing result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_retrieval(query1,top_k=2):\n",
    "    similarity_search_results=doc_search.similarity_search(query1,k=top_k)\n",
    "    return similarity_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Employing LLAMA MODELS over Doc_chunks in embedding space.-\n",
    "- (Analogy w.r.t MU & sIGMA FOR ML MODELS THAT MANDATES NUMERICAL INPUTS POST Pd.getdummies-(1 hot) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1=Ollama(model=\"llama3:latest\")\n",
    "llm2_hf=HuggingFaceEndpoint(endpoint_url=\"microsoft/Phi-3-mini-4k-instruct\",huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),model_kwargs={\"max_token_count\":256},temperature=0.4)\n",
    "chain_qa=load_qa_chain(llm2_hf,chain_type=\"stuff\",verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) RETRIEVAL OF ANSWERS BASED ON THE USER QUERY\n",
    "-***Imp-Note: \"\"\"tHE BELOW relevant_answer_srch_frm_doc_inembed_space_convrtd_to_origfmt IMPLIES IT RETURNS RELAVANT DOCUMENTATION CHUNK FROM THE EMBEDDING SPACE CONTAINING ANSWERS FROM EMBED SPACE CONVERTED TO ORIGINAL FMT SOAS TO FORM PASSAGE FOR INPUT DOCUMENT FOR Q_A_CHAIN\"\"\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_relvant_answer_search=0\n",
    "def Answer_retrieval(query_in_original_fmt):\n",
    "    # encoded_query=HF_embeddings_1.embed_query(query_in_original_fmt)\n",
    "    \"\"\"tHE BELOW relevant_answer_srch_frm_doc_inembed_space_convrtd_to_origfmt IMPLIES IT RETURNS RELAVANT DOCUMENTATION CHUNK FROM THE EMBEDDING SPACE CONTAINING ANSWERS FROM EMBED SPACE CONVERTED TO ORIGINAL FMT SOAS TO FORM PASSAGE FOR INPUT DOCUMENT FOR Q_A_CHAIN\"\"\"\n",
    "    relevant_answer_srch_frm_doc_inembed_space_convrtd_to_origfmt=query_retrieval(query_in_original_fmt)\n",
    "    # global global_relvant_answer_search\n",
    "    # global_relvant_answer_search=relevant_answer_search_from_doc_inembedding_space\n",
    "    print(\"*****\",relevant_answer_srch_frm_doc_inembed_space_convrtd_to_origfmt)\n",
    "        \n",
    "    \"\"\"the above print represents that answer lies in the above doc chunks with ocrresponding id therofre combine thspage contents of the corersponding id fro data frame & build a input documentwith resultant passage(combining ids content)\"\"\"\n",
    "        \n",
    "    # for i in relevant_answer_search_from_doc_inembedding_space:\n",
    "    #     print(i)\n",
    "    answer_response_in_original_fmt=chain_qa.invoke({\"input_documents\":relevant_answer_srch_frm_doc_inembed_space_convrtd_to_origfmt,\"question\":query_in_original_fmt})\n",
    "    return answer_response_in_original_fmt,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Output_answer garnered via OLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_documents': [Document(page_content='Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our serversâ€™ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate\\ndesign details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlowâ€™s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two\\nsets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlowâ€™s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlowâ€™s limitation that arises from separation of training and in-\\nference, Monolithâ€™s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation\\nis to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case ğ‘‚(1)time complexity\\nfor lookups and deletions, and an expected amortized ğ‘‚(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables ğ‘‡0,ğ‘‡1\\nwith different hash functions â„0(ğ‘¥),â„1(ğ‘¥), and an element would\\nbe stored in either one of them. When trying to insert an element\\nğ´intoğ‘‡0, it first attempts to place ğ´atâ„0(ğ´); Ifâ„0(ğ´)is occupied\\nby another element ğµ, it would evict ğµfromğ‘‡0and try inserting ğµ\\nintoğ‘‡1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;\\n(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;', metadata={'page': 2.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'}),\n",
       "   Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'})],\n",
       "  'question': 'Provide abstract of the research_paper',\n",
       "  'output_text': 'Here is an abstract of the research paper:\\n\\nThe Monolith online training architecture is designed to address the challenges of large-scale recommendation systems. The authors observe that storing embeddings for IDs with low occurrences can drain memory in vain, and that stale IDs from a distant history seldom contribute to the current model. They propose several feature ID filtering heuristics to improve memory efficiency, including filtering by occurrences and using a probabilistic filter. They also design Monolith to address non-stationary distribution of user data, known as Concept Drift, by updating serving models in real-time with online training. The authors demonstrate that Monolith consistently outperforms systems that adopt hash-tricks with collisions while achieving state-of-the-art results with roughly similar memory usage.'},)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query1=input(\"enter desired query pertained to the doument uploaded\")\n",
    "output_answer1=Answer_retrieval(user_query)\n",
    "output_answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. output_answer garnered via HuggingFacE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** [Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'}), Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\nâ€¢Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;â€¢Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5â„ğ‘Ÿ,1â„ğ‘Ÿ, and 30ğ‘šğ‘–ğ‘› respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66Â±0.020 79 .42Â±0.026\\n1 hr 79.78Â±0.005 79 .44Â±0.030\\n30 min 79.80Â±0.008 79 .43Â±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.', metadata={'page': 7.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'})]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "Data \n",
      "(Batch Training) \n",
      "Data \n",
      "(Online Training) \n",
      "Training \n",
      "Worker \n",
      "Training PS \n",
      " Serving PS \n",
      "Model \n",
      "Server \n",
      "User \n",
      "Batch \n",
      "Training \n",
      "Stage \n",
      "Online \n",
      "Training \n",
      "Stage \n",
      "Historical batch data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply gradient \n",
      "updates \n",
      "Online streaming \n",
      "data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply \n",
      "gradient \n",
      "updates \n",
      "Parameter \n",
      "Sync \n",
      "Parameter \n",
      "Sync Sync \n",
      "interval \n",
      "User Request \n",
      "Feature IDs \n",
      "Feature \n",
      "embeddings Embedding \n",
      "table lookup \n",
      "Ranking Result Model \n",
      "forward \n",
      "pass \n",
      "User Actions \n",
      "Data of \n",
      "features and \n",
      " user reactions \n",
      "Figure 1: Monolith Online Training Architecture.\n",
      "practice of mapping them to a high-dimensional embedding space\n",
      "would give rise to a series of issues:\n",
      "â€¢Unlike language models where number of word-pieces are\n",
      "limited, the amount of users and ranking items are orders of\n",
      "magnitude larger. Such an enormous embedding table would\n",
      "hardly fit into single host memory;\n",
      "â€¢Worse still, the size of embedding table is expected to grow\n",
      "over time as more users and items are admitted, while frame-\n",
      "works like [ 1,17] uses a fixed-size dense variables to repre-\n",
      "sent embedding table.\n",
      "In practice, many systems adopt low-collision hashing [ 3,6] as a\n",
      "way to reduce memory footprint and to allow growing of IDs. This\n",
      "relies on an over-idealistic assumption that IDs in the embedding\n",
      "table is distributed evenly in frequency, and collisions are harmless\n",
      "to the model quality. Unfortunately this is rarely true for a real-\n",
      "world recommendation system, where a small group of users or\n",
      "items have significantly more occurrences. With the organic growth\n",
      "of embedding table size, chances of hash key collision increases\n",
      "and lead to deterioration of model quality [3].\n",
      "Therefore it is a natural demand for production-scale recommen-\n",
      "dation systems to have the capacity to capture as many features in\n",
      "its parameters, and also have the capability of elastically adjusting\n",
      "the number of users and items it tries to book-keep.1.2 Non-stationary Distribution\n",
      "Visual and linguistic patterns barely develop in a time scale of\n",
      "centuries, while the same user interested in one topic could shift\n",
      "their zeal every next minute. As a result, the underlying distribution\n",
      "of user data is non-stationary, a phenomenon commonly referred\n",
      "to as Concept Drift [8].\n",
      "Intuitively, information from a more recent history can more\n",
      "effectively contribute to predicting the change in a userâ€™s behavior.\n",
      "To mitigate the effect of Concept Drift, serving models need to be\n",
      "updated from new user feedback as close to real-time as possible to\n",
      "reflect the latest interest of a user.\n",
      "In light of these distinction and in observation of issues that arises\n",
      "from our production, we designed Monolith , a large-scale recom-\n",
      "mendation system to address these pain-points. We did extensive\n",
      "experiments to verify and iterate our design in the production\n",
      "environment. Monolith is able to\n",
      "(1)Provide full expressive power for sparse features by design-\n",
      "ing a collisionless hash table and a dynamic feature eviction\n",
      "mechanism;\n",
      "(2)Loop serving feedback back to training in real-time with\n",
      "online training.\n",
      "Empowered by these architectural capacities, Monolith consis-\n",
      "tently outperforms systems that adopts hash-tricks with collisions\n",
      "with roughly similar memory usage, and achieves state-of-the-art\n",
      "\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "0 10 20 30 40 50\n",
      "Hours0.7920.7940.7960.798AUCw/ online training\n",
      "w/o online training\n",
      "(a) Online training with 5hrs sync interval\n",
      "0 10 20 30 40 50\n",
      "Hours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\n",
      "w/o online training (b) Online training with 1hr sync interval\n",
      "0 10 20 30 40 50\n",
      "Hours0.7900.7950.8000.8050.810AUCw/ online training\n",
      "w/o online training\n",
      "(c) Online training with 30min sync interval\n",
      "Figure 9: Online training v.s. Batch training on Criteo dataset.\n",
      "Blue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\n",
      "3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\n",
      "covered that a higher parameter synchronization frequency is al-\n",
      "ways conducive to improving online serving AUC, and that online\n",
      "serving models are more tolerant with loss of a few shard of PS\n",
      "than we expect.\n",
      "(1)The Effect of Parameter Synchronization Frequency .\n",
      "In our online streaming training experiment (1) with Criteo\n",
      "Display Ads Challenge dataset, model quality consistently\n",
      "improves with the increase of parameter synchronization\n",
      "frequency, as is evident by comparison from two perspec-\n",
      "tives:\n",
      "â€¢Models with online training performs better than models\n",
      "without. Figure 9a, 9b, 9c compares AUC of online training\n",
      "models evaluated by the following shard of data versus\n",
      "batch training models evaluated by each shard of data;â€¢Models with smaller parameter synchronization interval\n",
      "performs better that those with larger interval. Figure 10\n",
      "and Table 2 compares online serving AUC for models with\n",
      "sync interval of 5â„ğ‘Ÿ,1â„ğ‘Ÿ, and 30ğ‘šğ‘–ğ‘› respectively.\n",
      "Sync Interval Average AUC (online) Average AUC (batch)\n",
      "5 hr 79.66Â±0.020 79 .42Â±0.026\n",
      "1 hr 79.78Â±0.005 79 .44Â±0.030\n",
      "30 min 79.80Â±0.008 79 .43Â±0.025\n",
      "Table 2: Average AUC comparison for DeepFM model on\n",
      "Criteo dataset.\n",
      "\n",
      "Question: provide abstract of the research paper in 1 paragraph.\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_documents': [Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'}),\n",
       "   Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\nâ€¢Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;â€¢Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5â„ğ‘Ÿ,1â„ğ‘Ÿ, and 30ğ‘šğ‘–ğ‘› respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66Â±0.020 79 .42Â±0.026\\n1 hr 79.78Â±0.005 79 .44Â±0.030\\n30 min 79.80Â±0.008 79 .43Â±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.', metadata={'page': 7.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'})],\n",
       "  'question': 'provide abstract of the research paper in 1 paragraph.',\n",
       "  'output_text': '\\nIn this work, we present Monolith, a large-scale recommendation system designed to address the challenges of memory footprint and non-stationary distribution in recommendation systems. Monolith provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. It also incorporates online training to loop serving feedback back to training in real-time, allowing it to adapt to non-stationary user behavior. The system outperforms systems that adopt hash-tricks with collisions, achieving state-of-the-art results with similar memory usage.\\n\\nAnswer:\\nThe paper introduces Monolith, a large-scale recommendation system that addresses memory footprint and non-stationary distribution challenges. Monolith uses a collisionless hash table and dynamic feature eviction mechanism to handle sparse features, and incorporates online training to adapt to changing user behavior. The system outperforms systems with hash-tricks with collisions, achieving state-of-the-art results with similar memory usage.\\n\\n\\nAnswer:\\nMonolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems with hash-tricks with collisions and achieving state-of-the-art results with similar memory usage.\\n\\nAnswer:\\nMonolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems with hash-tricks with collisions and achieving state-of-the-art results with similar memory usage.\\n\\nAnswer:\\nMonolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems that adopt hash-tricks'},)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query2=input(\"enter desired query pertained to the doument uploaded\")\n",
    "output_answer2=Answer_retrieval(user_query)\n",
    "\n",
    "output_answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this work, we present Monolith, a large-scale recommendation system designed to address the challenges of memory footprint and non-stationary distribution in recommendation systems. Monolith provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. It also incorporates online training to loop serving feedback back to training in real-time, allowing it to adapt to non-stationary user behavior. The system outperforms systems that adopt hash-tricks with collisions, achieving state-of-the-art results with similar memory usage.\\n\\nAnswer:\\nThe paper introduces Monolith, a large-scale recommendation system that addresses memory footprint and non-stationary distribution challenges. Monolith uses a collisionless hash table and dynamic feature eviction mechanism to handle sparse features, and incorporates online training to adapt to changing user behavior. The system outperforms systems with hash-tricks with collisions, achieving state-of-the-art results with similar memory usage.\\n\\n\\nAnswer:\\nMonolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems with hash-tricks with collisions and achieving state-of-the-art results with similar memory usage.\\n\\nAnswer:\\nMonolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems with hash-tricks with collisions and achieving state-of-the-art results with similar memory usage.\\n\\nAnswer:\\nMonolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems that adopt hash-tricks'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Resultant output format\"\"\"\n",
    "output_answer2[0][\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) inclusive Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\SANJEEV\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# llm1=Ollama(model=\"llama3:latest\")\n",
    "llm2_hf=HuggingFaceEndpoint(endpoint_url=\"microsoft/Phi-3-mini-4k-instruct\",huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),model_kwargs={\"max_token_count\":256},temperature=0.4)\n",
    "# prompt = \n",
    "\"\"\"Address this by passing context as variable in prompt searched by document(created based on query provided to return contexts pertained docs.) \n",
    "error -document_variable_name context was not found in llm_chain input_variables: [] (type=value_error)\"\"\"\n",
    "prompt1 = ChatPromptTemplate.from_messages([\"human\",\"Behave as a Responsible AI assistant & respond in 3 lines. for {context}\"])\n",
    "# chain1=prompt1|llm1\n",
    "chain_qa=load_qa_chain(llm1,chain_type=\"stuff\",verbose=True, prompt=prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** [Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'}), Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'})]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: human\n",
      "Human: Behave as a Responsible AI assistant & respond in 3 lines. for ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "Data \n",
      "(Batch Training) \n",
      "Data \n",
      "(Online Training) \n",
      "Training \n",
      "Worker \n",
      "Training PS \n",
      " Serving PS \n",
      "Model \n",
      "Server \n",
      "User \n",
      "Batch \n",
      "Training \n",
      "Stage \n",
      "Online \n",
      "Training \n",
      "Stage \n",
      "Historical batch data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply gradient \n",
      "updates \n",
      "Online streaming \n",
      "data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply \n",
      "gradient \n",
      "updates \n",
      "Parameter \n",
      "Sync \n",
      "Parameter \n",
      "Sync Sync \n",
      "interval \n",
      "User Request \n",
      "Feature IDs \n",
      "Feature \n",
      "embeddings Embedding \n",
      "table lookup \n",
      "Ranking Result Model \n",
      "forward \n",
      "pass \n",
      "User Actions \n",
      "Data of \n",
      "features and \n",
      " user reactions \n",
      "Figure 1: Monolith Online Training Architecture.\n",
      "practice of mapping them to a high-dimensional embedding space\n",
      "would give rise to a series of issues:\n",
      "â€¢Unlike language models where number of word-pieces are\n",
      "limited, the amount of users and ranking items are orders of\n",
      "magnitude larger. Such an enormous embedding table would\n",
      "hardly fit into single host memory;\n",
      "â€¢Worse still, the size of embedding table is expected to grow\n",
      "over time as more users and items are admitted, while frame-\n",
      "works like [ 1,17] uses a fixed-size dense variables to repre-\n",
      "sent embedding table.\n",
      "In practice, many systems adopt low-collision hashing [ 3,6] as a\n",
      "way to reduce memory footprint and to allow growing of IDs. This\n",
      "relies on an over-idealistic assumption that IDs in the embedding\n",
      "table is distributed evenly in frequency, and collisions are harmless\n",
      "to the model quality. Unfortunately this is rarely true for a real-\n",
      "world recommendation system, where a small group of users or\n",
      "items have significantly more occurrences. With the organic growth\n",
      "of embedding table size, chances of hash key collision increases\n",
      "and lead to deterioration of model quality [3].\n",
      "Therefore it is a natural demand for production-scale recommen-\n",
      "dation systems to have the capacity to capture as many features in\n",
      "its parameters, and also have the capability of elastically adjusting\n",
      "the number of users and items it tries to book-keep.1.2 Non-stationary Distribution\n",
      "Visual and linguistic patterns barely develop in a time scale of\n",
      "centuries, while the same user interested in one topic could shift\n",
      "their zeal every next minute. As a result, the underlying distribution\n",
      "of user data is non-stationary, a phenomenon commonly referred\n",
      "to as Concept Drift [8].\n",
      "Intuitively, information from a more recent history can more\n",
      "effectively contribute to predicting the change in a userâ€™s behavior.\n",
      "To mitigate the effect of Concept Drift, serving models need to be\n",
      "updated from new user feedback as close to real-time as possible to\n",
      "reflect the latest interest of a user.\n",
      "In light of these distinction and in observation of issues that arises\n",
      "from our production, we designed Monolith , a large-scale recom-\n",
      "mendation system to address these pain-points. We did extensive\n",
      "experiments to verify and iterate our design in the production\n",
      "environment. Monolith is able to\n",
      "(1)Provide full expressive power for sparse features by design-\n",
      "ing a collisionless hash table and a dynamic feature eviction\n",
      "mechanism;\n",
      "(2)Loop serving feedback back to training in real-time with\n",
      "online training.\n",
      "Empowered by these architectural capacities, Monolith consis-\n",
      "tently outperforms systems that adopts hash-tricks with collisions\n",
      "with roughly similar memory usage, and achieves state-of-the-art\n",
      "\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "Data \n",
      "(Batch Training) \n",
      "Data \n",
      "(Online Training) \n",
      "Training \n",
      "Worker \n",
      "Training PS \n",
      " Serving PS \n",
      "Model \n",
      "Server \n",
      "User \n",
      "Batch \n",
      "Training \n",
      "Stage \n",
      "Online \n",
      "Training \n",
      "Stage \n",
      "Historical batch data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply gradient \n",
      "updates \n",
      "Online streaming \n",
      "data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply \n",
      "gradient \n",
      "updates \n",
      "Parameter \n",
      "Sync \n",
      "Parameter \n",
      "Sync Sync \n",
      "interval \n",
      "User Request \n",
      "Feature IDs \n",
      "Feature \n",
      "embeddings Embedding \n",
      "table lookup \n",
      "Ranking Result Model \n",
      "forward \n",
      "pass \n",
      "User Actions \n",
      "Data of \n",
      "features and \n",
      " user reactions \n",
      "Figure 1: Monolith Online Training Architecture.\n",
      "practice of mapping them to a high-dimensional embedding space\n",
      "would give rise to a series of issues:\n",
      "â€¢Unlike language models where number of word-pieces are\n",
      "limited, the amount of users and ranking items are orders of\n",
      "magnitude larger. Such an enormous embedding table would\n",
      "hardly fit into single host memory;\n",
      "â€¢Worse still, the size of embedding table is expected to grow\n",
      "over time as more users and items are admitted, while frame-\n",
      "works like [ 1,17] uses a fixed-size dense variables to repre-\n",
      "sent embedding table.\n",
      "In practice, many systems adopt low-collision hashing [ 3,6] as a\n",
      "way to reduce memory footprint and to allow growing of IDs. This\n",
      "relies on an over-idealistic assumption that IDs in the embedding\n",
      "table is distributed evenly in frequency, and collisions are harmless\n",
      "to the model quality. Unfortunately this is rarely true for a real-\n",
      "world recommendation system, where a small group of users or\n",
      "items have significantly more occurrences. With the organic growth\n",
      "of embedding table size, chances of hash key collision increases\n",
      "and lead to deterioration of model quality [3].\n",
      "Therefore it is a natural demand for production-scale recommen-\n",
      "dation systems to have the capacity to capture as many features in\n",
      "its parameters, and also have the capability of elastically adjusting\n",
      "the number of users and items it tries to book-keep.1.2 Non-stationary Distribution\n",
      "Visual and linguistic patterns barely develop in a time scale of\n",
      "centuries, while the same user interested in one topic could shift\n",
      "their zeal every next minute. As a result, the underlying distribution\n",
      "of user data is non-stationary, a phenomenon commonly referred\n",
      "to as Concept Drift [8].\n",
      "Intuitively, information from a more recent history can more\n",
      "effectively contribute to predicting the change in a userâ€™s behavior.\n",
      "To mitigate the effect of Concept Drift, serving models need to be\n",
      "updated from new user feedback as close to real-time as possible to\n",
      "reflect the latest interest of a user.\n",
      "In light of these distinction and in observation of issues that arises\n",
      "from our production, we designed Monolith , a large-scale recom-\n",
      "mendation system to address these pain-points. We did extensive\n",
      "experiments to verify and iterate our design in the production\n",
      "environment. Monolith is able to\n",
      "(1)Provide full expressive power for sparse features by design-\n",
      "ing a collisionless hash table and a dynamic feature eviction\n",
      "mechanism;\n",
      "(2)Loop serving feedback back to training in real-time with\n",
      "online training.\n",
      "Empowered by these architectural capacities, Monolith consis-\n",
      "tently outperforms systems that adopts hash-tricks with collisions\n",
      "with roughly similar memory usage, and achieves state-of-the-art\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_documents': [Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'}),\n",
       "   Document(page_content='ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\nâ€¢Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\nâ€¢Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a userâ€™s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-art', metadata={'page': 1.0, 'source': 'G:\\\\CORE-3\\\\OPEN_AI_1\\\\VECTOR_DB_SEARCH\\\\Monolith_Real Time Recommendation System_BD.pdf'})],\n",
       "  'question': 'provide abstract of the research paper',\n",
       "  'output_text': 'Here are three-line responses as a Responsible AI assistant:\\n\\n1. \"I understand your concern about the scalability of embedding tables in large-scale recommendation systems. Can I help you explore alternative solutions to tackle this issue?\"\\n2. \"I appreciate your emphasis on the importance of real-time updating serving models to mitigate Concept Drift. How can I assist you in designing an online training framework that suits your needs?\"\\n3. \"Your request for a large-scale recommendation system with elastic capacity to capture features and adapt to changing user behaviors resonates with me. Can I help you evaluate Monolith\\'s capabilities in addressing these pain points?\"'},)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query3=input(\"enter desired query pertained to the doument uploaded\")\n",
    "output_answer3=Answer_retrieval(user_query3)\n",
    "output_answer3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------THE END--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough_Work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provide abstract of the research paper in 1 paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In this work, we present Monolith, a large-scale recommendation system designed to address the challenges of memory footprint and non-stationary distribution in recommendation systems. Monolith provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. It also incorporates online training to loop serving feedback back to training in real-time, allowing it to adapt to non-stationary user behavior. The system outperforms systems that adopt hash-tricks with collisions, achieving state-of-the-art results with similar memory usage.\n",
      "\n",
      "Answer:\n",
      "The paper introduces Monolith, a large-scale recommendation system that addresses memory footprint and non-stationary distribution challenges. Monolith uses a collisionless hash table and dynamic feature eviction mechanism to handle sparse features, and incorporates online training to adapt to changing user behavior. The system outperforms systems with hash-tricks with collisions, achieving state-of-the-art results with similar memory usage.\n",
      "\n",
      "\n",
      "Answer:\n",
      "Monolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems with hash-tricks with collisions and achieving state-of-the-art results with similar memory usage.\n",
      "\n",
      "Answer:\n",
      "Monolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems with hash-tricks with collisions and achieving state-of-the-art results with similar memory usage.\n",
      "\n",
      "Answer:\n",
      "Monolith is a large-scale recommendation system designed to address memory footprint and non-stationary distribution challenges in recommendation systems. It provides full expressive power for sparse features by designing a collisionless hash table and a dynamic feature eviction mechanism. The system also incorporates online training to adapt to non-stationary user behavior, outperforming systems that adopt hash-tricks\n"
     ]
    }
   ],
   "source": [
    "for  i in output_answer2:\n",
    "   print(i[\"output_text\"])\n",
    "   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employ eval to evaluate expression insode output_answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Rough test\"\n",
    "# t = 'langchain embedding'\n",
    "\n",
    "# HF_embed_model_object = HuggingFaceEmbeddings(encode_kwargs={\"normalize_embeddings\": True})\n",
    "# # SentenceTransformer embeddings\n",
    "# Sent_trans_Embed_test = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\").encode(t, normalize_embeddings=True)\n",
    "# # Langchain.Huggingface embeddings\n",
    "# HF_embed_test = HF_embed_model_object.embed_query(t)\n",
    "# # z=x.embed_query(t)\n",
    "# print(y==x)\n",
    "# \"\"\"the above is true since inherently m uses x to convert the given text into embeddings\"\"\"\n",
    "# INFERENCE-:Tha bove HF_embeddings is |||AR TO SentenceTransformer.(model_name=\"sentence-transformers/all-mpnet-base-v2\").ENCODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
