{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import HuggingFaceHub,Ollama\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRELIMINARY_SUMMARY-\n",
    " 1. PromptTemplate & ChatPromptTemplate uses same both execution techniques(i.e both model method & invoke) \n",
    "    - but slightly differs in construction of prompt(i.e prompt1=PromptTemplate() & prompt2=ChatPromptTemplate.from_message(list()) or .from_template()\n",
    "\n",
    "2. **Ref-NOTE: https://python.langchain.com/v0.1/docs/use_cases/summarization/**\n",
    "3. StuffDocumentaionChain\n",
    "4.\"WITH CORRESPONDING CUSTOM CHUNK PROMPTS & COMBINED WHOLE DOC PROMPT \"\n",
    "5. MapReduce     USING map_prompt-chunk_prompt1 combined_prompt=whole_doc_prompt\n",
    "6. Refine Method USING refine_prompt-chunk_prompt2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1). Execution using PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANJEEV\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm1=Ollama(model=\"llama3:latest\")\n",
    "\n",
    "template1=\"provide brief overview on {text1}with word limit of {text2} and para to {text3}\"\n",
    "prompt1=PromptTemplate(input_variables=[\"text1\", \"text2\", \"text3\"],template=template1)\n",
    "llmchain1=LLMChain(llm=llm1,prompt=prompt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1,input2,input3=input(\"enter text1:\"),input(\"enter text2:\"),input(\"enter text3:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method-1=> model(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANJEEV\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is a brief overview of Furious with a word count of 20:\\n\\nFurious is an American action comedy film directed by James Wan. It stars Vin Diesel, Michelle Rodriguez, Tyrese Gibson, Chris \"Ludacris\" Bridges, Matt Gallant, Sung Kang, Gal Gadot, and Dwayne Johnson. The movie follows Dominic Toretto (Diesel) as he reunites with his crew to take down a ruthless businessman, who has targeted them for revenge.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myfunc(input1,input2,input3 ):\n",
    "\n",
    "    llm1=Ollama(model=\"llama3:latest\")\n",
    "\n",
    "    template1=\"provide brief overview on {text1}with word limit of {text2} and para to {text3}\"\n",
    "    prompt1=PromptTemplate(input_variables=[\"text1\", \"text2\", \"text3\"],template=template1)\n",
    "    # llmchain1=LLMChain(llm=llm1,prompt=prompt1)\n",
    "    x=llm1(prompt1.format(text1=input1,text2=input2,text3=input3))\n",
    "\n",
    "    return x\n",
    "\n",
    "# input2=input(\"enter text2:\")\n",
    "# input3=input(\"enter text3:\")\n",
    "output0=myfunc(input1,input2,input3)\n",
    "output0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method-2 Using LLMChain or chain | operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) using llmchain--might soon be deprecated but same formati.e (Input dictionary(containing multiple Key-Value pairs) is used to to call multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANJEEV\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text1': 'furious',\n",
       " 'text2': '20',\n",
       " 'text3': '1',\n",
       " 'text': 'Here is a brief overview of Furious with a 20-word limit and one paragraph:\\n\\n**Overview:** Fast-paced action film about Dominic Toretto (Vin Diesel) and his crew\\'s high-stakes heists.\\n\\n\"Ride or die,\" the Furious franchise motto, sums up the adrenaline-fueled adventures of Dom and his loyal crew as they outrun and outgun their foes in a series of high-octane heists.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  llmchain1=LLMChain(llm=llm1,prompt=prompt1)\n",
    "\n",
    "# input1=input(\"enter input1\")\n",
    "# input2=input(\"enter input2\")\n",
    "# input3=input(\"enter input3\")\n",
    "\"\"\"https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain\"\"\"\n",
    "\"\"\"for parameter inputs-Dictionary of inputs, or single input if chain expects only one param. Should contain all inputs specified in Chain.input_keys except for inputs that will be set by the chain’s memory.\"\"\"\n",
    "output1=llmchain1(inputs={\"text1\":input1,\"text2\":input2,\"text3\":input3})\n",
    "output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) using CHAIN | Operator(As Replacement to a) )\n",
    "- Holds same format for input parsing as above i.e (Input dictionary(containing multiple Key-Value pairs)) is used to to call multiple inputs\n",
    "where invoke implicitly calls inputs dictionary to call its input key-value pairs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['text1', 'text2', 'text3'], template='provide brief overview on {text1}with word limit of {text2} and para to {text3}')\n",
       "| Ollama(model='llama3:latest')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_chain=prompt1 |llm1\n",
    "alt_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a brief overview of Furious:\\n\\n**Overview (20 words)**\\nFurious: Fast & Furious franchise film, action-packed thriller with high-octane racing, heists, and family.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2=alt_chain.invoke({\"text1\":input1,\"text2\":input2,\"text3\":input3})\n",
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE:Number of tokens in output2 using the model  & can be perceived as prerequisite feature to LIMIT THE MAX TOKENS IF oN tOKEN CRUNCHFOR THE CURRENT BILLING PERIOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the number of tokens in the output using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm1.get_num_tokens(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s the translation:\\n\\n**ஒரு குறிப்பு (20 செய்தி)**\\n\\nஃஃ: Fast & Furious வழக்குறி, அதிருட்சி நிகண்ட் திரைப்படம், உயர்ந்த-ஒக்ட்னே ஓட்டப்பாடியும், ஏட்சும் மற்றும் குடும்பங்கள்.\\n\\nNote: I\\'ve translated \"high-octane\" to \"உயர்ந்த-ஒக்ட்னே\" which is a direct translation of the phrase, but it may not be as idiomatic in Tamil. A more natural translation would be something like \"உயர்ந்த விரிச்சி\" or \"அதிருட்சி ஓட்டப்பாடியும்\". Let me know if you\\'d like me to make any changes!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input=\"translate \"+output2+\" to {lang} \"\n",
    "prompt2=PromptTemplate(input_variables=[\"lang\",\"pronun_lang\"],template=new_input)\n",
    "input11=input(\"enter translational language to be converted into \")\n",
    "# input22=input(\"enter pronunciation_lang\")\n",
    "llm1(prompt2.format(lang=input11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Execution using ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input21=input(\"enter the topic of conversation\")\n",
    "input22=input(\"enter word limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOTE:- by default all input message fromat are enclosed in list except (.from _template version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a)Using Model method-1 to retrieve response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a1.) Using Default Baseline message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! However, I need more information from you.\\n\\nPlease provide the text ({text21}) and the word limit ({text22}), and I'll be delighted to assist you in crafting a brief overview within that limit.\\n\\nRemember, I'm your AI Sonnet assistant, here to help you shine with well-crafted content!\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt1_a=ChatPromptTemplate.from_messages([HumanMessage(content=\"provide brief overview on {text21}with word limit of {text22} .\")\n",
    "                                              ,SystemMessage(content=\"Nehave as Ai Sonnet assistant \")])\n",
    "\n",
    "llm1(chat_prompt1_a.format(text21=input21,text22=input22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a2.) Using Mixed message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A gun, a tool for hunting and defense,\\nWith metal and fire, it can take its offense.\\nA device for precision, or so they claim,\\nBut often used for harm, causing endless pain.\\n\\n(Note: I've kept the overview within the 10-word limit as per your request!)\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt1_b=ChatPromptTemplate.from_messages([(\"human\",\"provide brief overview on {text21}with word limit of {text22} .\"),\n",
    "                                              SystemMessage(content=\"Behave as AI Sonnet assistant \")])\n",
    "\n",
    "llm1(chat_prompt1_b.format(text21=input21,text22=input22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a3.) Using Default tuple based message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A request for a brief overview on guns!\\n\\nWithin the word limit of 10 words, here\\'s my attempt:\\n\\n\"Guns are firearms designed for hunting, self-defense, or sport.\"'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt1_c=ChatPromptTemplate.from_messages([(\"human\",\"provide brief overview on {text21}with word limit of {text22} .\"),\n",
    "                                              (\"system\",\"Behave as AI Sonnet assistant \")])\n",
    "\n",
    "llm1(chat_prompt1_c.format(text21=input21,text22=input22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a4.) Using TEMPLATE Based message format(by Default assumes input from Human Message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a brief overview of guns within the 10-word limit:\\n\\n\"A firearm is a portable device for shooting projectiles.\"'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt1_d=ChatPromptTemplate.from_template(\"provide brief overview on {text21}with word limit of {text22} .\")\n",
    "\n",
    "llm1(chat_prompt1_d.format(text21=input21,text22=input22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Using INVOKE method-2 to retrieve response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b1.) Using Default Baseline message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm happy to help! However, I need more information from you. It seems like you want me to provide a brief overview of some text, but you haven't specified what that text is.\\n\\nCould you please provide the text you'd like me to summarize, and also specify the word limit for the summary? Once I have that information, I'll be happy to assist you!\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt2_a=ChatPromptTemplate.from_messages([HumanMessage(content=\"provide brief overview on {text21}with word limit of {text22} .\")\n",
    "                                              ,SystemMessage(content=\"Nehave as Ai Sonnet assistant \")])\n",
    "chain_chat1_prompt_a=chat_prompt2_a |llm1\n",
    "\n",
    "chain_chat1_prompt_a.invoke({\"text21\":input21,\"text22\":input22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b2.) Using Mixed message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A gun, a tool of might,\\nFirearms for defense or delight.\\n\\n(Note: Within the 10-word limit, I've provided a brief overview of a gun)\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt2_b=ChatPromptTemplate.from_messages([(\"human\",\"provide brief overview on {text21}with word limit of {text22} .\"),\n",
    "                                              SystemMessage(content=\"Behave as AI Sonnet assistant \")])\n",
    "chain_chat1_prompt_b=chat_prompt2_b |llm1\n",
    "\n",
    "chain_chat1_prompt_b.invoke({\"text21\":input21,\"text22\":input22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b3.) Using Default tuple based message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a brief overview of guns within the 10-word limit:\\n\\n\"Guns are firearms designed for hunting, sport, or self-defense purposes.\"'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt2_c=ChatPromptTemplate.from_messages([(\"human\",\"provide brief overview on {text21}with word limit of {text22} .\"),\n",
    "                                              (\"system\",\"Behave as AI Sonnet assistant \")])\n",
    "chain_chat1_prompt_c=chat_prompt2_c |llm1\n",
    "\n",
    "chain_chat1_prompt_c.invoke({\"text21\":input21,\"text22\":input22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b4.) Using TEMPLATE Based message format(by Default assumes input from Human Message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a brief overview of a gun in 10 words:\\n\\n\"A firearm, typically made of metal and wood, for hunting/shooting.\"'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt1_d=ChatPromptTemplate.from_template(\"provide brief overview on {text21}with word limit of {text22} .\")\n",
    "chain_chat1_prompt_d=chat_prompt1_d |llm1\n",
    "\n",
    "chain_chat1_prompt_d.invoke({\"text21\":input21,\"text22\":input22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3). StuffDocumentaionChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_read=PdfReader(\"G:\\CORE-3\\OPEN_AI_1\\TEXT_SUMMARIZATION_IMPLEMENTATION\\Monolith_Real Time Recommendation System_BD.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such\\nframeworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.\\n∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\\nZhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural\\nfit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical\\nfeatures, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\n•Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\n•Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a user’s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-artMonolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our servers’ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate\\ndesign details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlow’s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two\\nsets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlow’s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlow’s limitation that arises from separation of training and in-\\nference, Monolith’s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation\\nis to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case 𝑂(1)time complexity\\nfor lookups and deletions, and an expected amortized 𝑂(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables 𝑇0,𝑇1\\nwith different hash functions ℎ0(𝑥),ℎ1(𝑥), and an element would\\nbe stored in either one of them. When trying to insert an element\\n𝐴into𝑇0, it first attempts to place 𝐴atℎ0(𝐴); Ifℎ0(𝐴)is occupied\\nby another element 𝐵, it would evict 𝐵from𝑇0and try inserting 𝐵\\ninto𝑇1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;\\n(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nUser \\n Log Kafka \\nActions \\n(Click, Like, \\nConvert …) \\nFeature Kafka \\n Model Server \\n Training Worker \\nTraining PS \\n Serving PS \\nJoiner \\nFlink Job \\nTraining Example \\nKafka \\nBatch Training Data \\nHDFS \\nGenerate \\nFeatures \\nParameter Synchronization \\nDump Batch Data \\nOnline \\nTraining \\nBatch Training \\nFigure 4: Streaming Engine.\\nThe information feedback loop from [User →Model Server→Training Worker→Model Server→User] would spend a long time when taking the Batch Training\\npath, while the Online Training will close the loop more instantly.\\n(2)IDs are timed and set to expire after being inactive for a\\npredefined period of time. The expire time is also tunable for\\neach embedding table to allow for distinguishing features\\nwith different sensitivity to historical information.\\nIn our implementation, HashTable is implemented as a Tensor-\\nFlow resource operation. Similar to Variable , look-ups and updates\\nare also implemented as native TensorFlow operations for easier\\nintegration and better compatibility.\\n2.2 Online Training\\nIn Monolith, training is divided into two stages (Figure 1):\\n(1)Batch training stage. This stage works as an ordinary Tensor-\\nFlow training loop: In each training step, a training worker\\nreads one mini-batch of training examples from the stor-\\nage, requests parameters from PS, computes a forward and\\na backward pass, and finally push updated parameters to\\nthe training PS. Slightly different from other common deep\\nlearning tasks, we only train our dataset for one pass. Batch\\ntraining is useful for training historical data when we modify\\nour model architecture and retrain the model;\\n(2)Online training stage. After a model is deployed to online\\nserving, the training does not stop but enters the online train-\\ning stage. Instead of reading mini-batch examples from the\\nstorage, a training worker consumes realtime data on-the-fly\\nand updates the training PS. The training PS periodically syn-\\nchronizes its parameters to the serving PS, which will take\\neffect on the user side immediately. This enables our model\\nto interactively adapt itself according to a user’s feedback in\\nrealtime.2.2.1 Streaming Engine. Monolith is built with the capability of\\nseamlessly switching between batch training and online training.\\nThis is enabled by our design of streaming engine as illustrated by\\nFigure 4.\\nIn our design, we use one Kafka [ 13] queue to log actions of users\\n(E.g. Click on an item or like an item etc.) and another Kafka queue\\nfor features. At the core of the engine is a Flink [ 4] streaming job for\\nonline feature Joiner. The online joiner concatenates features with\\nlabels from user actions and produces training examples, which are\\nthen written to a Kafka queue. The queue for training examples is\\nconsumed by both online training and batch training:\\n•For online training, the training worker directly reads data\\nfrom the Kafka queue;\\n•For batch training, a data dumping job will first dump data\\nto HDFS [ 18]; After data in HDFS accumulated to certain\\namount, training worker will retrieve data from HDFS and\\nperform batch training.\\nUpdated parameters in training PS will be pushed to serving PS\\naccording to the parameter synchronization schedule.\\n2.2.2 Online Joiner. In real-world applications, user actions log\\nand features are streamed into the online joiner (Figure 5) without\\nguarantee in time order. Therefore we use a unique key for each\\nrequest so that user action and features could correctly pair up.\\nThe lag of user action could also be a problem. For example, a\\nuser may take a few days before they decide to buy an item they\\nwere presented days ago. This is a challenge for the joiner because\\nif all features are kept in cache, it would simply not fit in memory.\\nIn our system, an on-disk key-value storage is utilized to store\\nfeatures that are waiting for over certain time period. When a user\\naction log arrives, it first looks up the in-memory cache, and then\\nlooks up the key-value storage in case of a missing cache.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nLog Kafka \\n(User Actions) \\nFeature Kafka \\nIn-memory \\nCache \\nOn-disk \\nKV-Store \\nJoin \\nNegative \\nSampling \\nTraining Example \\nKafka \\nFound in Cache \\nRead from KV-Store \\nFigure 5: Online Joiner.\\nAnother problem that arise in real-world application is that\\nthe distribution of negative and positive examples are highly un-\\neven, where number of the former could be magnitudes of order\\nhigher than the latter. To prevent positive examples from being\\noverwhelmed by negative ones, a common strategy is to do negative\\nsampling. This would certainly change the underlying distribution\\nof the trained model, tweaking it towards higher probability of\\nmaking positive predictions. As a remedy, we apply log odds cor-\\nrection [ 19] during serving, making sure that the online model is\\nan unbiased estimator of the original distribution.\\n2.2.3 Parameter Synchronization. During online training, the Mono-\\nlith training cluster keeps receiving data from the online serving\\nmodule and updates parameters on the training PS. A crucial step\\nto enable the online serving PS to benefit from these newly trained\\nparameters is the synchronization of updated model parameters. In\\nproduction environment, we are encountered by several challenges:\\n•Models on the online serving PS must not stop serving when\\nupdating. Our models in production is usually several ter-\\nabytes in size, and as a result replacing all parameters takes a\\nwhile. It would be intolerable to stop an online PS from serv-\\ning the model during the replacement process, and updates\\nmust be made on-the-fly;\\n•Transferring a multi-terabyte model of its entirety from train-\\ning PS to the online serving PS would pose huge pressure\\nto both the network bandwidth and memory on PS, since it\\nrequires doubled model size of memory to accept the newly\\narriving model.\\nFor the online training to scale up to the size of our business\\nscenario, we designed an incremental on-the-fly periodic parameter\\nsynchronization mechanism in Monolith based on several notice-\\nable characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\\ntion models;\\n(2)Given a short range of time window, only a small subset of\\nIDs gets trained and their embeddings updated;\\n(3)Dense variables move much slower than sparse embeddings.\\nThis is because in momentum-based optimizers, the accumu-\\nlation of momentum for dense variables is magnified by the\\ngigantic size of recommendation training data, while only\\na few sparse embeddings receives updates in a single data\\nbatch.\\n(1) and (2) allows us to exploit the sparse updates across all feature\\nIDs. In Monolith, we maintain a hash set of touched keys, repre-\\nsenting IDs whose embeddings get trained since the last parameter\\nsynchronization. We push the subset of sparse parameters whose\\nkeys are in the touched-keys set with a minute-level time interval\\nfrom the training PS to the online serving PS. This relatively small\\npack of incremental parameter update is lightweight for network\\ntransmission and will not cause a sharp memory spike during the\\nsynchronization.\\nWe also exploit (3) to further reduce network I/O and memory\\nusage by setting a more aggressive sync schedule for sparse pa-\\nrameters, while updating dense parameters less frequently. This\\ncould render us a situation where the dense parameters we serve is\\na relatively stale version compared to sparse part. However, such\\ninconsistency could be tolerated due to the reason mentioned in (3)\\nas no conspicuous loss has been observed.\\n2.3 Fault Tolerance\\nAs a system in production, Monolith is designed with the ability to\\nrecover a PS in case it fails. A common choice for fault tolerance is\\nto snapshot the state of a model periodically, and recover from theORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nlatest snapshot when PS failure is detected. The choice of snapshot\\nfrequency has two major impacts:\\n(1)Model quality. Intuitively, model quality suffers less from\\nloss of recent history with increased snapshot frequency.\\n(2)Computation overhead. Snapshotting a multi-terabyte model\\nis not free. It incurs large chunks of memory copy and disk\\nI/O.\\nAs a trade-off between model quality and computation overhead,\\nMonolith snapshots all training PS every day. Though a PS will lose\\none day’s worth of update in case of a failure, we discover that the\\nperformance degradation is tolerable through our experiments. We\\nwill analyze the effect of PS reliability in the next section.\\n. . .\\n. . .\\nIDsEmbeddings FMMLP Output \\nFigure 6: DeepFM model architecture.\\n3 EVALUATION\\nFor a better understanding of benefits and trade-offs brought about\\nby our proposed design, we conducted several experiments at pro-\\nduction scale and A/B test with live serving traffic to evaluate and\\nverify Monolith from different aspects. We aim to answer the fol-\\nlowing questions by our experiments:\\n(1) How much can we benefit from a collisionless HashTable ?\\n(2) How important is realtime online training?\\n(3)Is Monolith’s design of parameter synchronization robust\\nenough in a large-scale production scenario?\\nIn this section, we first present our experimental settings and\\nthen discuss results and our findings in detail.\\n3.1 Experimental Setup\\n3.1.1 Embedding Table. As described in Section 2.1, embedding\\ntables in Monolith are implemented as collisionless HashTable s.\\nTo prove the necessity of avoiding collisions in embedding tables\\nand to quantify gains from our collisionless implementation, we\\nperformed two groups of experiments on the Movielens dataset\\nand on our internal production dataset respectively:\\n(1)MovieLens ml-25m dataset [ 11]. This is a standard public\\ndataset for movie ratings, containing 25 million ratings that\\ninvolves approximately 162000 users and 62000 movies.\\n•Preprocessing of labels . The original labels are ratings from\\n0.5 to 5.0, while in production our tasks are mostly re-\\nceiving binary signals from users. To better simulate our\\nproduction models, we convert scale labels to binary labelsby treating scores≥3.5as positive samples and the rest\\nas negative samples.\\n•Model and metrics . We implemented a standard DeepFM\\n[9] model, a commonly used model architecture for recom-\\nmendation problems. It consist of an FM component and a\\ndense component (Figure 6). Predictions are evaluated by\\nAUC [ 2] as this is the major measurement for real models.\\n•Embedding collisions . This dataset contains approximately\\n160K user IDs and 60K movie IDs. To compare with the\\ncollisionless version of embedding table implementation,\\nwe performed another group of experiment where IDs are\\npreprocessed with MD5 hashing and then mapped to a\\nsmaller ID space. As a result, some IDs will share their\\nembedding with others. Table 1 shows detailed statistics\\nof user and movie IDs before and after hashing.\\nUser IDs Movie IDs\\n# Before Hashing 162541 59047\\n# After Hashing 149970 57361\\nCollision rate 7.73% 2 .86%\\nTable 1: Statistics of IDs Before and After Hashing.\\n(2)Internal Recommendation dataset .\\nWe also performed experiments on a recommendation model\\nin production environment. This model generally follows a\\nmulti-tower architecture, with each tower responsible for\\nlearning to predict a specialized kind of user behavior.\\n•Each model has around 1000 embedding tables, and distri-\\nbution of size of embedding tables are very uneven;\\n•The original ID space of embedding table was 248. In our\\nbaseline, we applied a hashing trick by decomposing to\\ncurb the size of embedding table. To be more specific, we\\nuse two smaller embedding tables instead of a gigantic\\none to generate a unique embedding for each ID by vector\\ncombination:\\n𝐼𝐷𝑟=𝐼𝐷%224\\n𝐼𝐷𝑞=𝐼𝐷÷224\\n𝐸=𝐸𝑟+𝐸𝑞,\\nwhere𝐸𝑟,𝐸𝑞are embeddings corresponding to 𝐼𝐷𝑟,𝐼𝐷𝑞.\\nThis effectively reduces embedding table sizes from 248\\nto225;\\n•This model is serving in real production, and the perfor-\\nmance of this experiment is measured by online AUC with\\nreal serving traffic.\\n3.1.2 Online Training. During online training, we update our on-\\nline serving PS with the latest set of parameters with minute-level\\nintervals. We designed two groups of experiments to verify model\\nquality and system robustness.\\n(1)Update frequency . To investigate the necessity of minute-\\nlevel update frequency, we conducted experiments that syn-\\nchronize parameters from training model to prediction model\\nwith different intervals.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nAlgorithm 1 Simulated Online Training.\\n1:Input:𝐷𝑏𝑎𝑡𝑐ℎ; /* Data for batch training. */\\n2:Input:𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖=1···𝑁; /* Data for online training, split into 𝑁shards. */\\n3:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑏𝑎𝑡𝑐ℎ,𝜃𝑡𝑟𝑎𝑖𝑛); /* Batch training. */\\n/* Online training. */\\n4:for𝑖=1···𝑁do\\n5:𝜃𝑠𝑒𝑟𝑣𝑒←𝜃𝑡𝑟𝑎𝑖𝑛 ; /* Sync training parameters to serving model. */\\n6:𝐴𝑈𝐶𝑖=Evaluate(𝜃𝑠𝑒𝑟𝑣𝑒,𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖); /* Evaluate online prediction on new data. */\\n7:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖,𝜃𝑡𝑟𝑎𝑖𝑛); /* Train with new data. */\\n8:end for\\nThe dataset we use is the Criteo Display Ads Challenge\\ndataset3, a large-scale standard dataset for benchmarking\\nCTR models. It contains 7 days of chronologically ordered\\ndata recording features and click actions. For this experiment,\\nwe use a standard DeepFM [9] model as described in 6.\\nTo simulate online training, we did the following preprocess-\\ning for the dataset. We take 7 days of data from the dataset,\\nand split it to two parts: 5 days of data for batch training, and\\n2 days for online training. We further split the 2 days of data\\ninto𝑁shards chronologically. Online training is simulated\\nby algorithm 1.\\nAs such, we simulate synchronizing trained parameters to\\nonline serving PS with an interval determined by number of\\ndata shards. We experimented with 𝑁=10,50,100, which\\nroughly correspond to update interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛.\\n(2)Live experiment . In addition, we also performed a live ex-\\nperiment with real serving traffic to further demonstrate the\\nimportance of online training in real-world application. This\\nA/B experiment compares online training to batch training\\none one of our Ads model in production.\\n3.2 Results and Analysis\\n3.2.1 The Effect of Embedding Collision. Results from MovieLens\\ndataset and the Internal recommedation dataset both show that\\nembedding collisions will jeopardize model quality.\\n(1)Models with collisionless HashTable consistently outper-\\nforms those with collision. This conclusion holds true re-\\ngardless of\\n•Increase of number of training epochs. As shown in Figure\\n7, the model with collisionless embedding table has higher\\nAUC from the first epoch and converges at higher value;\\n•Change of distribution with passage of time due to Con-\\ncept Drift. As shown in Figure 8, models with collision-\\nless embedding table is also robust as time passes by and\\nusers/items context changes.\\n(2)Data sparsity caused by collisionless embedding table will\\nnot lead to model overfitting. As shown in Figure 7, a model\\nwith collisionless embedding table does not overfit after it\\nconverges.\\n3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\\n1 2 3 4 5 6 7 8 9 10\\nepoch0.8050.8100.8150.8200.825test auccollision-free hash\\nhash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\\nMovieLens\\n2 4 6 8 10 12\\nDay0.7700.7750.7800.7850.790Online serving AUChash w/ collision\\ncollisionless hash table\\nFigure 8: Effect of Embedding Collision On A\\nRecommendation Model In Production\\nWe measure performance of this recommendation model by online serving AUC,\\nwhich is fluctuating across different days due to concept-drift.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\n•Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;•Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛 respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66±0.020 79 .42±0.026\\n1 hr 79.78±0.005 79 .44±0.030\\n30 min 79.80±0.008 79 .43±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nDay 1 2 3 4 5 6 7\\nAUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\\nTable 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\\nSync Interval 1 hr\\nSync Interval 30 min\\nFigure 10: Comparison of different sync intervals for\\nonline training.\\nThe live A/B experiment between online training and batch\\ntraining on an Ads model in production also show that there\\nis a significant bump in online serving AUC (Table 3).\\nInspired by this observation, we synchronize sparse parame-\\nters to serving PS of our production models as frequent as\\npossible (currently at minute-level), to the extent that the\\ncomputation overhead and system reliability could endure.\\nRecall that dense variables requires a less frequent update as\\ndiscussed in 2.2.3, we update them at day-level. By doing so,\\nwe can bring down our computation overhead to a very low\\nlevel. Suppose 100,000 IDs gets updated in a minute, and the\\ndimension of embedding is 1024, the total size of data need\\nto be transferred is 4𝐾𝐵×100,000≈400𝑀𝐵per minute.\\nFor dense parameters, since they are synchronized daily, we\\nchoose to schedule the synchronization when the traffic is\\nlowest (e.g. midnight).\\n(2)The Effect of PS reliability.\\nWith a minute-level parameter synchronization, we initially\\nexpect a more frequent snapshot of training PS to match the\\nrealtime update. To our surprise, we enlarged the snapshot\\ninterval to 1 day and still observed nearly no loss of model\\nquality.\\nFinding the right trade-off between model quality and com-\\nputation overhead is difficult for personalized ranking sys-\\ntems since users are extremely sensitive on recommendation\\nquality. Traditionally, large-scale systems tend to set a fre-\\nquent snapshot schedule for their models, which sacrifices\\ncomputation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\\nregard and to our surprise, model quality is more robust than\\nexpected. With a 0.01% failure rate of PS machine per day,\\nwe find a model from the previous day works embarrassingly\\nwell. This is explicable by the following calculation: Suppose\\na model’s parameters are sharded across 1000 PS, and they\\nsnapshot every day. Given 0.01% failure rate, one of them\\nwill go down every 10 days and we lose all updates on this\\nPS for 1 day. Assuming a DAU of 15 Million and an even\\ndistribution of user IDs on each PS, we lose 1 day’s feedback\\nfrom 15000 users every 10 days. This is acceptable because\\n(a) For sparse features which is user-specific, this is equiv-\\nalent to losing a tiny fraction of 0.01% DAU; (b) For dense\\nvariables, since they are updated slowly as we discussed in\\n2.2.3, losing 1 day’s update out of 1000 PS is negligible.\\nBased on the above observation and calculation, we radically\\nlowered our snapshot frequency and thereby saved quite a\\nbit in computation overhead.\\n4 RELATED WORK\\nEver since some earliest successful application of deep learning to\\nindustry-level recommendation systems [ 6,10], researchers and\\nengineers have been employing various techniques to ameliorate\\nissues mentioned in Section 1.\\nTo tackle the issue of sparse feature representation, [ 3,6] uses\\nfixed-size embedding table with hash-trick. There are also attempts\\nin improving hashing to reduce collision [ 3,7]. Other works directly\\nutilize native key-value hash table to allow dynamic growth of table\\nsize [ 12,15,20,21]. These implementations builds upon TensorFlow\\nbut relies either on specially designed software mechanism [ 14,\\n15,20] or hardware [ 21] to access and manage their hash-tables.\\nCompared to these solutions, Monolith’s hash-table is yet another\\nnative TensorFlow operation. It is developer friendly and has higher\\ncross-platform interoperability, which is suitable for ToB scenarios.\\nAn organic and tight integration with TensorFlow also enables\\neasier optimizations of computation performance.\\nBridging the gap between training and serving and alleviation\\nof Concept Drift [ 8] is another topic of interest. To support online\\nupdate and avoid memory issues, both [ 12] and [ 20] designed fea-\\nture eviction mechanisms to flexibly adjust the size of embedding\\ntables. Both [ 12] and [ 14] support some form of online training,\\nwhere learned parameters are synced to serving with a relatively\\nshort interval compared to traditional batch training, with fault tol-\\nerance mechanisms. Monolith took similar approach to elastically\\nadmit and evict features, while it has a more lightweight parameter\\nsynchronization mechanism to guarantee model quality.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n5 CONCLUSION\\nIn this work, we reviewed several most important challenges for\\nindustrial-level recommendation systems and present our system\\nin production, Monolith, to address them and achieved best perfor-\\nmance compared to existing solutions.\\nWe proved that a collisionless embedding table is essential for\\nmodel quality, and demonstrated that our implementation of Cuckoo\\nHashMap based embedding table is both memory efficient and help-\\nful for improving online serving metrics.\\nWe also proved that realtime serving is crucial in recommenda-\\ntion systems, and that parameter synchronization interval should\\nbe as short as possible for an ultimate model performance. Our\\nsolution for online realtime serving in Monolith has a delicately\\ndesigned parameter synchronization and a fault tolerance mecha-\\nnism: In our parameter synchronization algorithm, we showed that\\nconsistency of version across different parts of parameters could be\\ntraded-off for reducing network bandwidth consumption; In fault\\ntolerance design, we demonstrated that our strategy of trading-off\\nPS reliability for realtime-ness is a robust solution.\\nTo conclude, Monolith succeeded in providing a general solution\\nfor production scale recommendation systems.\\nACKNOWLEDGMENTS\\nHanzhi Zhou provided useful suggestions on revision of this paper.\\nREFERENCES\\n[1]Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\\nYuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\\nmachine learning. ArXiv abs/1605.08695 (2016).\\n[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–\\n1159.\\n[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\\nengineering.com/core-modeling-at-instagram-a51e0158aa48\\n[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\\nand Kostas Tzoumas. 2015. Apache Flink ™: Stream and Batch Processing in a\\nSingle Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\\n[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\\nIspir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\\nHemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\\nof the 1st Workshop on Deep Learning for Recommender Systems (2016).\\n[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\\nfor YouTube Recommendations. Proceedings of the 10th ACM Conference on\\nRecommender Systems (2016).\\n[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\\nteenth ACM Conference on Recommender Systems (2021).\\n[8]João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\\n2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\\n(2014), 1 – 37.\\n[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\\nIJCAI .\\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\\nDavid M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\\nAndrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\\nXuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based\\nPersonalized Recommendation. 2020 IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA) (2020), 488–501.\\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\\nand Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\\nHuang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\\nSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\\nProceedings of the 1st International Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data (2019).\\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\\nWu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan\\nLuo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\\nLin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\\nbo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\\nScaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\\nabs/2111.05897 (2021).\\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\\nmender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\\ntensorflow-in-recommender-systems.html\\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS .\\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\\n2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) (2010), 1–10.\\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\\nSampling and Log Odds Correction with Rare Events Data. In Advances in Neural\\nInformation Processing Systems .\\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\\nLin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\\nContinual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\\nnational Conference for High Performance Computing, Networking, Storage and\\nAnalysis (2020), 1–17.\\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\\n2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\\n28th ACM International Conference on Information and Knowledge Management\\n(2019).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"For visualization pupose to user - Not implemented down the lane\"\"\"\n",
    "content_of_pdf=\"\"\n",
    "for i,page in enumerate(pdf_read.pages):\n",
    "    text_extracted=page.extract_text()\n",
    "    if(text_extracted):\n",
    "        content_of_pdf+=text_extracted\n",
    "content_of_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such\\nframeworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.\\n∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\\nZhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural\\nfit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical\\nfeatures, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\n•Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\n•Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a user’s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-artMonolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our servers’ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate\\ndesign details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlow’s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two\\nsets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlow’s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlow’s limitation that arises from separation of training and in-\\nference, Monolith’s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation\\nis to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case 𝑂(1)time complexity\\nfor lookups and deletions, and an expected amortized 𝑂(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables 𝑇0,𝑇1\\nwith different hash functions ℎ0(𝑥),ℎ1(𝑥), and an element would\\nbe stored in either one of them. When trying to insert an element\\n𝐴into𝑇0, it first attempts to place 𝐴atℎ0(𝐴); Ifℎ0(𝐴)is occupied\\nby another element 𝐵, it would evict 𝐵from𝑇0and try inserting 𝐵\\ninto𝑇1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;\\n(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nUser \\n Log Kafka \\nActions \\n(Click, Like, \\nConvert …) \\nFeature Kafka \\n Model Server \\n Training Worker \\nTraining PS \\n Serving PS \\nJoiner \\nFlink Job \\nTraining Example \\nKafka \\nBatch Training Data \\nHDFS \\nGenerate \\nFeatures \\nParameter Synchronization \\nDump Batch Data \\nOnline \\nTraining \\nBatch Training \\nFigure 4: Streaming Engine.\\nThe information feedback loop from [User →Model Server→Training Worker→Model Server→User] would spend a long time when taking the Batch Training\\npath, while the Online Training will close the loop more instantly.\\n(2)IDs are timed and set to expire after being inactive for a\\npredefined period of time. The expire time is also tunable for\\neach embedding table to allow for distinguishing features\\nwith different sensitivity to historical information.\\nIn our implementation, HashTable is implemented as a Tensor-\\nFlow resource operation. Similar to Variable , look-ups and updates\\nare also implemented as native TensorFlow operations for easier\\nintegration and better compatibility.\\n2.2 Online Training\\nIn Monolith, training is divided into two stages (Figure 1):\\n(1)Batch training stage. This stage works as an ordinary Tensor-\\nFlow training loop: In each training step, a training worker\\nreads one mini-batch of training examples from the stor-\\nage, requests parameters from PS, computes a forward and\\na backward pass, and finally push updated parameters to\\nthe training PS. Slightly different from other common deep\\nlearning tasks, we only train our dataset for one pass. Batch\\ntraining is useful for training historical data when we modify\\nour model architecture and retrain the model;\\n(2)Online training stage. After a model is deployed to online\\nserving, the training does not stop but enters the online train-\\ning stage. Instead of reading mini-batch examples from the\\nstorage, a training worker consumes realtime data on-the-fly\\nand updates the training PS. The training PS periodically syn-\\nchronizes its parameters to the serving PS, which will take\\neffect on the user side immediately. This enables our model\\nto interactively adapt itself according to a user’s feedback in\\nrealtime.2.2.1 Streaming Engine. Monolith is built with the capability of\\nseamlessly switching between batch training and online training.\\nThis is enabled by our design of streaming engine as illustrated by\\nFigure 4.\\nIn our design, we use one Kafka [ 13] queue to log actions of users\\n(E.g. Click on an item or like an item etc.) and another Kafka queue\\nfor features. At the core of the engine is a Flink [ 4] streaming job for\\nonline feature Joiner. The online joiner concatenates features with\\nlabels from user actions and produces training examples, which are\\nthen written to a Kafka queue. The queue for training examples is\\nconsumed by both online training and batch training:\\n•For online training, the training worker directly reads data\\nfrom the Kafka queue;\\n•For batch training, a data dumping job will first dump data\\nto HDFS [ 18]; After data in HDFS accumulated to certain\\namount, training worker will retrieve data from HDFS and\\nperform batch training.\\nUpdated parameters in training PS will be pushed to serving PS\\naccording to the parameter synchronization schedule.\\n2.2.2 Online Joiner. In real-world applications, user actions log\\nand features are streamed into the online joiner (Figure 5) without\\nguarantee in time order. Therefore we use a unique key for each\\nrequest so that user action and features could correctly pair up.\\nThe lag of user action could also be a problem. For example, a\\nuser may take a few days before they decide to buy an item they\\nwere presented days ago. This is a challenge for the joiner because\\nif all features are kept in cache, it would simply not fit in memory.\\nIn our system, an on-disk key-value storage is utilized to store\\nfeatures that are waiting for over certain time period. When a user\\naction log arrives, it first looks up the in-memory cache, and then\\nlooks up the key-value storage in case of a missing cache.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nLog Kafka \\n(User Actions) \\nFeature Kafka \\nIn-memory \\nCache \\nOn-disk \\nKV-Store \\nJoin \\nNegative \\nSampling \\nTraining Example \\nKafka \\nFound in Cache \\nRead from KV-Store \\nFigure 5: Online Joiner.\\nAnother problem that arise in real-world application is that\\nthe distribution of negative and positive examples are highly un-\\neven, where number of the former could be magnitudes of order\\nhigher than the latter. To prevent positive examples from being\\noverwhelmed by negative ones, a common strategy is to do negative\\nsampling. This would certainly change the underlying distribution\\nof the trained model, tweaking it towards higher probability of\\nmaking positive predictions. As a remedy, we apply log odds cor-\\nrection [ 19] during serving, making sure that the online model is\\nan unbiased estimator of the original distribution.\\n2.2.3 Parameter Synchronization. During online training, the Mono-\\nlith training cluster keeps receiving data from the online serving\\nmodule and updates parameters on the training PS. A crucial step\\nto enable the online serving PS to benefit from these newly trained\\nparameters is the synchronization of updated model parameters. In\\nproduction environment, we are encountered by several challenges:\\n•Models on the online serving PS must not stop serving when\\nupdating. Our models in production is usually several ter-\\nabytes in size, and as a result replacing all parameters takes a\\nwhile. It would be intolerable to stop an online PS from serv-\\ning the model during the replacement process, and updates\\nmust be made on-the-fly;\\n•Transferring a multi-terabyte model of its entirety from train-\\ning PS to the online serving PS would pose huge pressure\\nto both the network bandwidth and memory on PS, since it\\nrequires doubled model size of memory to accept the newly\\narriving model.\\nFor the online training to scale up to the size of our business\\nscenario, we designed an incremental on-the-fly periodic parameter\\nsynchronization mechanism in Monolith based on several notice-\\nable characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\\ntion models;\\n(2)Given a short range of time window, only a small subset of\\nIDs gets trained and their embeddings updated;\\n(3)Dense variables move much slower than sparse embeddings.\\nThis is because in momentum-based optimizers, the accumu-\\nlation of momentum for dense variables is magnified by the\\ngigantic size of recommendation training data, while only\\na few sparse embeddings receives updates in a single data\\nbatch.\\n(1) and (2) allows us to exploit the sparse updates across all feature\\nIDs. In Monolith, we maintain a hash set of touched keys, repre-\\nsenting IDs whose embeddings get trained since the last parameter\\nsynchronization. We push the subset of sparse parameters whose\\nkeys are in the touched-keys set with a minute-level time interval\\nfrom the training PS to the online serving PS. This relatively small\\npack of incremental parameter update is lightweight for network\\ntransmission and will not cause a sharp memory spike during the\\nsynchronization.\\nWe also exploit (3) to further reduce network I/O and memory\\nusage by setting a more aggressive sync schedule for sparse pa-\\nrameters, while updating dense parameters less frequently. This\\ncould render us a situation where the dense parameters we serve is\\na relatively stale version compared to sparse part. However, such\\ninconsistency could be tolerated due to the reason mentioned in (3)\\nas no conspicuous loss has been observed.\\n2.3 Fault Tolerance\\nAs a system in production, Monolith is designed with the ability to\\nrecover a PS in case it fails. A common choice for fault tolerance is\\nto snapshot the state of a model periodically, and recover from theORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nlatest snapshot when PS failure is detected. The choice of snapshot\\nfrequency has two major impacts:\\n(1)Model quality. Intuitively, model quality suffers less from\\nloss of recent history with increased snapshot frequency.\\n(2)Computation overhead. Snapshotting a multi-terabyte model\\nis not free. It incurs large chunks of memory copy and disk\\nI/O.\\nAs a trade-off between model quality and computation overhead,\\nMonolith snapshots all training PS every day. Though a PS will lose\\none day’s worth of update in case of a failure, we discover that the\\nperformance degradation is tolerable through our experiments. We\\nwill analyze the effect of PS reliability in the next section.\\n. . .\\n. . .\\nIDsEmbeddings FMMLP Output \\nFigure 6: DeepFM model architecture.\\n3 EVALUATION\\nFor a better understanding of benefits and trade-offs brought about\\nby our proposed design, we conducted several experiments at pro-\\nduction scale and A/B test with live serving traffic to evaluate and\\nverify Monolith from different aspects. We aim to answer the fol-\\nlowing questions by our experiments:\\n(1) How much can we benefit from a collisionless HashTable ?\\n(2) How important is realtime online training?\\n(3)Is Monolith’s design of parameter synchronization robust\\nenough in a large-scale production scenario?\\nIn this section, we first present our experimental settings and\\nthen discuss results and our findings in detail.\\n3.1 Experimental Setup\\n3.1.1 Embedding Table. As described in Section 2.1, embedding\\ntables in Monolith are implemented as collisionless HashTable s.\\nTo prove the necessity of avoiding collisions in embedding tables\\nand to quantify gains from our collisionless implementation, we\\nperformed two groups of experiments on the Movielens dataset\\nand on our internal production dataset respectively:\\n(1)MovieLens ml-25m dataset [ 11]. This is a standard public\\ndataset for movie ratings, containing 25 million ratings that\\ninvolves approximately 162000 users and 62000 movies.\\n•Preprocessing of labels . The original labels are ratings from\\n0.5 to 5.0, while in production our tasks are mostly re-\\nceiving binary signals from users. To better simulate our\\nproduction models, we convert scale labels to binary labelsby treating scores≥3.5as positive samples and the rest\\nas negative samples.\\n•Model and metrics . We implemented a standard DeepFM\\n[9] model, a commonly used model architecture for recom-\\nmendation problems. It consist of an FM component and a\\ndense component (Figure 6). Predictions are evaluated by\\nAUC [ 2] as this is the major measurement for real models.\\n•Embedding collisions . This dataset contains approximately\\n160K user IDs and 60K movie IDs. To compare with the\\ncollisionless version of embedding table implementation,\\nwe performed another group of experiment where IDs are\\npreprocessed with MD5 hashing and then mapped to a\\nsmaller ID space. As a result, some IDs will share their\\nembedding with others. Table 1 shows detailed statistics\\nof user and movie IDs before and after hashing.\\nUser IDs Movie IDs\\n# Before Hashing 162541 59047\\n# After Hashing 149970 57361\\nCollision rate 7.73% 2 .86%\\nTable 1: Statistics of IDs Before and After Hashing.\\n(2)Internal Recommendation dataset .\\nWe also performed experiments on a recommendation model\\nin production environment. This model generally follows a\\nmulti-tower architecture, with each tower responsible for\\nlearning to predict a specialized kind of user behavior.\\n•Each model has around 1000 embedding tables, and distri-\\nbution of size of embedding tables are very uneven;\\n•The original ID space of embedding table was 248. In our\\nbaseline, we applied a hashing trick by decomposing to\\ncurb the size of embedding table. To be more specific, we\\nuse two smaller embedding tables instead of a gigantic\\none to generate a unique embedding for each ID by vector\\ncombination:\\n𝐼𝐷𝑟=𝐼𝐷%224\\n𝐼𝐷𝑞=𝐼𝐷÷224\\n𝐸=𝐸𝑟+𝐸𝑞,\\nwhere𝐸𝑟,𝐸𝑞are embeddings corresponding to 𝐼𝐷𝑟,𝐼𝐷𝑞.\\nThis effectively reduces embedding table sizes from 248\\nto225;\\n•This model is serving in real production, and the perfor-\\nmance of this experiment is measured by online AUC with\\nreal serving traffic.\\n3.1.2 Online Training. During online training, we update our on-\\nline serving PS with the latest set of parameters with minute-level\\nintervals. We designed two groups of experiments to verify model\\nquality and system robustness.\\n(1)Update frequency . To investigate the necessity of minute-\\nlevel update frequency, we conducted experiments that syn-\\nchronize parameters from training model to prediction model\\nwith different intervals.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nAlgorithm 1 Simulated Online Training.\\n1:Input:𝐷𝑏𝑎𝑡𝑐ℎ; /* Data for batch training. */\\n2:Input:𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖=1···𝑁; /* Data for online training, split into 𝑁shards. */\\n3:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑏𝑎𝑡𝑐ℎ,𝜃𝑡𝑟𝑎𝑖𝑛); /* Batch training. */\\n/* Online training. */\\n4:for𝑖=1···𝑁do\\n5:𝜃𝑠𝑒𝑟𝑣𝑒←𝜃𝑡𝑟𝑎𝑖𝑛 ; /* Sync training parameters to serving model. */\\n6:𝐴𝑈𝐶𝑖=Evaluate(𝜃𝑠𝑒𝑟𝑣𝑒,𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖); /* Evaluate online prediction on new data. */\\n7:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖,𝜃𝑡𝑟𝑎𝑖𝑛); /* Train with new data. */\\n8:end for\\nThe dataset we use is the Criteo Display Ads Challenge\\ndataset3, a large-scale standard dataset for benchmarking\\nCTR models. It contains 7 days of chronologically ordered\\ndata recording features and click actions. For this experiment,\\nwe use a standard DeepFM [9] model as described in 6.\\nTo simulate online training, we did the following preprocess-\\ning for the dataset. We take 7 days of data from the dataset,\\nand split it to two parts: 5 days of data for batch training, and\\n2 days for online training. We further split the 2 days of data\\ninto𝑁shards chronologically. Online training is simulated\\nby algorithm 1.\\nAs such, we simulate synchronizing trained parameters to\\nonline serving PS with an interval determined by number of\\ndata shards. We experimented with 𝑁=10,50,100, which\\nroughly correspond to update interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛.\\n(2)Live experiment . In addition, we also performed a live ex-\\nperiment with real serving traffic to further demonstrate the\\nimportance of online training in real-world application. This\\nA/B experiment compares online training to batch training\\none one of our Ads model in production.\\n3.2 Results and Analysis\\n3.2.1 The Effect of Embedding Collision. Results from MovieLens\\ndataset and the Internal recommedation dataset both show that\\nembedding collisions will jeopardize model quality.\\n(1)Models with collisionless HashTable consistently outper-\\nforms those with collision. This conclusion holds true re-\\ngardless of\\n•Increase of number of training epochs. As shown in Figure\\n7, the model with collisionless embedding table has higher\\nAUC from the first epoch and converges at higher value;\\n•Change of distribution with passage of time due to Con-\\ncept Drift. As shown in Figure 8, models with collision-\\nless embedding table is also robust as time passes by and\\nusers/items context changes.\\n(2)Data sparsity caused by collisionless embedding table will\\nnot lead to model overfitting. As shown in Figure 7, a model\\nwith collisionless embedding table does not overfit after it\\nconverges.\\n3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\\n1 2 3 4 5 6 7 8 9 10\\nepoch0.8050.8100.8150.8200.825test auccollision-free hash\\nhash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\\nMovieLens\\n2 4 6 8 10 12\\nDay0.7700.7750.7800.7850.790Online serving AUChash w/ collision\\ncollisionless hash table\\nFigure 8: Effect of Embedding Collision On A\\nRecommendation Model In Production\\nWe measure performance of this recommendation model by online serving AUC,\\nwhich is fluctuating across different days due to concept-drift.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\n•Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;•Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛 respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66±0.020 79 .42±0.026\\n1 hr 79.78±0.005 79 .44±0.030\\n30 min 79.80±0.008 79 .43±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nDay 1 2 3 4 5 6 7\\nAUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\\nTable 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\\nSync Interval 1 hr\\nSync Interval 30 min\\nFigure 10: Comparison of different sync intervals for\\nonline training.\\nThe live A/B experiment between online training and batch\\ntraining on an Ads model in production also show that there\\nis a significant bump in online serving AUC (Table 3).\\nInspired by this observation, we synchronize sparse parame-\\nters to serving PS of our production models as frequent as\\npossible (currently at minute-level), to the extent that the\\ncomputation overhead and system reliability could endure.\\nRecall that dense variables requires a less frequent update as\\ndiscussed in 2.2.3, we update them at day-level. By doing so,\\nwe can bring down our computation overhead to a very low\\nlevel. Suppose 100,000 IDs gets updated in a minute, and the\\ndimension of embedding is 1024, the total size of data need\\nto be transferred is 4𝐾𝐵×100,000≈400𝑀𝐵per minute.\\nFor dense parameters, since they are synchronized daily, we\\nchoose to schedule the synchronization when the traffic is\\nlowest (e.g. midnight).\\n(2)The Effect of PS reliability.\\nWith a minute-level parameter synchronization, we initially\\nexpect a more frequent snapshot of training PS to match the\\nrealtime update. To our surprise, we enlarged the snapshot\\ninterval to 1 day and still observed nearly no loss of model\\nquality.\\nFinding the right trade-off between model quality and com-\\nputation overhead is difficult for personalized ranking sys-\\ntems since users are extremely sensitive on recommendation\\nquality. Traditionally, large-scale systems tend to set a fre-\\nquent snapshot schedule for their models, which sacrifices\\ncomputation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\\nregard and to our surprise, model quality is more robust than\\nexpected. With a 0.01% failure rate of PS machine per day,\\nwe find a model from the previous day works embarrassingly\\nwell. This is explicable by the following calculation: Suppose\\na model’s parameters are sharded across 1000 PS, and they\\nsnapshot every day. Given 0.01% failure rate, one of them\\nwill go down every 10 days and we lose all updates on this\\nPS for 1 day. Assuming a DAU of 15 Million and an even\\ndistribution of user IDs on each PS, we lose 1 day’s feedback\\nfrom 15000 users every 10 days. This is acceptable because\\n(a) For sparse features which is user-specific, this is equiv-\\nalent to losing a tiny fraction of 0.01% DAU; (b) For dense\\nvariables, since they are updated slowly as we discussed in\\n2.2.3, losing 1 day’s update out of 1000 PS is negligible.\\nBased on the above observation and calculation, we radically\\nlowered our snapshot frequency and thereby saved quite a\\nbit in computation overhead.\\n4 RELATED WORK\\nEver since some earliest successful application of deep learning to\\nindustry-level recommendation systems [ 6,10], researchers and\\nengineers have been employing various techniques to ameliorate\\nissues mentioned in Section 1.\\nTo tackle the issue of sparse feature representation, [ 3,6] uses\\nfixed-size embedding table with hash-trick. There are also attempts\\nin improving hashing to reduce collision [ 3,7]. Other works directly\\nutilize native key-value hash table to allow dynamic growth of table\\nsize [ 12,15,20,21]. These implementations builds upon TensorFlow\\nbut relies either on specially designed software mechanism [ 14,\\n15,20] or hardware [ 21] to access and manage their hash-tables.\\nCompared to these solutions, Monolith’s hash-table is yet another\\nnative TensorFlow operation. It is developer friendly and has higher\\ncross-platform interoperability, which is suitable for ToB scenarios.\\nAn organic and tight integration with TensorFlow also enables\\neasier optimizations of computation performance.\\nBridging the gap between training and serving and alleviation\\nof Concept Drift [ 8] is another topic of interest. To support online\\nupdate and avoid memory issues, both [ 12] and [ 20] designed fea-\\nture eviction mechanisms to flexibly adjust the size of embedding\\ntables. Both [ 12] and [ 14] support some form of online training,\\nwhere learned parameters are synced to serving with a relatively\\nshort interval compared to traditional batch training, with fault tol-\\nerance mechanisms. Monolith took similar approach to elastically\\nadmit and evict features, while it has a more lightweight parameter\\nsynchronization mechanism to guarantee model quality.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n5 CONCLUSION\\nIn this work, we reviewed several most important challenges for\\nindustrial-level recommendation systems and present our system\\nin production, Monolith, to address them and achieved best perfor-\\nmance compared to existing solutions.\\nWe proved that a collisionless embedding table is essential for\\nmodel quality, and demonstrated that our implementation of Cuckoo\\nHashMap based embedding table is both memory efficient and help-\\nful for improving online serving metrics.\\nWe also proved that realtime serving is crucial in recommenda-\\ntion systems, and that parameter synchronization interval should\\nbe as short as possible for an ultimate model performance. Our\\nsolution for online realtime serving in Monolith has a delicately\\ndesigned parameter synchronization and a fault tolerance mecha-\\nnism: In our parameter synchronization algorithm, we showed that\\nconsistency of version across different parts of parameters could be\\ntraded-off for reducing network bandwidth consumption; In fault\\ntolerance design, we demonstrated that our strategy of trading-off\\nPS reliability for realtime-ness is a robust solution.\\nTo conclude, Monolith succeeded in providing a general solution\\nfor production scale recommendation systems.\\nACKNOWLEDGMENTS\\nHanzhi Zhou provided useful suggestions on revision of this paper.\\nREFERENCES\\n[1]Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\\nYuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\\nmachine learning. ArXiv abs/1605.08695 (2016).\\n[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–\\n1159.\\n[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\\nengineering.com/core-modeling-at-instagram-a51e0158aa48\\n[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\\nand Kostas Tzoumas. 2015. Apache Flink ™: Stream and Batch Processing in a\\nSingle Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\\n[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\\nIspir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\\nHemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\\nof the 1st Workshop on Deep Learning for Recommender Systems (2016).\\n[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\\nfor YouTube Recommendations. Proceedings of the 10th ACM Conference on\\nRecommender Systems (2016).\\n[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\\nteenth ACM Conference on Recommender Systems (2021).\\n[8]João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\\n2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\\n(2014), 1 – 37.\\n[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\\nIJCAI .\\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\\nDavid M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\\nAndrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\\nXuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based\\nPersonalized Recommendation. 2020 IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA) (2020), 488–501.\\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\\nand Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\\nHuang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\\nSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\\nProceedings of the 1st International Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data (2019).\\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\\nWu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan\\nLuo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\\nLin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\\nbo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\\nScaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\\nabs/2111.05897 (2021).\\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\\nmender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\\ntensorflow-in-recommender-systems.html\\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS .\\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\\n2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) (2010), 1–10.\\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\\nSampling and Log Odds Correction with Rare Events Data. In Advances in Neural\\nInformation Processing Systems .\\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\\nLin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\\nContinual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\\nnational Conference for High Performance Computing, Networking, Storage and\\nAnalysis (2020), 1–17.\\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\\n2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\\n28th ACM International Conference on Information and Knowledge Management\\n(2019).')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_store=[Document(page_content=content_of_pdf)]\n",
    "doc_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  IMP-NOTE:-Below the stuff documentation chain  expects variable {\"text\"} in its Prompt Template rather than other randome input variable for parsing the given doc store as an input while invoking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=\"Provide summary of the above pdf {text}\"\n",
    "prompt_doc=PromptTemplate(input_variables=[\"text\"],template=template2)\n",
    "# Chain_doc=load_summarize_chain(llm=llm1, prompt=prompt_doc,chain_type=\"stuff\")\n",
    "# Chain_doc(doc_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the above pdf Monolith: Real Time Recommendation System With\n",
      "Collisionless Embedding Table\n",
      "Zhuoran Liu\n",
      "Bytedance Inc.Leqi Zou\n",
      "Bytedance Inc.Xuan Zou\n",
      "Bytedance Inc.\n",
      "Caihua Wang\n",
      "Bytedance Inc.Biao Zhang\n",
      "Bytedance Inc.Da Tang\n",
      "Bytedance Inc.\n",
      "Bolin Zhu∗\n",
      "Fudan UniversityYijie Zhu\n",
      "Bytedance Inc.Peng Wu\n",
      "Bytedance Inc.\n",
      "Ke Wang\n",
      "Bytedance Inc.Youlong Cheng†\n",
      "Bytedance Inc.\n",
      "youlong.cheng@bytedance.com\n",
      "ABSTRACT\n",
      "Building a scalable and real-time recommendation system is vital\n",
      "for many businesses driven by time-sensitive customer feedback,\n",
      "such as short-videos ranking or online ads. Despite the ubiquitous\n",
      "adoption of production-scale deep learning frameworks like Ten-\n",
      "sorFlow or PyTorch, these general-purpose frameworks fall short\n",
      "of business demands in recommendation scenarios for various rea-\n",
      "sons: on one hand, tweaking systems based on static parameters and\n",
      "dense computations for recommendation with dynamic and sparse\n",
      "features is detrimental to model quality; on the other hand, such\n",
      "frameworks are designed with batch-training stage and serving\n",
      "stage completely separated, preventing the model from interacting\n",
      "with customer feedback in real-time. These issues led us to reex-\n",
      "amine traditional approaches and explore radically different design\n",
      "choices. In this paper, we present Monolith1, a system tailored\n",
      "for online training. Our design has been driven by observations\n",
      "of our application workloads and production environment that re-\n",
      "flects a marked departure from other recommendations systems.\n",
      "Our contributions are manifold: first, we crafted a collisionless em-\n",
      "bedding table with optimizations such as expirable embeddings\n",
      "and frequency filtering to reduce its memory footprint; second, we\n",
      "provide an production-ready online training architecture with high\n",
      "fault-tolerance; finally, we proved that system reliability could be\n",
      "traded-off for real-time learning. Monolith has successfully landed\n",
      "in the BytePlus Recommend2product.\n",
      "∗Work done during internship at Bytedance Inc.\n",
      "†Corresponding author.\n",
      "1Code to be released soon.\n",
      "2https://www.byteplus.com/en/product/recommend\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "©2022 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\n",
      "https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\n",
      "Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\n",
      "Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\n",
      "Real Time Recommendation System With Collisionless Embedding Table.\n",
      "InProceedings of 5th Workshop on Online Recommender Systems and User\n",
      "Modeling, in conjunction with the 16th ACM Conference on Recommender\n",
      "Systems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "1 INTRODUCTION\n",
      "The past decade witnessed a boom of businesses powered by recom-\n",
      "mendation techniques. In pursuit of a better customer experience,\n",
      "delivering personalized content for each individual user as real-time\n",
      "response is a common goal of these business applications. To this\n",
      "end, information from a user’s latest interaction is often used as\n",
      "the primary input for training a model, as it would best depict a\n",
      "user’s portrait and make predictions of user’s interest and future\n",
      "behaviors.\n",
      "Deep learning have been dominating recommendation models\n",
      "[5,6,10,12,20,21] as the gigantic amount of user data is a natural\n",
      "fit for massively data-driven neural models. However, efforts to\n",
      "leverage the power of deep learning in industry-level recommen-\n",
      "dation systems are constantly encountered with problems arising\n",
      "from the unique characteristics of data derived from real-world\n",
      "user behavior. These data are drastically different from those used\n",
      "in conventional deep learning problems like language modeling or\n",
      "computer vision in two aspects:\n",
      "(1)The features are mostly sparse, categorical and dynamically\n",
      "changing;\n",
      "(2)The underlying distribution of training data is non-stationary,\n",
      "a.k.a. Concept Drift [8].\n",
      "Such differences have posed unique challenges to researchers\n",
      "and engineers working on recommendation systems.\n",
      "1.1 Sparsity and Dynamism\n",
      "The data for recommendation mostly contain sparse categorical\n",
      "features, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "Data \n",
      "(Batch Training) \n",
      "Data \n",
      "(Online Training) \n",
      "Training \n",
      "Worker \n",
      "Training PS \n",
      " Serving PS \n",
      "Model \n",
      "Server \n",
      "User \n",
      "Batch \n",
      "Training \n",
      "Stage \n",
      "Online \n",
      "Training \n",
      "Stage \n",
      "Historical batch data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply gradient \n",
      "updates \n",
      "Online streaming \n",
      "data \n",
      "Feature IDs \n",
      "Feature embeddings \n",
      "Feature IDs and gradients Embedding table \n",
      "lookup \n",
      "Model forward \n",
      "and backward pass \n",
      "Apply \n",
      "gradient \n",
      "updates \n",
      "Parameter \n",
      "Sync \n",
      "Parameter \n",
      "Sync Sync \n",
      "interval \n",
      "User Request \n",
      "Feature IDs \n",
      "Feature \n",
      "embeddings Embedding \n",
      "table lookup \n",
      "Ranking Result Model \n",
      "forward \n",
      "pass \n",
      "User Actions \n",
      "Data of \n",
      "features and \n",
      " user reactions \n",
      "Figure 1: Monolith Online Training Architecture.\n",
      "practice of mapping them to a high-dimensional embedding space\n",
      "would give rise to a series of issues:\n",
      "•Unlike language models where number of word-pieces are\n",
      "limited, the amount of users and ranking items are orders of\n",
      "magnitude larger. Such an enormous embedding table would\n",
      "hardly fit into single host memory;\n",
      "•Worse still, the size of embedding table is expected to grow\n",
      "over time as more users and items are admitted, while frame-\n",
      "works like [ 1,17] uses a fixed-size dense variables to repre-\n",
      "sent embedding table.\n",
      "In practice, many systems adopt low-collision hashing [ 3,6] as a\n",
      "way to reduce memory footprint and to allow growing of IDs. This\n",
      "relies on an over-idealistic assumption that IDs in the embedding\n",
      "table is distributed evenly in frequency, and collisions are harmless\n",
      "to the model quality. Unfortunately this is rarely true for a real-\n",
      "world recommendation system, where a small group of users or\n",
      "items have significantly more occurrences. With the organic growth\n",
      "of embedding table size, chances of hash key collision increases\n",
      "and lead to deterioration of model quality [3].\n",
      "Therefore it is a natural demand for production-scale recommen-\n",
      "dation systems to have the capacity to capture as many features in\n",
      "its parameters, and also have the capability of elastically adjusting\n",
      "the number of users and items it tries to book-keep.1.2 Non-stationary Distribution\n",
      "Visual and linguistic patterns barely develop in a time scale of\n",
      "centuries, while the same user interested in one topic could shift\n",
      "their zeal every next minute. As a result, the underlying distribution\n",
      "of user data is non-stationary, a phenomenon commonly referred\n",
      "to as Concept Drift [8].\n",
      "Intuitively, information from a more recent history can more\n",
      "effectively contribute to predicting the change in a user’s behavior.\n",
      "To mitigate the effect of Concept Drift, serving models need to be\n",
      "updated from new user feedback as close to real-time as possible to\n",
      "reflect the latest interest of a user.\n",
      "In light of these distinction and in observation of issues that arises\n",
      "from our production, we designed Monolith , a large-scale recom-\n",
      "mendation system to address these pain-points. We did extensive\n",
      "experiments to verify and iterate our design in the production\n",
      "environment. Monolith is able to\n",
      "(1)Provide full expressive power for sparse features by design-\n",
      "ing a collisionless hash table and a dynamic feature eviction\n",
      "mechanism;\n",
      "(2)Loop serving feedback back to training in real-time with\n",
      "online training.\n",
      "Empowered by these architectural capacities, Monolith consis-\n",
      "tently outperforms systems that adopts hash-tricks with collisions\n",
      "with roughly similar memory usage, and achieves state-of-the-artMonolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "online serving AUC without overly burdening our servers’ compu-\n",
      "tation power.\n",
      "The rest of the paper is organized as follows. We first elaborate\n",
      "design details of how Monolith tackles existing challenge with colli-\n",
      "sionless hash table and realtime training in Section 2. Experiments\n",
      "and results will be presented in Section 3, along with production-\n",
      "tested conclusions and some discussion of trade-offs between time-\n",
      "sensitivity, reliability and model quality. Section 4 summarizes re-\n",
      "lated work and compares them with Monolith. Section 5 concludes\n",
      "this work.\n",
      "Client \n",
      " Master \n",
      "Worker \n",
      " Worker \n",
      "Parameter \n",
      "Server \n",
      "Parameter \n",
      "Server \n",
      ". . .\n",
      ". . .\n",
      "Figure 2: Worker-PS Architecture.\n",
      "2 DESIGN\n",
      "The overall architecture of Monolith generally follows TensorFlow’s\n",
      "distributed Worker- Parameter Server setting (Figure 2). In a Worker-\n",
      "PS architecture, machines are assigned different roles; Worker ma-\n",
      "chines are responsible for performing computations as defined by\n",
      "the graph, and PS machines stores parameters and updates them\n",
      "according to gradients computed by Workers.\n",
      "In recommendation models, parameters are categorized into two\n",
      "sets: dense and sparse. Dense parameters are weights/variables in\n",
      "a deep neural network, and sparse parameters refer to embedding\n",
      "tables that corresponds to sparse features. In our design, both dense\n",
      "and sparse parameters are part of TensorFlow Graph, and are stored\n",
      "on parameter servers.\n",
      "Similar to TensorFlow’s Variable for dense parameters, we de-\n",
      "signed a set of highly-efficient, collisionless, and flexible HashTable\n",
      "B A D C G\n",
      "D E F C BT0\n",
      "T1A\n",
      "h0(A) \n",
      "h1(B) h0(C) h1(D) \n",
      "Figure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\n",
      "Flow’s limitation that arises from separation of training and in-\n",
      "ference, Monolith’s elastically scalable online training is designed\n",
      "to efficiently synchronize parameters from training-PS to online\n",
      "serving-PS within short intervals, with model robustness guarantee\n",
      "provided by fault tolerance mechanism.\n",
      "2.1 Hash Table\n",
      "A first principle in our design of sparse parameter representation\n",
      "is to avoid cramping information from different IDs into the same\n",
      "fixed-size embedding. Simulating a dynamic size embedding table\n",
      "with an out-of-the-box TensorFlow Variable inevitably leads to ID\n",
      "collision, which exacerbates as new IDs arrive and table grows.\n",
      "Therefore instead of building upon Variable , we developed a new\n",
      "key-value HashTable for our sparse parameters.\n",
      "OurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\n",
      "which supports inserting new keys without colliding with existing\n",
      "ones. Cuckoo Hashing achieves worst-case 𝑂(1)time complexity\n",
      "for lookups and deletions, and an expected amortized 𝑂(1)time for\n",
      "insertions. As illustrated in Figure 3 it maintains two tables 𝑇0,𝑇1\n",
      "with different hash functions ℎ0(𝑥),ℎ1(𝑥), and an element would\n",
      "be stored in either one of them. When trying to insert an element\n",
      "𝐴into𝑇0, it first attempts to place 𝐴atℎ0(𝐴); Ifℎ0(𝐴)is occupied\n",
      "by another element 𝐵, it would evict 𝐵from𝑇0and try inserting 𝐵\n",
      "into𝑇1with the same logic. This process will be repeated until all\n",
      "elements stabilize, or rehash happens when insertion runs into a\n",
      "cycle.\n",
      "Memory footprint reduction is also an important consideration\n",
      "in our design. A naive approach of inserting every new ID into\n",
      "theHashTable will deplete memory quickly. Observation of real\n",
      "production models lead to two conclusions:\n",
      "(1)IDs that appears only a handful of times have limited con-\n",
      "tribution to improving model quality. An important obser-\n",
      "vation is that IDs are long-tail distributed, where popular\n",
      "IDs may occur millions of times while the unpopular ones\n",
      "appear no more than ten times. Embeddings corresponding\n",
      "to these infrequent IDs are underfit due to lack of training\n",
      "data and the model will not be able to make a good estima-\n",
      "tion based on them. At the end of the day these IDs are not\n",
      "likely to affect the result, so model quality will not suffer\n",
      "from removal of these IDs with low occurrences;\n",
      "(2)Stale IDs from a distant history seldom contribute to the\n",
      "current model as many of them are never visited. This could\n",
      "possibly due to a user that is no longer active, or a short-\n",
      "video that is out-of-date. Storing embeddings for these IDs\n",
      "could not help model in any way but to drain our PS memory\n",
      "in vain.\n",
      "Based on these observation, we designed several feature ID fil-\n",
      "tering heuristics for a more memory-efficient implementation of\n",
      "HashTable :\n",
      "(1)IDs are filtered before they are admitted into embedding\n",
      "tables. We have two filtering methods: First we filter by\n",
      "their occurrences before they are inserted as keys, where the\n",
      "threshold of occurrences is a tunable hyperparameter that\n",
      "varies for each model; In addition we utilize a probabilistic\n",
      "filter which helps further reduce memory usage;ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "User \n",
      " Log Kafka \n",
      "Actions \n",
      "(Click, Like, \n",
      "Convert …) \n",
      "Feature Kafka \n",
      " Model Server \n",
      " Training Worker \n",
      "Training PS \n",
      " Serving PS \n",
      "Joiner \n",
      "Flink Job \n",
      "Training Example \n",
      "Kafka \n",
      "Batch Training Data \n",
      "HDFS \n",
      "Generate \n",
      "Features \n",
      "Parameter Synchronization \n",
      "Dump Batch Data \n",
      "Online \n",
      "Training \n",
      "Batch Training \n",
      "Figure 4: Streaming Engine.\n",
      "The information feedback loop from [User →Model Server→Training Worker→Model Server→User] would spend a long time when taking the Batch Training\n",
      "path, while the Online Training will close the loop more instantly.\n",
      "(2)IDs are timed and set to expire after being inactive for a\n",
      "predefined period of time. The expire time is also tunable for\n",
      "each embedding table to allow for distinguishing features\n",
      "with different sensitivity to historical information.\n",
      "In our implementation, HashTable is implemented as a Tensor-\n",
      "Flow resource operation. Similar to Variable , look-ups and updates\n",
      "are also implemented as native TensorFlow operations for easier\n",
      "integration and better compatibility.\n",
      "2.2 Online Training\n",
      "In Monolith, training is divided into two stages (Figure 1):\n",
      "(1)Batch training stage. This stage works as an ordinary Tensor-\n",
      "Flow training loop: In each training step, a training worker\n",
      "reads one mini-batch of training examples from the stor-\n",
      "age, requests parameters from PS, computes a forward and\n",
      "a backward pass, and finally push updated parameters to\n",
      "the training PS. Slightly different from other common deep\n",
      "learning tasks, we only train our dataset for one pass. Batch\n",
      "training is useful for training historical data when we modify\n",
      "our model architecture and retrain the model;\n",
      "(2)Online training stage. After a model is deployed to online\n",
      "serving, the training does not stop but enters the online train-\n",
      "ing stage. Instead of reading mini-batch examples from the\n",
      "storage, a training worker consumes realtime data on-the-fly\n",
      "and updates the training PS. The training PS periodically syn-\n",
      "chronizes its parameters to the serving PS, which will take\n",
      "effect on the user side immediately. This enables our model\n",
      "to interactively adapt itself according to a user’s feedback in\n",
      "realtime.2.2.1 Streaming Engine. Monolith is built with the capability of\n",
      "seamlessly switching between batch training and online training.\n",
      "This is enabled by our design of streaming engine as illustrated by\n",
      "Figure 4.\n",
      "In our design, we use one Kafka [ 13] queue to log actions of users\n",
      "(E.g. Click on an item or like an item etc.) and another Kafka queue\n",
      "for features. At the core of the engine is a Flink [ 4] streaming job for\n",
      "online feature Joiner. The online joiner concatenates features with\n",
      "labels from user actions and produces training examples, which are\n",
      "then written to a Kafka queue. The queue for training examples is\n",
      "consumed by both online training and batch training:\n",
      "•For online training, the training worker directly reads data\n",
      "from the Kafka queue;\n",
      "•For batch training, a data dumping job will first dump data\n",
      "to HDFS [ 18]; After data in HDFS accumulated to certain\n",
      "amount, training worker will retrieve data from HDFS and\n",
      "perform batch training.\n",
      "Updated parameters in training PS will be pushed to serving PS\n",
      "according to the parameter synchronization schedule.\n",
      "2.2.2 Online Joiner. In real-world applications, user actions log\n",
      "and features are streamed into the online joiner (Figure 5) without\n",
      "guarantee in time order. Therefore we use a unique key for each\n",
      "request so that user action and features could correctly pair up.\n",
      "The lag of user action could also be a problem. For example, a\n",
      "user may take a few days before they decide to buy an item they\n",
      "were presented days ago. This is a challenge for the joiner because\n",
      "if all features are kept in cache, it would simply not fit in memory.\n",
      "In our system, an on-disk key-value storage is utilized to store\n",
      "features that are waiting for over certain time period. When a user\n",
      "action log arrives, it first looks up the in-memory cache, and then\n",
      "looks up the key-value storage in case of a missing cache.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "Log Kafka \n",
      "(User Actions) \n",
      "Feature Kafka \n",
      "In-memory \n",
      "Cache \n",
      "On-disk \n",
      "KV-Store \n",
      "Join \n",
      "Negative \n",
      "Sampling \n",
      "Training Example \n",
      "Kafka \n",
      "Found in Cache \n",
      "Read from KV-Store \n",
      "Figure 5: Online Joiner.\n",
      "Another problem that arise in real-world application is that\n",
      "the distribution of negative and positive examples are highly un-\n",
      "even, where number of the former could be magnitudes of order\n",
      "higher than the latter. To prevent positive examples from being\n",
      "overwhelmed by negative ones, a common strategy is to do negative\n",
      "sampling. This would certainly change the underlying distribution\n",
      "of the trained model, tweaking it towards higher probability of\n",
      "making positive predictions. As a remedy, we apply log odds cor-\n",
      "rection [ 19] during serving, making sure that the online model is\n",
      "an unbiased estimator of the original distribution.\n",
      "2.2.3 Parameter Synchronization. During online training, the Mono-\n",
      "lith training cluster keeps receiving data from the online serving\n",
      "module and updates parameters on the training PS. A crucial step\n",
      "to enable the online serving PS to benefit from these newly trained\n",
      "parameters is the synchronization of updated model parameters. In\n",
      "production environment, we are encountered by several challenges:\n",
      "•Models on the online serving PS must not stop serving when\n",
      "updating. Our models in production is usually several ter-\n",
      "abytes in size, and as a result replacing all parameters takes a\n",
      "while. It would be intolerable to stop an online PS from serv-\n",
      "ing the model during the replacement process, and updates\n",
      "must be made on-the-fly;\n",
      "•Transferring a multi-terabyte model of its entirety from train-\n",
      "ing PS to the online serving PS would pose huge pressure\n",
      "to both the network bandwidth and memory on PS, since it\n",
      "requires doubled model size of memory to accept the newly\n",
      "arriving model.\n",
      "For the online training to scale up to the size of our business\n",
      "scenario, we designed an incremental on-the-fly periodic parameter\n",
      "synchronization mechanism in Monolith based on several notice-\n",
      "able characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\n",
      "tion models;\n",
      "(2)Given a short range of time window, only a small subset of\n",
      "IDs gets trained and their embeddings updated;\n",
      "(3)Dense variables move much slower than sparse embeddings.\n",
      "This is because in momentum-based optimizers, the accumu-\n",
      "lation of momentum for dense variables is magnified by the\n",
      "gigantic size of recommendation training data, while only\n",
      "a few sparse embeddings receives updates in a single data\n",
      "batch.\n",
      "(1) and (2) allows us to exploit the sparse updates across all feature\n",
      "IDs. In Monolith, we maintain a hash set of touched keys, repre-\n",
      "senting IDs whose embeddings get trained since the last parameter\n",
      "synchronization. We push the subset of sparse parameters whose\n",
      "keys are in the touched-keys set with a minute-level time interval\n",
      "from the training PS to the online serving PS. This relatively small\n",
      "pack of incremental parameter update is lightweight for network\n",
      "transmission and will not cause a sharp memory spike during the\n",
      "synchronization.\n",
      "We also exploit (3) to further reduce network I/O and memory\n",
      "usage by setting a more aggressive sync schedule for sparse pa-\n",
      "rameters, while updating dense parameters less frequently. This\n",
      "could render us a situation where the dense parameters we serve is\n",
      "a relatively stale version compared to sparse part. However, such\n",
      "inconsistency could be tolerated due to the reason mentioned in (3)\n",
      "as no conspicuous loss has been observed.\n",
      "2.3 Fault Tolerance\n",
      "As a system in production, Monolith is designed with the ability to\n",
      "recover a PS in case it fails. A common choice for fault tolerance is\n",
      "to snapshot the state of a model periodically, and recover from theORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "latest snapshot when PS failure is detected. The choice of snapshot\n",
      "frequency has two major impacts:\n",
      "(1)Model quality. Intuitively, model quality suffers less from\n",
      "loss of recent history with increased snapshot frequency.\n",
      "(2)Computation overhead. Snapshotting a multi-terabyte model\n",
      "is not free. It incurs large chunks of memory copy and disk\n",
      "I/O.\n",
      "As a trade-off between model quality and computation overhead,\n",
      "Monolith snapshots all training PS every day. Though a PS will lose\n",
      "one day’s worth of update in case of a failure, we discover that the\n",
      "performance degradation is tolerable through our experiments. We\n",
      "will analyze the effect of PS reliability in the next section.\n",
      ". . .\n",
      ". . .\n",
      "IDsEmbeddings FMMLP Output \n",
      "Figure 6: DeepFM model architecture.\n",
      "3 EVALUATION\n",
      "For a better understanding of benefits and trade-offs brought about\n",
      "by our proposed design, we conducted several experiments at pro-\n",
      "duction scale and A/B test with live serving traffic to evaluate and\n",
      "verify Monolith from different aspects. We aim to answer the fol-\n",
      "lowing questions by our experiments:\n",
      "(1) How much can we benefit from a collisionless HashTable ?\n",
      "(2) How important is realtime online training?\n",
      "(3)Is Monolith’s design of parameter synchronization robust\n",
      "enough in a large-scale production scenario?\n",
      "In this section, we first present our experimental settings and\n",
      "then discuss results and our findings in detail.\n",
      "3.1 Experimental Setup\n",
      "3.1.1 Embedding Table. As described in Section 2.1, embedding\n",
      "tables in Monolith are implemented as collisionless HashTable s.\n",
      "To prove the necessity of avoiding collisions in embedding tables\n",
      "and to quantify gains from our collisionless implementation, we\n",
      "performed two groups of experiments on the Movielens dataset\n",
      "and on our internal production dataset respectively:\n",
      "(1)MovieLens ml-25m dataset [ 11]. This is a standard public\n",
      "dataset for movie ratings, containing 25 million ratings that\n",
      "involves approximately 162000 users and 62000 movies.\n",
      "•Preprocessing of labels . The original labels are ratings from\n",
      "0.5 to 5.0, while in production our tasks are mostly re-\n",
      "ceiving binary signals from users. To better simulate our\n",
      "production models, we convert scale labels to binary labelsby treating scores≥3.5as positive samples and the rest\n",
      "as negative samples.\n",
      "•Model and metrics . We implemented a standard DeepFM\n",
      "[9] model, a commonly used model architecture for recom-\n",
      "mendation problems. It consist of an FM component and a\n",
      "dense component (Figure 6). Predictions are evaluated by\n",
      "AUC [ 2] as this is the major measurement for real models.\n",
      "•Embedding collisions . This dataset contains approximately\n",
      "160K user IDs and 60K movie IDs. To compare with the\n",
      "collisionless version of embedding table implementation,\n",
      "we performed another group of experiment where IDs are\n",
      "preprocessed with MD5 hashing and then mapped to a\n",
      "smaller ID space. As a result, some IDs will share their\n",
      "embedding with others. Table 1 shows detailed statistics\n",
      "of user and movie IDs before and after hashing.\n",
      "User IDs Movie IDs\n",
      "# Before Hashing 162541 59047\n",
      "# After Hashing 149970 57361\n",
      "Collision rate 7.73% 2 .86%\n",
      "Table 1: Statistics of IDs Before and After Hashing.\n",
      "(2)Internal Recommendation dataset .\n",
      "We also performed experiments on a recommendation model\n",
      "in production environment. This model generally follows a\n",
      "multi-tower architecture, with each tower responsible for\n",
      "learning to predict a specialized kind of user behavior.\n",
      "•Each model has around 1000 embedding tables, and distri-\n",
      "bution of size of embedding tables are very uneven;\n",
      "•The original ID space of embedding table was 248. In our\n",
      "baseline, we applied a hashing trick by decomposing to\n",
      "curb the size of embedding table. To be more specific, we\n",
      "use two smaller embedding tables instead of a gigantic\n",
      "one to generate a unique embedding for each ID by vector\n",
      "combination:\n",
      "𝐼𝐷𝑟=𝐼𝐷%224\n",
      "𝐼𝐷𝑞=𝐼𝐷÷224\n",
      "𝐸=𝐸𝑟+𝐸𝑞,\n",
      "where𝐸𝑟,𝐸𝑞are embeddings corresponding to 𝐼𝐷𝑟,𝐼𝐷𝑞.\n",
      "This effectively reduces embedding table sizes from 248\n",
      "to225;\n",
      "•This model is serving in real production, and the perfor-\n",
      "mance of this experiment is measured by online AUC with\n",
      "real serving traffic.\n",
      "3.1.2 Online Training. During online training, we update our on-\n",
      "line serving PS with the latest set of parameters with minute-level\n",
      "intervals. We designed two groups of experiments to verify model\n",
      "quality and system robustness.\n",
      "(1)Update frequency . To investigate the necessity of minute-\n",
      "level update frequency, we conducted experiments that syn-\n",
      "chronize parameters from training model to prediction model\n",
      "with different intervals.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "Algorithm 1 Simulated Online Training.\n",
      "1:Input:𝐷𝑏𝑎𝑡𝑐ℎ; /* Data for batch training. */\n",
      "2:Input:𝐷𝑜𝑛𝑙𝑖𝑛𝑒\n",
      "𝑖=1···𝑁; /* Data for online training, split into 𝑁shards. */\n",
      "3:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑏𝑎𝑡𝑐ℎ,𝜃𝑡𝑟𝑎𝑖𝑛); /* Batch training. */\n",
      "/* Online training. */\n",
      "4:for𝑖=1···𝑁do\n",
      "5:𝜃𝑠𝑒𝑟𝑣𝑒←𝜃𝑡𝑟𝑎𝑖𝑛 ; /* Sync training parameters to serving model. */\n",
      "6:𝐴𝑈𝐶𝑖=Evaluate(𝜃𝑠𝑒𝑟𝑣𝑒,𝐷𝑜𝑛𝑙𝑖𝑛𝑒\n",
      "𝑖); /* Evaluate online prediction on new data. */\n",
      "7:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑜𝑛𝑙𝑖𝑛𝑒\n",
      "𝑖,𝜃𝑡𝑟𝑎𝑖𝑛); /* Train with new data. */\n",
      "8:end for\n",
      "The dataset we use is the Criteo Display Ads Challenge\n",
      "dataset3, a large-scale standard dataset for benchmarking\n",
      "CTR models. It contains 7 days of chronologically ordered\n",
      "data recording features and click actions. For this experiment,\n",
      "we use a standard DeepFM [9] model as described in 6.\n",
      "To simulate online training, we did the following preprocess-\n",
      "ing for the dataset. We take 7 days of data from the dataset,\n",
      "and split it to two parts: 5 days of data for batch training, and\n",
      "2 days for online training. We further split the 2 days of data\n",
      "into𝑁shards chronologically. Online training is simulated\n",
      "by algorithm 1.\n",
      "As such, we simulate synchronizing trained parameters to\n",
      "online serving PS with an interval determined by number of\n",
      "data shards. We experimented with 𝑁=10,50,100, which\n",
      "roughly correspond to update interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛.\n",
      "(2)Live experiment . In addition, we also performed a live ex-\n",
      "periment with real serving traffic to further demonstrate the\n",
      "importance of online training in real-world application. This\n",
      "A/B experiment compares online training to batch training\n",
      "one one of our Ads model in production.\n",
      "3.2 Results and Analysis\n",
      "3.2.1 The Effect of Embedding Collision. Results from MovieLens\n",
      "dataset and the Internal recommedation dataset both show that\n",
      "embedding collisions will jeopardize model quality.\n",
      "(1)Models with collisionless HashTable consistently outper-\n",
      "forms those with collision. This conclusion holds true re-\n",
      "gardless of\n",
      "•Increase of number of training epochs. As shown in Figure\n",
      "7, the model with collisionless embedding table has higher\n",
      "AUC from the first epoch and converges at higher value;\n",
      "•Change of distribution with passage of time due to Con-\n",
      "cept Drift. As shown in Figure 8, models with collision-\n",
      "less embedding table is also robust as time passes by and\n",
      "users/items context changes.\n",
      "(2)Data sparsity caused by collisionless embedding table will\n",
      "not lead to model overfitting. As shown in Figure 7, a model\n",
      "with collisionless embedding table does not overfit after it\n",
      "converges.\n",
      "3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "epoch0.8050.8100.8150.8200.825test auccollision-free hash\n",
      "hash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\n",
      "MovieLens\n",
      "2 4 6 8 10 12\n",
      "Day0.7700.7750.7800.7850.790Online serving AUChash w/ collision\n",
      "collisionless hash table\n",
      "Figure 8: Effect of Embedding Collision On A\n",
      "Recommendation Model In Production\n",
      "We measure performance of this recommendation model by online serving AUC,\n",
      "which is fluctuating across different days due to concept-drift.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "0 10 20 30 40 50\n",
      "Hours0.7920.7940.7960.798AUCw/ online training\n",
      "w/o online training\n",
      "(a) Online training with 5hrs sync interval\n",
      "0 10 20 30 40 50\n",
      "Hours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\n",
      "w/o online training (b) Online training with 1hr sync interval\n",
      "0 10 20 30 40 50\n",
      "Hours0.7900.7950.8000.8050.810AUCw/ online training\n",
      "w/o online training\n",
      "(c) Online training with 30min sync interval\n",
      "Figure 9: Online training v.s. Batch training on Criteo dataset.\n",
      "Blue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\n",
      "3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\n",
      "covered that a higher parameter synchronization frequency is al-\n",
      "ways conducive to improving online serving AUC, and that online\n",
      "serving models are more tolerant with loss of a few shard of PS\n",
      "than we expect.\n",
      "(1)The Effect of Parameter Synchronization Frequency .\n",
      "In our online streaming training experiment (1) with Criteo\n",
      "Display Ads Challenge dataset, model quality consistently\n",
      "improves with the increase of parameter synchronization\n",
      "frequency, as is evident by comparison from two perspec-\n",
      "tives:\n",
      "•Models with online training performs better than models\n",
      "without. Figure 9a, 9b, 9c compares AUC of online training\n",
      "models evaluated by the following shard of data versus\n",
      "batch training models evaluated by each shard of data;•Models with smaller parameter synchronization interval\n",
      "performs better that those with larger interval. Figure 10\n",
      "and Table 2 compares online serving AUC for models with\n",
      "sync interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛 respectively.\n",
      "Sync Interval Average AUC (online) Average AUC (batch)\n",
      "5 hr 79.66±0.020 79 .42±0.026\n",
      "1 hr 79.78±0.005 79 .44±0.030\n",
      "30 min 79.80±0.008 79 .43±0.025\n",
      "Table 2: Average AUC comparison for DeepFM model on\n",
      "Criteo dataset.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "Day 1 2 3 4 5 6 7\n",
      "AUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\n",
      "Table 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\n",
      "0 10 20 30 40 50\n",
      "Hours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\n",
      "Sync Interval 1 hr\n",
      "Sync Interval 30 min\n",
      "Figure 10: Comparison of different sync intervals for\n",
      "online training.\n",
      "The live A/B experiment between online training and batch\n",
      "training on an Ads model in production also show that there\n",
      "is a significant bump in online serving AUC (Table 3).\n",
      "Inspired by this observation, we synchronize sparse parame-\n",
      "ters to serving PS of our production models as frequent as\n",
      "possible (currently at minute-level), to the extent that the\n",
      "computation overhead and system reliability could endure.\n",
      "Recall that dense variables requires a less frequent update as\n",
      "discussed in 2.2.3, we update them at day-level. By doing so,\n",
      "we can bring down our computation overhead to a very low\n",
      "level. Suppose 100,000 IDs gets updated in a minute, and the\n",
      "dimension of embedding is 1024, the total size of data need\n",
      "to be transferred is 4𝐾𝐵×100,000≈400𝑀𝐵per minute.\n",
      "For dense parameters, since they are synchronized daily, we\n",
      "choose to schedule the synchronization when the traffic is\n",
      "lowest (e.g. midnight).\n",
      "(2)The Effect of PS reliability.\n",
      "With a minute-level parameter synchronization, we initially\n",
      "expect a more frequent snapshot of training PS to match the\n",
      "realtime update. To our surprise, we enlarged the snapshot\n",
      "interval to 1 day and still observed nearly no loss of model\n",
      "quality.\n",
      "Finding the right trade-off between model quality and com-\n",
      "putation overhead is difficult for personalized ranking sys-\n",
      "tems since users are extremely sensitive on recommendation\n",
      "quality. Traditionally, large-scale systems tend to set a fre-\n",
      "quent snapshot schedule for their models, which sacrifices\n",
      "computation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\n",
      "regard and to our surprise, model quality is more robust than\n",
      "expected. With a 0.01% failure rate of PS machine per day,\n",
      "we find a model from the previous day works embarrassingly\n",
      "well. This is explicable by the following calculation: Suppose\n",
      "a model’s parameters are sharded across 1000 PS, and they\n",
      "snapshot every day. Given 0.01% failure rate, one of them\n",
      "will go down every 10 days and we lose all updates on this\n",
      "PS for 1 day. Assuming a DAU of 15 Million and an even\n",
      "distribution of user IDs on each PS, we lose 1 day’s feedback\n",
      "from 15000 users every 10 days. This is acceptable because\n",
      "(a) For sparse features which is user-specific, this is equiv-\n",
      "alent to losing a tiny fraction of 0.01% DAU; (b) For dense\n",
      "variables, since they are updated slowly as we discussed in\n",
      "2.2.3, losing 1 day’s update out of 1000 PS is negligible.\n",
      "Based on the above observation and calculation, we radically\n",
      "lowered our snapshot frequency and thereby saved quite a\n",
      "bit in computation overhead.\n",
      "4 RELATED WORK\n",
      "Ever since some earliest successful application of deep learning to\n",
      "industry-level recommendation systems [ 6,10], researchers and\n",
      "engineers have been employing various techniques to ameliorate\n",
      "issues mentioned in Section 1.\n",
      "To tackle the issue of sparse feature representation, [ 3,6] uses\n",
      "fixed-size embedding table with hash-trick. There are also attempts\n",
      "in improving hashing to reduce collision [ 3,7]. Other works directly\n",
      "utilize native key-value hash table to allow dynamic growth of table\n",
      "size [ 12,15,20,21]. These implementations builds upon TensorFlow\n",
      "but relies either on specially designed software mechanism [ 14,\n",
      "15,20] or hardware [ 21] to access and manage their hash-tables.\n",
      "Compared to these solutions, Monolith’s hash-table is yet another\n",
      "native TensorFlow operation. It is developer friendly and has higher\n",
      "cross-platform interoperability, which is suitable for ToB scenarios.\n",
      "An organic and tight integration with TensorFlow also enables\n",
      "easier optimizations of computation performance.\n",
      "Bridging the gap between training and serving and alleviation\n",
      "of Concept Drift [ 8] is another topic of interest. To support online\n",
      "update and avoid memory issues, both [ 12] and [ 20] designed fea-\n",
      "ture eviction mechanisms to flexibly adjust the size of embedding\n",
      "tables. Both [ 12] and [ 14] support some form of online training,\n",
      "where learned parameters are synced to serving with a relatively\n",
      "short interval compared to traditional batch training, with fault tol-\n",
      "erance mechanisms. Monolith took similar approach to elastically\n",
      "admit and evict features, while it has a more lightweight parameter\n",
      "synchronization mechanism to guarantee model quality.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\n",
      "5 CONCLUSION\n",
      "In this work, we reviewed several most important challenges for\n",
      "industrial-level recommendation systems and present our system\n",
      "in production, Monolith, to address them and achieved best perfor-\n",
      "mance compared to existing solutions.\n",
      "We proved that a collisionless embedding table is essential for\n",
      "model quality, and demonstrated that our implementation of Cuckoo\n",
      "HashMap based embedding table is both memory efficient and help-\n",
      "ful for improving online serving metrics.\n",
      "We also proved that realtime serving is crucial in recommenda-\n",
      "tion systems, and that parameter synchronization interval should\n",
      "be as short as possible for an ultimate model performance. Our\n",
      "solution for online realtime serving in Monolith has a delicately\n",
      "designed parameter synchronization and a fault tolerance mecha-\n",
      "nism: In our parameter synchronization algorithm, we showed that\n",
      "consistency of version across different parts of parameters could be\n",
      "traded-off for reducing network bandwidth consumption; In fault\n",
      "tolerance design, we demonstrated that our strategy of trading-off\n",
      "PS reliability for realtime-ness is a robust solution.\n",
      "To conclude, Monolith succeeded in providing a general solution\n",
      "for production scale recommendation systems.\n",
      "ACKNOWLEDGMENTS\n",
      "Hanzhi Zhou provided useful suggestions on revision of this paper.\n",
      "REFERENCES\n",
      "[1]Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\n",
      "Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\n",
      "Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\n",
      "Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\n",
      "Yuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\n",
      "machine learning. ArXiv abs/1605.08695 (2016).\n",
      "[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\n",
      "evaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–\n",
      "1159.\n",
      "[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\n",
      "engineering.com/core-modeling-at-instagram-a51e0158aa48\n",
      "[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\n",
      "and Kostas Tzoumas. 2015. Apache Flink ™: Stream and Batch Processing in a\n",
      "Single Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\n",
      "[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\n",
      "Hrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\n",
      "Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\n",
      "Hemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\n",
      "of the 1st Workshop on Deep Learning for Recommender Systems (2016).\n",
      "[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\n",
      "for YouTube Recommendations. Proceedings of the 10th ACM Conference on\n",
      "Recommender Systems (2016).\n",
      "[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\n",
      "teenth ACM Conference on Recommender Systems (2021).\n",
      "[8]João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\n",
      "2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\n",
      "(2014), 1 – 37.\n",
      "[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\n",
      "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\n",
      "IJCAI .\n",
      "[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\n",
      "David M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\n",
      "Andrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\n",
      "Xuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based\n",
      "Personalized Recommendation. 2020 IEEE International Symposium on High\n",
      "Performance Computer Architecture (HPCA) (2020), 488–501.\n",
      "[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\n",
      "and Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\n",
      "[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\n",
      "Huang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\n",
      "Sun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\n",
      "Proceedings of the 1st International Workshop on Deep Learning Practice for High-\n",
      "Dimensional Sparse Data (2019).\n",
      "[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\n",
      "[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\n",
      "Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan\n",
      "Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\n",
      "Lin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\n",
      "bo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\n",
      "Scaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\n",
      "abs/2111.05897 (2021).\n",
      "[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\n",
      "mender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\n",
      "tensorflow-in-recommender-systems.html\n",
      "[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\n",
      "[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\n",
      "Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\n",
      "Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\n",
      "Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\n",
      "Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\n",
      "Library. In NeurIPS .\n",
      "[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\n",
      "2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\n",
      "Storage Systems and Technologies (MSST) (2010), 1–10.\n",
      "[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\n",
      "Sampling and Log Odds Correction with Rare Events Data. In Advances in Neural\n",
      "Information Processing Systems .\n",
      "[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\n",
      "Lin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\n",
      "Continual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\n",
      "national Conference for High Performance Computing, Networking, Storage and\n",
      "Analysis (2020), 1–17.\n",
      "[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\n",
      "2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\n",
      "28th ACM International Conference on Information and Knowledge Management\n",
      "(2019).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such\\nframeworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.\\n∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\\nZhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural\\nfit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical\\nfeatures, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\n•Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\n•Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a user’s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-artMonolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our servers’ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate\\ndesign details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlow’s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two\\nsets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlow’s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlow’s limitation that arises from separation of training and in-\\nference, Monolith’s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation\\nis to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case 𝑂(1)time complexity\\nfor lookups and deletions, and an expected amortized 𝑂(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables 𝑇0,𝑇1\\nwith different hash functions ℎ0(𝑥),ℎ1(𝑥), and an element would\\nbe stored in either one of them. When trying to insert an element\\n𝐴into𝑇0, it first attempts to place 𝐴atℎ0(𝐴); Ifℎ0(𝐴)is occupied\\nby another element 𝐵, it would evict 𝐵from𝑇0and try inserting 𝐵\\ninto𝑇1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;\\n(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nUser \\n Log Kafka \\nActions \\n(Click, Like, \\nConvert …) \\nFeature Kafka \\n Model Server \\n Training Worker \\nTraining PS \\n Serving PS \\nJoiner \\nFlink Job \\nTraining Example \\nKafka \\nBatch Training Data \\nHDFS \\nGenerate \\nFeatures \\nParameter Synchronization \\nDump Batch Data \\nOnline \\nTraining \\nBatch Training \\nFigure 4: Streaming Engine.\\nThe information feedback loop from [User →Model Server→Training Worker→Model Server→User] would spend a long time when taking the Batch Training\\npath, while the Online Training will close the loop more instantly.\\n(2)IDs are timed and set to expire after being inactive for a\\npredefined period of time. The expire time is also tunable for\\neach embedding table to allow for distinguishing features\\nwith different sensitivity to historical information.\\nIn our implementation, HashTable is implemented as a Tensor-\\nFlow resource operation. Similar to Variable , look-ups and updates\\nare also implemented as native TensorFlow operations for easier\\nintegration and better compatibility.\\n2.2 Online Training\\nIn Monolith, training is divided into two stages (Figure 1):\\n(1)Batch training stage. This stage works as an ordinary Tensor-\\nFlow training loop: In each training step, a training worker\\nreads one mini-batch of training examples from the stor-\\nage, requests parameters from PS, computes a forward and\\na backward pass, and finally push updated parameters to\\nthe training PS. Slightly different from other common deep\\nlearning tasks, we only train our dataset for one pass. Batch\\ntraining is useful for training historical data when we modify\\nour model architecture and retrain the model;\\n(2)Online training stage. After a model is deployed to online\\nserving, the training does not stop but enters the online train-\\ning stage. Instead of reading mini-batch examples from the\\nstorage, a training worker consumes realtime data on-the-fly\\nand updates the training PS. The training PS periodically syn-\\nchronizes its parameters to the serving PS, which will take\\neffect on the user side immediately. This enables our model\\nto interactively adapt itself according to a user’s feedback in\\nrealtime.2.2.1 Streaming Engine. Monolith is built with the capability of\\nseamlessly switching between batch training and online training.\\nThis is enabled by our design of streaming engine as illustrated by\\nFigure 4.\\nIn our design, we use one Kafka [ 13] queue to log actions of users\\n(E.g. Click on an item or like an item etc.) and another Kafka queue\\nfor features. At the core of the engine is a Flink [ 4] streaming job for\\nonline feature Joiner. The online joiner concatenates features with\\nlabels from user actions and produces training examples, which are\\nthen written to a Kafka queue. The queue for training examples is\\nconsumed by both online training and batch training:\\n•For online training, the training worker directly reads data\\nfrom the Kafka queue;\\n•For batch training, a data dumping job will first dump data\\nto HDFS [ 18]; After data in HDFS accumulated to certain\\namount, training worker will retrieve data from HDFS and\\nperform batch training.\\nUpdated parameters in training PS will be pushed to serving PS\\naccording to the parameter synchronization schedule.\\n2.2.2 Online Joiner. In real-world applications, user actions log\\nand features are streamed into the online joiner (Figure 5) without\\nguarantee in time order. Therefore we use a unique key for each\\nrequest so that user action and features could correctly pair up.\\nThe lag of user action could also be a problem. For example, a\\nuser may take a few days before they decide to buy an item they\\nwere presented days ago. This is a challenge for the joiner because\\nif all features are kept in cache, it would simply not fit in memory.\\nIn our system, an on-disk key-value storage is utilized to store\\nfeatures that are waiting for over certain time period. When a user\\naction log arrives, it first looks up the in-memory cache, and then\\nlooks up the key-value storage in case of a missing cache.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nLog Kafka \\n(User Actions) \\nFeature Kafka \\nIn-memory \\nCache \\nOn-disk \\nKV-Store \\nJoin \\nNegative \\nSampling \\nTraining Example \\nKafka \\nFound in Cache \\nRead from KV-Store \\nFigure 5: Online Joiner.\\nAnother problem that arise in real-world application is that\\nthe distribution of negative and positive examples are highly un-\\neven, where number of the former could be magnitudes of order\\nhigher than the latter. To prevent positive examples from being\\noverwhelmed by negative ones, a common strategy is to do negative\\nsampling. This would certainly change the underlying distribution\\nof the trained model, tweaking it towards higher probability of\\nmaking positive predictions. As a remedy, we apply log odds cor-\\nrection [ 19] during serving, making sure that the online model is\\nan unbiased estimator of the original distribution.\\n2.2.3 Parameter Synchronization. During online training, the Mono-\\nlith training cluster keeps receiving data from the online serving\\nmodule and updates parameters on the training PS. A crucial step\\nto enable the online serving PS to benefit from these newly trained\\nparameters is the synchronization of updated model parameters. In\\nproduction environment, we are encountered by several challenges:\\n•Models on the online serving PS must not stop serving when\\nupdating. Our models in production is usually several ter-\\nabytes in size, and as a result replacing all parameters takes a\\nwhile. It would be intolerable to stop an online PS from serv-\\ning the model during the replacement process, and updates\\nmust be made on-the-fly;\\n•Transferring a multi-terabyte model of its entirety from train-\\ning PS to the online serving PS would pose huge pressure\\nto both the network bandwidth and memory on PS, since it\\nrequires doubled model size of memory to accept the newly\\narriving model.\\nFor the online training to scale up to the size of our business\\nscenario, we designed an incremental on-the-fly periodic parameter\\nsynchronization mechanism in Monolith based on several notice-\\nable characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\\ntion models;\\n(2)Given a short range of time window, only a small subset of\\nIDs gets trained and their embeddings updated;\\n(3)Dense variables move much slower than sparse embeddings.\\nThis is because in momentum-based optimizers, the accumu-\\nlation of momentum for dense variables is magnified by the\\ngigantic size of recommendation training data, while only\\na few sparse embeddings receives updates in a single data\\nbatch.\\n(1) and (2) allows us to exploit the sparse updates across all feature\\nIDs. In Monolith, we maintain a hash set of touched keys, repre-\\nsenting IDs whose embeddings get trained since the last parameter\\nsynchronization. We push the subset of sparse parameters whose\\nkeys are in the touched-keys set with a minute-level time interval\\nfrom the training PS to the online serving PS. This relatively small\\npack of incremental parameter update is lightweight for network\\ntransmission and will not cause a sharp memory spike during the\\nsynchronization.\\nWe also exploit (3) to further reduce network I/O and memory\\nusage by setting a more aggressive sync schedule for sparse pa-\\nrameters, while updating dense parameters less frequently. This\\ncould render us a situation where the dense parameters we serve is\\na relatively stale version compared to sparse part. However, such\\ninconsistency could be tolerated due to the reason mentioned in (3)\\nas no conspicuous loss has been observed.\\n2.3 Fault Tolerance\\nAs a system in production, Monolith is designed with the ability to\\nrecover a PS in case it fails. A common choice for fault tolerance is\\nto snapshot the state of a model periodically, and recover from theORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nlatest snapshot when PS failure is detected. The choice of snapshot\\nfrequency has two major impacts:\\n(1)Model quality. Intuitively, model quality suffers less from\\nloss of recent history with increased snapshot frequency.\\n(2)Computation overhead. Snapshotting a multi-terabyte model\\nis not free. It incurs large chunks of memory copy and disk\\nI/O.\\nAs a trade-off between model quality and computation overhead,\\nMonolith snapshots all training PS every day. Though a PS will lose\\none day’s worth of update in case of a failure, we discover that the\\nperformance degradation is tolerable through our experiments. We\\nwill analyze the effect of PS reliability in the next section.\\n. . .\\n. . .\\nIDsEmbeddings FMMLP Output \\nFigure 6: DeepFM model architecture.\\n3 EVALUATION\\nFor a better understanding of benefits and trade-offs brought about\\nby our proposed design, we conducted several experiments at pro-\\nduction scale and A/B test with live serving traffic to evaluate and\\nverify Monolith from different aspects. We aim to answer the fol-\\nlowing questions by our experiments:\\n(1) How much can we benefit from a collisionless HashTable ?\\n(2) How important is realtime online training?\\n(3)Is Monolith’s design of parameter synchronization robust\\nenough in a large-scale production scenario?\\nIn this section, we first present our experimental settings and\\nthen discuss results and our findings in detail.\\n3.1 Experimental Setup\\n3.1.1 Embedding Table. As described in Section 2.1, embedding\\ntables in Monolith are implemented as collisionless HashTable s.\\nTo prove the necessity of avoiding collisions in embedding tables\\nand to quantify gains from our collisionless implementation, we\\nperformed two groups of experiments on the Movielens dataset\\nand on our internal production dataset respectively:\\n(1)MovieLens ml-25m dataset [ 11]. This is a standard public\\ndataset for movie ratings, containing 25 million ratings that\\ninvolves approximately 162000 users and 62000 movies.\\n•Preprocessing of labels . The original labels are ratings from\\n0.5 to 5.0, while in production our tasks are mostly re-\\nceiving binary signals from users. To better simulate our\\nproduction models, we convert scale labels to binary labelsby treating scores≥3.5as positive samples and the rest\\nas negative samples.\\n•Model and metrics . We implemented a standard DeepFM\\n[9] model, a commonly used model architecture for recom-\\nmendation problems. It consist of an FM component and a\\ndense component (Figure 6). Predictions are evaluated by\\nAUC [ 2] as this is the major measurement for real models.\\n•Embedding collisions . This dataset contains approximately\\n160K user IDs and 60K movie IDs. To compare with the\\ncollisionless version of embedding table implementation,\\nwe performed another group of experiment where IDs are\\npreprocessed with MD5 hashing and then mapped to a\\nsmaller ID space. As a result, some IDs will share their\\nembedding with others. Table 1 shows detailed statistics\\nof user and movie IDs before and after hashing.\\nUser IDs Movie IDs\\n# Before Hashing 162541 59047\\n# After Hashing 149970 57361\\nCollision rate 7.73% 2 .86%\\nTable 1: Statistics of IDs Before and After Hashing.\\n(2)Internal Recommendation dataset .\\nWe also performed experiments on a recommendation model\\nin production environment. This model generally follows a\\nmulti-tower architecture, with each tower responsible for\\nlearning to predict a specialized kind of user behavior.\\n•Each model has around 1000 embedding tables, and distri-\\nbution of size of embedding tables are very uneven;\\n•The original ID space of embedding table was 248. In our\\nbaseline, we applied a hashing trick by decomposing to\\ncurb the size of embedding table. To be more specific, we\\nuse two smaller embedding tables instead of a gigantic\\none to generate a unique embedding for each ID by vector\\ncombination:\\n𝐼𝐷𝑟=𝐼𝐷%224\\n𝐼𝐷𝑞=𝐼𝐷÷224\\n𝐸=𝐸𝑟+𝐸𝑞,\\nwhere𝐸𝑟,𝐸𝑞are embeddings corresponding to 𝐼𝐷𝑟,𝐼𝐷𝑞.\\nThis effectively reduces embedding table sizes from 248\\nto225;\\n•This model is serving in real production, and the perfor-\\nmance of this experiment is measured by online AUC with\\nreal serving traffic.\\n3.1.2 Online Training. During online training, we update our on-\\nline serving PS with the latest set of parameters with minute-level\\nintervals. We designed two groups of experiments to verify model\\nquality and system robustness.\\n(1)Update frequency . To investigate the necessity of minute-\\nlevel update frequency, we conducted experiments that syn-\\nchronize parameters from training model to prediction model\\nwith different intervals.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nAlgorithm 1 Simulated Online Training.\\n1:Input:𝐷𝑏𝑎𝑡𝑐ℎ; /* Data for batch training. */\\n2:Input:𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖=1···𝑁; /* Data for online training, split into 𝑁shards. */\\n3:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑏𝑎𝑡𝑐ℎ,𝜃𝑡𝑟𝑎𝑖𝑛); /* Batch training. */\\n/* Online training. */\\n4:for𝑖=1···𝑁do\\n5:𝜃𝑠𝑒𝑟𝑣𝑒←𝜃𝑡𝑟𝑎𝑖𝑛 ; /* Sync training parameters to serving model. */\\n6:𝐴𝑈𝐶𝑖=Evaluate(𝜃𝑠𝑒𝑟𝑣𝑒,𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖); /* Evaluate online prediction on new data. */\\n7:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖,𝜃𝑡𝑟𝑎𝑖𝑛); /* Train with new data. */\\n8:end for\\nThe dataset we use is the Criteo Display Ads Challenge\\ndataset3, a large-scale standard dataset for benchmarking\\nCTR models. It contains 7 days of chronologically ordered\\ndata recording features and click actions. For this experiment,\\nwe use a standard DeepFM [9] model as described in 6.\\nTo simulate online training, we did the following preprocess-\\ning for the dataset. We take 7 days of data from the dataset,\\nand split it to two parts: 5 days of data for batch training, and\\n2 days for online training. We further split the 2 days of data\\ninto𝑁shards chronologically. Online training is simulated\\nby algorithm 1.\\nAs such, we simulate synchronizing trained parameters to\\nonline serving PS with an interval determined by number of\\ndata shards. We experimented with 𝑁=10,50,100, which\\nroughly correspond to update interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛.\\n(2)Live experiment . In addition, we also performed a live ex-\\nperiment with real serving traffic to further demonstrate the\\nimportance of online training in real-world application. This\\nA/B experiment compares online training to batch training\\none one of our Ads model in production.\\n3.2 Results and Analysis\\n3.2.1 The Effect of Embedding Collision. Results from MovieLens\\ndataset and the Internal recommedation dataset both show that\\nembedding collisions will jeopardize model quality.\\n(1)Models with collisionless HashTable consistently outper-\\nforms those with collision. This conclusion holds true re-\\ngardless of\\n•Increase of number of training epochs. As shown in Figure\\n7, the model with collisionless embedding table has higher\\nAUC from the first epoch and converges at higher value;\\n•Change of distribution with passage of time due to Con-\\ncept Drift. As shown in Figure 8, models with collision-\\nless embedding table is also robust as time passes by and\\nusers/items context changes.\\n(2)Data sparsity caused by collisionless embedding table will\\nnot lead to model overfitting. As shown in Figure 7, a model\\nwith collisionless embedding table does not overfit after it\\nconverges.\\n3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\\n1 2 3 4 5 6 7 8 9 10\\nepoch0.8050.8100.8150.8200.825test auccollision-free hash\\nhash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\\nMovieLens\\n2 4 6 8 10 12\\nDay0.7700.7750.7800.7850.790Online serving AUChash w/ collision\\ncollisionless hash table\\nFigure 8: Effect of Embedding Collision On A\\nRecommendation Model In Production\\nWe measure performance of this recommendation model by online serving AUC,\\nwhich is fluctuating across different days due to concept-drift.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\n•Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;•Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛 respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66±0.020 79 .42±0.026\\n1 hr 79.78±0.005 79 .44±0.030\\n30 min 79.80±0.008 79 .43±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nDay 1 2 3 4 5 6 7\\nAUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\\nTable 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\\nSync Interval 1 hr\\nSync Interval 30 min\\nFigure 10: Comparison of different sync intervals for\\nonline training.\\nThe live A/B experiment between online training and batch\\ntraining on an Ads model in production also show that there\\nis a significant bump in online serving AUC (Table 3).\\nInspired by this observation, we synchronize sparse parame-\\nters to serving PS of our production models as frequent as\\npossible (currently at minute-level), to the extent that the\\ncomputation overhead and system reliability could endure.\\nRecall that dense variables requires a less frequent update as\\ndiscussed in 2.2.3, we update them at day-level. By doing so,\\nwe can bring down our computation overhead to a very low\\nlevel. Suppose 100,000 IDs gets updated in a minute, and the\\ndimension of embedding is 1024, the total size of data need\\nto be transferred is 4𝐾𝐵×100,000≈400𝑀𝐵per minute.\\nFor dense parameters, since they are synchronized daily, we\\nchoose to schedule the synchronization when the traffic is\\nlowest (e.g. midnight).\\n(2)The Effect of PS reliability.\\nWith a minute-level parameter synchronization, we initially\\nexpect a more frequent snapshot of training PS to match the\\nrealtime update. To our surprise, we enlarged the snapshot\\ninterval to 1 day and still observed nearly no loss of model\\nquality.\\nFinding the right trade-off between model quality and com-\\nputation overhead is difficult for personalized ranking sys-\\ntems since users are extremely sensitive on recommendation\\nquality. Traditionally, large-scale systems tend to set a fre-\\nquent snapshot schedule for their models, which sacrifices\\ncomputation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\\nregard and to our surprise, model quality is more robust than\\nexpected. With a 0.01% failure rate of PS machine per day,\\nwe find a model from the previous day works embarrassingly\\nwell. This is explicable by the following calculation: Suppose\\na model’s parameters are sharded across 1000 PS, and they\\nsnapshot every day. Given 0.01% failure rate, one of them\\nwill go down every 10 days and we lose all updates on this\\nPS for 1 day. Assuming a DAU of 15 Million and an even\\ndistribution of user IDs on each PS, we lose 1 day’s feedback\\nfrom 15000 users every 10 days. This is acceptable because\\n(a) For sparse features which is user-specific, this is equiv-\\nalent to losing a tiny fraction of 0.01% DAU; (b) For dense\\nvariables, since they are updated slowly as we discussed in\\n2.2.3, losing 1 day’s update out of 1000 PS is negligible.\\nBased on the above observation and calculation, we radically\\nlowered our snapshot frequency and thereby saved quite a\\nbit in computation overhead.\\n4 RELATED WORK\\nEver since some earliest successful application of deep learning to\\nindustry-level recommendation systems [ 6,10], researchers and\\nengineers have been employing various techniques to ameliorate\\nissues mentioned in Section 1.\\nTo tackle the issue of sparse feature representation, [ 3,6] uses\\nfixed-size embedding table with hash-trick. There are also attempts\\nin improving hashing to reduce collision [ 3,7]. Other works directly\\nutilize native key-value hash table to allow dynamic growth of table\\nsize [ 12,15,20,21]. These implementations builds upon TensorFlow\\nbut relies either on specially designed software mechanism [ 14,\\n15,20] or hardware [ 21] to access and manage their hash-tables.\\nCompared to these solutions, Monolith’s hash-table is yet another\\nnative TensorFlow operation. It is developer friendly and has higher\\ncross-platform interoperability, which is suitable for ToB scenarios.\\nAn organic and tight integration with TensorFlow also enables\\neasier optimizations of computation performance.\\nBridging the gap between training and serving and alleviation\\nof Concept Drift [ 8] is another topic of interest. To support online\\nupdate and avoid memory issues, both [ 12] and [ 20] designed fea-\\nture eviction mechanisms to flexibly adjust the size of embedding\\ntables. Both [ 12] and [ 14] support some form of online training,\\nwhere learned parameters are synced to serving with a relatively\\nshort interval compared to traditional batch training, with fault tol-\\nerance mechanisms. Monolith took similar approach to elastically\\nadmit and evict features, while it has a more lightweight parameter\\nsynchronization mechanism to guarantee model quality.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n5 CONCLUSION\\nIn this work, we reviewed several most important challenges for\\nindustrial-level recommendation systems and present our system\\nin production, Monolith, to address them and achieved best perfor-\\nmance compared to existing solutions.\\nWe proved that a collisionless embedding table is essential for\\nmodel quality, and demonstrated that our implementation of Cuckoo\\nHashMap based embedding table is both memory efficient and help-\\nful for improving online serving metrics.\\nWe also proved that realtime serving is crucial in recommenda-\\ntion systems, and that parameter synchronization interval should\\nbe as short as possible for an ultimate model performance. Our\\nsolution for online realtime serving in Monolith has a delicately\\ndesigned parameter synchronization and a fault tolerance mecha-\\nnism: In our parameter synchronization algorithm, we showed that\\nconsistency of version across different parts of parameters could be\\ntraded-off for reducing network bandwidth consumption; In fault\\ntolerance design, we demonstrated that our strategy of trading-off\\nPS reliability for realtime-ness is a robust solution.\\nTo conclude, Monolith succeeded in providing a general solution\\nfor production scale recommendation systems.\\nACKNOWLEDGMENTS\\nHanzhi Zhou provided useful suggestions on revision of this paper.\\nREFERENCES\\n[1]Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\\nYuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\\nmachine learning. ArXiv abs/1605.08695 (2016).\\n[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–\\n1159.\\n[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\\nengineering.com/core-modeling-at-instagram-a51e0158aa48\\n[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\\nand Kostas Tzoumas. 2015. Apache Flink ™: Stream and Batch Processing in a\\nSingle Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\\n[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\\nIspir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\\nHemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\\nof the 1st Workshop on Deep Learning for Recommender Systems (2016).\\n[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\\nfor YouTube Recommendations. Proceedings of the 10th ACM Conference on\\nRecommender Systems (2016).\\n[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\\nteenth ACM Conference on Recommender Systems (2021).\\n[8]João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\\n2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\\n(2014), 1 – 37.\\n[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\\nIJCAI .\\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\\nDavid M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\\nAndrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\\nXuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based\\nPersonalized Recommendation. 2020 IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA) (2020), 488–501.\\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\\nand Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\\nHuang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\\nSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\\nProceedings of the 1st International Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data (2019).\\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\\nWu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan\\nLuo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\\nLin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\\nbo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\\nScaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\\nabs/2111.05897 (2021).\\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\\nmender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\\ntensorflow-in-recommender-systems.html\\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS .\\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\\n2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) (2010), 1–10.\\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\\nSampling and Log Odds Correction with Rare Events Data. In Advances in Neural\\nInformation Processing Systems .\\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\\nLin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\\nContinual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\\nnational Conference for High Performance Computing, Networking, Storage and\\nAnalysis (2020), 1–17.\\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\\n2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\\n28th ACM International Conference on Information and Knowledge Management\\n(2019).')],\n",
       " 'output_text': \"The PDF discusses various research papers and articles related to real-time recommendation systems. Here's a summary:\\n\\n1. **Monolith:** A real-time recommendation system that uses collisionless embedding for efficient processing.\\n2. **DeepFM:** A neural network-based CTR (Click-Through Rate) prediction model that combines factorization machines with deep learning techniques.\\n3. **The Architectural Implications of Facebook's DNN-Based Personalized Recommendation:** A study on the architectural implications of using deep neural networks for personalized recommendations at scale.\\n4. **MovieLens Datasets: History and Context:** An article providing context and history about the MovieLens datasets, which are widely used in recommendation system research.\\n5. **XDL:** An industrial deep learning framework for high-dimensional sparse data, with applications to recommendation systems.\\n6. **Persia:** A hybrid system that scales deep learning-based recommenders up to 100 trillion parameters, making it suitable for large-scale recommendation systems.\\n7. **Distributed Training Optimization for TensorFlow in Recommender Systems:** An article discussing the optimization of distributed training for TensorFlow in recommender systems.\\n8. **Cuckoo Hashing:** A paper on cuckoo hashing, a technique used in some recommendation systems to efficiently store and retrieve data.\\n\\nThe PDF also mentions various tools and technologies used in recommendation system research, including:\\n\\n1. **PyTorch:** An imperative-style deep learning library for building recommender systems.\\n2. **Hadoop Distributed File System (HDFS):** A distributed file system designed for large-scale data processing.\\n3. **Kafka:** A distributed messaging system for log processing and event-driven architectures.\\n\\nAdditionally, the PDF references several papers and articles related to recommendation systems, including:\\n\\n1. **Nonuniform Negative Sampling and Log Odds Correction with Rare Events Data:** A paper discussing techniques for handling rare events in recommender systems.\\n2. **Kraken:** A memory-efficient continual learning approach for large-scale real-time recommendations.\\n3. **AIBox:** A CTR prediction model training system that can be run on a single node.\\n\\nThese references provide insights into various aspects of recommendation systems, including architecture, scalability, and optimization techniques.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chain_doc=load_summarize_chain(llm=llm1, chain_type=\"stuff\", prompt=prompt_doc,verbose=True)\n",
    "\"\"\"ValueError: Missing some input keys: {'input_documents'}\"\"\"\n",
    "Chain_doc.invoke(doc_store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) MAP_REDUCE METHOD \n",
    "- (Overcomes drawback of Stuff Documentation Chain to parse Large pdf files that has TOken-Restriction set at model)\n",
    "- i.e if document is large to be parsed & tokens of summarization exceeds the maximum number of tokens set in the model, then we can use MAP_REDUCE method to split the large document into multiple chunks, thereby minimizing errors &parsing the output chunks until it hits the max_tokens set by model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Note:Addtional Lnagchain module to split the large documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utlizing above content_of_pdf as text to create multiple chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such\\nframeworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.\\n∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\\nZhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural\\nfit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical\\nfeatures, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.\\npractice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\n•Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\n•Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth\\nof embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a user’s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.\\nIn light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-artMonolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our servers’ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate\\ndesign details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlow’s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two\\nsets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlow’s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlow’s limitation that arises from separation of training and in-\\nference, Monolith’s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation\\nis to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case 𝑂(1)time complexity\\nfor lookups and deletions, and an expected amortized 𝑂(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables 𝑇0,𝑇1\\nwith different hash functions ℎ0(𝑥),ℎ1(𝑥), and an element would\\nbe stored in either one of them. When trying to insert an element\\n𝐴into𝑇0, it first attempts to place 𝐴atℎ0(𝐴); Ifℎ0(𝐴)is occupied\\nby another element 𝐵, it would evict 𝐵from𝑇0and try inserting 𝐵\\ninto𝑇1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;\\n(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nUser \\n Log Kafka \\nActions \\n(Click, Like, \\nConvert …) \\nFeature Kafka \\n Model Server \\n Training Worker \\nTraining PS \\n Serving PS \\nJoiner \\nFlink Job \\nTraining Example \\nKafka \\nBatch Training Data \\nHDFS \\nGenerate \\nFeatures \\nParameter Synchronization \\nDump Batch Data \\nOnline \\nTraining \\nBatch Training \\nFigure 4: Streaming Engine.\\nThe information feedback loop from [User →Model Server→Training Worker→Model Server→User] would spend a long time when taking the Batch Training\\npath, while the Online Training will close the loop more instantly.\\n(2)IDs are timed and set to expire after being inactive for a\\npredefined period of time. The expire time is also tunable for\\neach embedding table to allow for distinguishing features\\nwith different sensitivity to historical information.\\nIn our implementation, HashTable is implemented as a Tensor-\\nFlow resource operation. Similar to Variable , look-ups and updates\\nare also implemented as native TensorFlow operations for easier\\nintegration and better compatibility.\\n2.2 Online Training\\nIn Monolith, training is divided into two stages (Figure 1):\\n(1)Batch training stage. This stage works as an ordinary Tensor-\\nFlow training loop: In each training step, a training worker\\nreads one mini-batch of training examples from the stor-\\nage, requests parameters from PS, computes a forward and\\na backward pass, and finally push updated parameters to\\nthe training PS. Slightly different from other common deep\\nlearning tasks, we only train our dataset for one pass. Batch\\ntraining is useful for training historical data when we modify\\nour model architecture and retrain the model;\\n(2)Online training stage. After a model is deployed to online\\nserving, the training does not stop but enters the online train-\\ning stage. Instead of reading mini-batch examples from the\\nstorage, a training worker consumes realtime data on-the-fly\\nand updates the training PS. The training PS periodically syn-\\nchronizes its parameters to the serving PS, which will take\\neffect on the user side immediately. This enables our model\\nto interactively adapt itself according to a user’s feedback in\\nrealtime.2.2.1 Streaming Engine. Monolith is built with the capability of\\nseamlessly switching between batch training and online training.\\nThis is enabled by our design of streaming engine as illustrated by\\nFigure 4.\\nIn our design, we use one Kafka [ 13] queue to log actions of users\\n(E.g. Click on an item or like an item etc.) and another Kafka queue\\nfor features. At the core of the engine is a Flink [ 4] streaming job for\\nonline feature Joiner. The online joiner concatenates features with\\nlabels from user actions and produces training examples, which are\\nthen written to a Kafka queue. The queue for training examples is\\nconsumed by both online training and batch training:\\n•For online training, the training worker directly reads data\\nfrom the Kafka queue;\\n•For batch training, a data dumping job will first dump data\\nto HDFS [ 18]; After data in HDFS accumulated to certain\\namount, training worker will retrieve data from HDFS and\\nperform batch training.\\nUpdated parameters in training PS will be pushed to serving PS\\naccording to the parameter synchronization schedule.\\n2.2.2 Online Joiner. In real-world applications, user actions log\\nand features are streamed into the online joiner (Figure 5) without\\nguarantee in time order. Therefore we use a unique key for each\\nrequest so that user action and features could correctly pair up.\\nThe lag of user action could also be a problem. For example, a\\nuser may take a few days before they decide to buy an item they\\nwere presented days ago. This is a challenge for the joiner because\\nif all features are kept in cache, it would simply not fit in memory.\\nIn our system, an on-disk key-value storage is utilized to store\\nfeatures that are waiting for over certain time period. When a user\\naction log arrives, it first looks up the in-memory cache, and then\\nlooks up the key-value storage in case of a missing cache.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nLog Kafka \\n(User Actions) \\nFeature Kafka \\nIn-memory \\nCache \\nOn-disk \\nKV-Store \\nJoin \\nNegative \\nSampling \\nTraining Example \\nKafka \\nFound in Cache \\nRead from KV-Store \\nFigure 5: Online Joiner.\\nAnother problem that arise in real-world application is that\\nthe distribution of negative and positive examples are highly un-\\neven, where number of the former could be magnitudes of order\\nhigher than the latter. To prevent positive examples from being\\noverwhelmed by negative ones, a common strategy is to do negative\\nsampling. This would certainly change the underlying distribution\\nof the trained model, tweaking it towards higher probability of\\nmaking positive predictions. As a remedy, we apply log odds cor-\\nrection [ 19] during serving, making sure that the online model is\\nan unbiased estimator of the original distribution.\\n2.2.3 Parameter Synchronization. During online training, the Mono-\\nlith training cluster keeps receiving data from the online serving\\nmodule and updates parameters on the training PS. A crucial step\\nto enable the online serving PS to benefit from these newly trained\\nparameters is the synchronization of updated model parameters. In\\nproduction environment, we are encountered by several challenges:\\n•Models on the online serving PS must not stop serving when\\nupdating. Our models in production is usually several ter-\\nabytes in size, and as a result replacing all parameters takes a\\nwhile. It would be intolerable to stop an online PS from serv-\\ning the model during the replacement process, and updates\\nmust be made on-the-fly;\\n•Transferring a multi-terabyte model of its entirety from train-\\ning PS to the online serving PS would pose huge pressure\\nto both the network bandwidth and memory on PS, since it\\nrequires doubled model size of memory to accept the newly\\narriving model.\\nFor the online training to scale up to the size of our business\\nscenario, we designed an incremental on-the-fly periodic parameter\\nsynchronization mechanism in Monolith based on several notice-\\nable characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\\ntion models;\\n(2)Given a short range of time window, only a small subset of\\nIDs gets trained and their embeddings updated;\\n(3)Dense variables move much slower than sparse embeddings.\\nThis is because in momentum-based optimizers, the accumu-\\nlation of momentum for dense variables is magnified by the\\ngigantic size of recommendation training data, while only\\na few sparse embeddings receives updates in a single data\\nbatch.\\n(1) and (2) allows us to exploit the sparse updates across all feature\\nIDs. In Monolith, we maintain a hash set of touched keys, repre-\\nsenting IDs whose embeddings get trained since the last parameter\\nsynchronization. We push the subset of sparse parameters whose\\nkeys are in the touched-keys set with a minute-level time interval\\nfrom the training PS to the online serving PS. This relatively small\\npack of incremental parameter update is lightweight for network\\ntransmission and will not cause a sharp memory spike during the\\nsynchronization.\\nWe also exploit (3) to further reduce network I/O and memory\\nusage by setting a more aggressive sync schedule for sparse pa-\\nrameters, while updating dense parameters less frequently. This\\ncould render us a situation where the dense parameters we serve is\\na relatively stale version compared to sparse part. However, such\\ninconsistency could be tolerated due to the reason mentioned in (3)\\nas no conspicuous loss has been observed.\\n2.3 Fault Tolerance\\nAs a system in production, Monolith is designed with the ability to\\nrecover a PS in case it fails. A common choice for fault tolerance is\\nto snapshot the state of a model periodically, and recover from theORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nlatest snapshot when PS failure is detected. The choice of snapshot\\nfrequency has two major impacts:\\n(1)Model quality. Intuitively, model quality suffers less from\\nloss of recent history with increased snapshot frequency.\\n(2)Computation overhead. Snapshotting a multi-terabyte model\\nis not free. It incurs large chunks of memory copy and disk\\nI/O.\\nAs a trade-off between model quality and computation overhead,\\nMonolith snapshots all training PS every day. Though a PS will lose\\none day’s worth of update in case of a failure, we discover that the\\nperformance degradation is tolerable through our experiments. We\\nwill analyze the effect of PS reliability in the next section.\\n. . .\\n. . .\\nIDsEmbeddings FMMLP Output \\nFigure 6: DeepFM model architecture.\\n3 EVALUATION\\nFor a better understanding of benefits and trade-offs brought about\\nby our proposed design, we conducted several experiments at pro-\\nduction scale and A/B test with live serving traffic to evaluate and\\nverify Monolith from different aspects. We aim to answer the fol-\\nlowing questions by our experiments:\\n(1) How much can we benefit from a collisionless HashTable ?\\n(2) How important is realtime online training?\\n(3)Is Monolith’s design of parameter synchronization robust\\nenough in a large-scale production scenario?\\nIn this section, we first present our experimental settings and\\nthen discuss results and our findings in detail.\\n3.1 Experimental Setup\\n3.1.1 Embedding Table. As described in Section 2.1, embedding\\ntables in Monolith are implemented as collisionless HashTable s.\\nTo prove the necessity of avoiding collisions in embedding tables\\nand to quantify gains from our collisionless implementation, we\\nperformed two groups of experiments on the Movielens dataset\\nand on our internal production dataset respectively:\\n(1)MovieLens ml-25m dataset [ 11]. This is a standard public\\ndataset for movie ratings, containing 25 million ratings that\\ninvolves approximately 162000 users and 62000 movies.\\n•Preprocessing of labels . The original labels are ratings from\\n0.5 to 5.0, while in production our tasks are mostly re-\\nceiving binary signals from users. To better simulate our\\nproduction models, we convert scale labels to binary labelsby treating scores≥3.5as positive samples and the rest\\nas negative samples.\\n•Model and metrics . We implemented a standard DeepFM\\n[9] model, a commonly used model architecture for recom-\\nmendation problems. It consist of an FM component and a\\ndense component (Figure 6). Predictions are evaluated by\\nAUC [ 2] as this is the major measurement for real models.\\n•Embedding collisions . This dataset contains approximately\\n160K user IDs and 60K movie IDs. To compare with the\\ncollisionless version of embedding table implementation,\\nwe performed another group of experiment where IDs are\\npreprocessed with MD5 hashing and then mapped to a\\nsmaller ID space. As a result, some IDs will share their\\nembedding with others. Table 1 shows detailed statistics\\nof user and movie IDs before and after hashing.\\nUser IDs Movie IDs\\n# Before Hashing 162541 59047\\n# After Hashing 149970 57361\\nCollision rate 7.73% 2 .86%\\nTable 1: Statistics of IDs Before and After Hashing.\\n(2)Internal Recommendation dataset .\\nWe also performed experiments on a recommendation model\\nin production environment. This model generally follows a\\nmulti-tower architecture, with each tower responsible for\\nlearning to predict a specialized kind of user behavior.\\n•Each model has around 1000 embedding tables, and distri-\\nbution of size of embedding tables are very uneven;\\n•The original ID space of embedding table was 248. In our\\nbaseline, we applied a hashing trick by decomposing to\\ncurb the size of embedding table. To be more specific, we\\nuse two smaller embedding tables instead of a gigantic\\none to generate a unique embedding for each ID by vector\\ncombination:\\n𝐼𝐷𝑟=𝐼𝐷%224\\n𝐼𝐷𝑞=𝐼𝐷÷224\\n𝐸=𝐸𝑟+𝐸𝑞,\\nwhere𝐸𝑟,𝐸𝑞are embeddings corresponding to 𝐼𝐷𝑟,𝐼𝐷𝑞.\\nThis effectively reduces embedding table sizes from 248\\nto225;\\n•This model is serving in real production, and the perfor-\\nmance of this experiment is measured by online AUC with\\nreal serving traffic.\\n3.1.2 Online Training. During online training, we update our on-\\nline serving PS with the latest set of parameters with minute-level\\nintervals. We designed two groups of experiments to verify model\\nquality and system robustness.\\n(1)Update frequency . To investigate the necessity of minute-\\nlevel update frequency, we conducted experiments that syn-\\nchronize parameters from training model to prediction model\\nwith different intervals.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nAlgorithm 1 Simulated Online Training.\\n1:Input:𝐷𝑏𝑎𝑡𝑐ℎ; /* Data for batch training. */\\n2:Input:𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖=1···𝑁; /* Data for online training, split into 𝑁shards. */\\n3:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑏𝑎𝑡𝑐ℎ,𝜃𝑡𝑟𝑎𝑖𝑛); /* Batch training. */\\n/* Online training. */\\n4:for𝑖=1···𝑁do\\n5:𝜃𝑠𝑒𝑟𝑣𝑒←𝜃𝑡𝑟𝑎𝑖𝑛 ; /* Sync training parameters to serving model. */\\n6:𝐴𝑈𝐶𝑖=Evaluate(𝜃𝑠𝑒𝑟𝑣𝑒,𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖); /* Evaluate online prediction on new data. */\\n7:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖,𝜃𝑡𝑟𝑎𝑖𝑛); /* Train with new data. */\\n8:end for\\nThe dataset we use is the Criteo Display Ads Challenge\\ndataset3, a large-scale standard dataset for benchmarking\\nCTR models. It contains 7 days of chronologically ordered\\ndata recording features and click actions. For this experiment,\\nwe use a standard DeepFM [9] model as described in 6.\\nTo simulate online training, we did the following preprocess-\\ning for the dataset. We take 7 days of data from the dataset,\\nand split it to two parts: 5 days of data for batch training, and\\n2 days for online training. We further split the 2 days of data\\ninto𝑁shards chronologically. Online training is simulated\\nby algorithm 1.\\nAs such, we simulate synchronizing trained parameters to\\nonline serving PS with an interval determined by number of\\ndata shards. We experimented with 𝑁=10,50,100, which\\nroughly correspond to update interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛.\\n(2)Live experiment . In addition, we also performed a live ex-\\nperiment with real serving traffic to further demonstrate the\\nimportance of online training in real-world application. This\\nA/B experiment compares online training to batch training\\none one of our Ads model in production.\\n3.2 Results and Analysis\\n3.2.1 The Effect of Embedding Collision. Results from MovieLens\\ndataset and the Internal recommedation dataset both show that\\nembedding collisions will jeopardize model quality.\\n(1)Models with collisionless HashTable consistently outper-\\nforms those with collision. This conclusion holds true re-\\ngardless of\\n•Increase of number of training epochs. As shown in Figure\\n7, the model with collisionless embedding table has higher\\nAUC from the first epoch and converges at higher value;\\n•Change of distribution with passage of time due to Con-\\ncept Drift. As shown in Figure 8, models with collision-\\nless embedding table is also robust as time passes by and\\nusers/items context changes.\\n(2)Data sparsity caused by collisionless embedding table will\\nnot lead to model overfitting. As shown in Figure 7, a model\\nwith collisionless embedding table does not overfit after it\\nconverges.\\n3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\\n1 2 3 4 5 6 7 8 9 10\\nepoch0.8050.8100.8150.8200.825test auccollision-free hash\\nhash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\\nMovieLens\\n2 4 6 8 10 12\\nDay0.7700.7750.7800.7850.790Online serving AUChash w/ collision\\ncollisionless hash table\\nFigure 8: Effect of Embedding Collision On A\\nRecommendation Model In Production\\nWe measure performance of this recommendation model by online serving AUC,\\nwhich is fluctuating across different days due to concept-drift.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-\\ncovered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\n•Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;•Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛 respectively.\\nSync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66±0.020 79 .42±0.026\\n1 hr 79.78±0.005 79 .44±0.030\\n30 min 79.80±0.008 79 .43±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nDay 1 2 3 4 5 6 7\\nAUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\\nTable 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\\nSync Interval 1 hr\\nSync Interval 30 min\\nFigure 10: Comparison of different sync intervals for\\nonline training.\\nThe live A/B experiment between online training and batch\\ntraining on an Ads model in production also show that there\\nis a significant bump in online serving AUC (Table 3).\\nInspired by this observation, we synchronize sparse parame-\\nters to serving PS of our production models as frequent as\\npossible (currently at minute-level), to the extent that the\\ncomputation overhead and system reliability could endure.\\nRecall that dense variables requires a less frequent update as\\ndiscussed in 2.2.3, we update them at day-level. By doing so,\\nwe can bring down our computation overhead to a very low\\nlevel. Suppose 100,000 IDs gets updated in a minute, and the\\ndimension of embedding is 1024, the total size of data need\\nto be transferred is 4𝐾𝐵×100,000≈400𝑀𝐵per minute.\\nFor dense parameters, since they are synchronized daily, we\\nchoose to schedule the synchronization when the traffic is\\nlowest (e.g. midnight).\\n(2)The Effect of PS reliability.\\nWith a minute-level parameter synchronization, we initially\\nexpect a more frequent snapshot of training PS to match the\\nrealtime update. To our surprise, we enlarged the snapshot\\ninterval to 1 day and still observed nearly no loss of model\\nquality.\\nFinding the right trade-off between model quality and com-\\nputation overhead is difficult for personalized ranking sys-\\ntems since users are extremely sensitive on recommendation\\nquality. Traditionally, large-scale systems tend to set a fre-\\nquent snapshot schedule for their models, which sacrifices\\ncomputation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\\nregard and to our surprise, model quality is more robust than\\nexpected. With a 0.01% failure rate of PS machine per day,\\nwe find a model from the previous day works embarrassingly\\nwell. This is explicable by the following calculation: Suppose\\na model’s parameters are sharded across 1000 PS, and they\\nsnapshot every day. Given 0.01% failure rate, one of them\\nwill go down every 10 days and we lose all updates on this\\nPS for 1 day. Assuming a DAU of 15 Million and an even\\ndistribution of user IDs on each PS, we lose 1 day’s feedback\\nfrom 15000 users every 10 days. This is acceptable because\\n(a) For sparse features which is user-specific, this is equiv-\\nalent to losing a tiny fraction of 0.01% DAU; (b) For dense\\nvariables, since they are updated slowly as we discussed in\\n2.2.3, losing 1 day’s update out of 1000 PS is negligible.\\nBased on the above observation and calculation, we radically\\nlowered our snapshot frequency and thereby saved quite a\\nbit in computation overhead.\\n4 RELATED WORK\\nEver since some earliest successful application of deep learning to\\nindustry-level recommendation systems [ 6,10], researchers and\\nengineers have been employing various techniques to ameliorate\\nissues mentioned in Section 1.\\nTo tackle the issue of sparse feature representation, [ 3,6] uses\\nfixed-size embedding table with hash-trick. There are also attempts\\nin improving hashing to reduce collision [ 3,7]. Other works directly\\nutilize native key-value hash table to allow dynamic growth of table\\nsize [ 12,15,20,21]. These implementations builds upon TensorFlow\\nbut relies either on specially designed software mechanism [ 14,\\n15,20] or hardware [ 21] to access and manage their hash-tables.\\nCompared to these solutions, Monolith’s hash-table is yet another\\nnative TensorFlow operation. It is developer friendly and has higher\\ncross-platform interoperability, which is suitable for ToB scenarios.\\nAn organic and tight integration with TensorFlow also enables\\neasier optimizations of computation performance.\\nBridging the gap between training and serving and alleviation\\nof Concept Drift [ 8] is another topic of interest. To support online\\nupdate and avoid memory issues, both [ 12] and [ 20] designed fea-\\nture eviction mechanisms to flexibly adjust the size of embedding\\ntables. Both [ 12] and [ 14] support some form of online training,\\nwhere learned parameters are synced to serving with a relatively\\nshort interval compared to traditional batch training, with fault tol-\\nerance mechanisms. Monolith took similar approach to elastically\\nadmit and evict features, while it has a more lightweight parameter\\nsynchronization mechanism to guarantee model quality.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n5 CONCLUSION\\nIn this work, we reviewed several most important challenges for\\nindustrial-level recommendation systems and present our system\\nin production, Monolith, to address them and achieved best perfor-\\nmance compared to existing solutions.\\nWe proved that a collisionless embedding table is essential for\\nmodel quality, and demonstrated that our implementation of Cuckoo\\nHashMap based embedding table is both memory efficient and help-\\nful for improving online serving metrics.\\nWe also proved that realtime serving is crucial in recommenda-\\ntion systems, and that parameter synchronization interval should\\nbe as short as possible for an ultimate model performance. Our\\nsolution for online realtime serving in Monolith has a delicately\\ndesigned parameter synchronization and a fault tolerance mecha-\\nnism: In our parameter synchronization algorithm, we showed that\\nconsistency of version across different parts of parameters could be\\ntraded-off for reducing network bandwidth consumption; In fault\\ntolerance design, we demonstrated that our strategy of trading-off\\nPS reliability for realtime-ness is a robust solution.\\nTo conclude, Monolith succeeded in providing a general solution\\nfor production scale recommendation systems.\\nACKNOWLEDGMENTS\\nHanzhi Zhou provided useful suggestions on revision of this paper.\\nREFERENCES\\n[1]Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,\\nYuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\\nmachine learning. ArXiv abs/1605.08695 (2016).\\n[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–\\n1159.\\n[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\\nengineering.com/core-modeling-at-instagram-a51e0158aa48\\n[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\\nand Kostas Tzoumas. 2015. Apache Flink ™: Stream and Batch Processing in a\\nSingle Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\\n[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\\nIspir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\\nHemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\\nof the 1st Workshop on Deep Learning for Recommender Systems (2016).\\n[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\\nfor YouTube Recommendations. Proceedings of the 10th ACM Conference on\\nRecommender Systems (2016).\\n[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\\nteenth ACM Conference on Recommender Systems (2021).\\n[8]João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\\n2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\\n(2014), 1 – 37.\\n[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\\nIJCAI .\\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\\nDavid M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\\nAndrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\\nXuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based\\nPersonalized Recommendation. 2020 IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA) (2020), 488–501.\\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\\nand Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\\nHuang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\\nSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\\nProceedings of the 1st International Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data (2019).\\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\\nWu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan\\nLuo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\\nLin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\\nbo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\\nScaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\\nabs/2111.05897 (2021).\\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\\nmender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\\ntensorflow-in-recommender-systems.html\\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS .\\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\\n2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) (2010), 1–10.\\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\\nSampling and Log Odds Correction with Rare Events Data. In Advances in Neural\\nInformation Processing Systems .\\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\\nLin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\\nContinual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\\nnational Conference for High Performance Computing, Networking, Storage and\\nAnalysis (2020), 1–17.\\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\\n2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\\n28th ACM International Conference on Information and Knowledge Management\\n(2019).'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_of_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such'),\n",
       " Document(page_content='frameworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.'),\n",
       " Document(page_content='∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin'),\n",
       " Document(page_content='Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural'),\n",
       " Document(page_content='fit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical'),\n",
       " Document(page_content='features, some of which appear with low frequency. The commonarXiv:2209.07663v2  [cs.IR]  27 Sep 2022ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nData \\n(Batch Training) \\nData \\n(Online Training) \\nTraining \\nWorker \\nTraining PS \\n Serving PS \\nModel \\nServer \\nUser \\nBatch \\nTraining \\nStage \\nOnline \\nTraining \\nStage \\nHistorical batch data \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply gradient \\nupdates \\nOnline streaming \\ndata \\nFeature IDs \\nFeature embeddings \\nFeature IDs and gradients Embedding table \\nlookup \\nModel forward \\nand backward pass \\nApply \\ngradient \\nupdates \\nParameter \\nSync \\nParameter \\nSync Sync \\ninterval \\nUser Request \\nFeature IDs \\nFeature \\nembeddings Embedding \\ntable lookup \\nRanking Result Model \\nforward \\npass \\nUser Actions \\nData of \\nfeatures and \\n user reactions \\nFigure 1: Monolith Online Training Architecture.'),\n",
       " Document(page_content='practice of mapping them to a high-dimensional embedding space\\nwould give rise to a series of issues:\\n•Unlike language models where number of word-pieces are\\nlimited, the amount of users and ranking items are orders of\\nmagnitude larger. Such an enormous embedding table would\\nhardly fit into single host memory;\\n•Worse still, the size of embedding table is expected to grow\\nover time as more users and items are admitted, while frame-\\nworks like [ 1,17] uses a fixed-size dense variables to repre-\\nsent embedding table.\\nIn practice, many systems adopt low-collision hashing [ 3,6] as a\\nway to reduce memory footprint and to allow growing of IDs. This\\nrelies on an over-idealistic assumption that IDs in the embedding\\ntable is distributed evenly in frequency, and collisions are harmless\\nto the model quality. Unfortunately this is rarely true for a real-\\nworld recommendation system, where a small group of users or\\nitems have significantly more occurrences. With the organic growth'),\n",
       " Document(page_content='of embedding table size, chances of hash key collision increases\\nand lead to deterioration of model quality [3].\\nTherefore it is a natural demand for production-scale recommen-\\ndation systems to have the capacity to capture as many features in\\nits parameters, and also have the capability of elastically adjusting\\nthe number of users and items it tries to book-keep.1.2 Non-stationary Distribution\\nVisual and linguistic patterns barely develop in a time scale of\\ncenturies, while the same user interested in one topic could shift\\ntheir zeal every next minute. As a result, the underlying distribution\\nof user data is non-stationary, a phenomenon commonly referred\\nto as Concept Drift [8].\\nIntuitively, information from a more recent history can more\\neffectively contribute to predicting the change in a user’s behavior.\\nTo mitigate the effect of Concept Drift, serving models need to be\\nupdated from new user feedback as close to real-time as possible to\\nreflect the latest interest of a user.'),\n",
       " Document(page_content='In light of these distinction and in observation of issues that arises\\nfrom our production, we designed Monolith , a large-scale recom-\\nmendation system to address these pain-points. We did extensive\\nexperiments to verify and iterate our design in the production\\nenvironment. Monolith is able to\\n(1)Provide full expressive power for sparse features by design-\\ning a collisionless hash table and a dynamic feature eviction\\nmechanism;\\n(2)Loop serving feedback back to training in real-time with\\nonline training.\\nEmpowered by these architectural capacities, Monolith consis-\\ntently outperforms systems that adopts hash-tricks with collisions\\nwith roughly similar memory usage, and achieves state-of-the-artMonolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nonline serving AUC without overly burdening our servers’ compu-\\ntation power.\\nThe rest of the paper is organized as follows. We first elaborate'),\n",
       " Document(page_content='design details of how Monolith tackles existing challenge with colli-\\nsionless hash table and realtime training in Section 2. Experiments\\nand results will be presented in Section 3, along with production-\\ntested conclusions and some discussion of trade-offs between time-\\nsensitivity, reliability and model quality. Section 4 summarizes re-\\nlated work and compares them with Monolith. Section 5 concludes\\nthis work.\\nClient \\n Master \\nWorker \\n Worker \\nParameter \\nServer \\nParameter \\nServer \\n. . .\\n. . .\\nFigure 2: Worker-PS Architecture.\\n2 DESIGN\\nThe overall architecture of Monolith generally follows TensorFlow’s\\ndistributed Worker- Parameter Server setting (Figure 2). In a Worker-\\nPS architecture, machines are assigned different roles; Worker ma-\\nchines are responsible for performing computations as defined by\\nthe graph, and PS machines stores parameters and updates them\\naccording to gradients computed by Workers.\\nIn recommendation models, parameters are categorized into two'),\n",
       " Document(page_content='sets: dense and sparse. Dense parameters are weights/variables in\\na deep neural network, and sparse parameters refer to embedding\\ntables that corresponds to sparse features. In our design, both dense\\nand sparse parameters are part of TensorFlow Graph, and are stored\\non parameter servers.\\nSimilar to TensorFlow’s Variable for dense parameters, we de-\\nsigned a set of highly-efficient, collisionless, and flexible HashTable\\nB A D C G\\nD E F C BT0\\nT1A\\nh0(A) \\nh1(B) h0(C) h1(D) \\nFigure 3: Cuckoo HashMap.operations for sparse parameters. As an complement to Tensor-\\nFlow’s limitation that arises from separation of training and in-\\nference, Monolith’s elastically scalable online training is designed\\nto efficiently synchronize parameters from training-PS to online\\nserving-PS within short intervals, with model robustness guarantee\\nprovided by fault tolerance mechanism.\\n2.1 Hash Table\\nA first principle in our design of sparse parameter representation'),\n",
       " Document(page_content='is to avoid cramping information from different IDs into the same\\nfixed-size embedding. Simulating a dynamic size embedding table\\nwith an out-of-the-box TensorFlow Variable inevitably leads to ID\\ncollision, which exacerbates as new IDs arrive and table grows.\\nTherefore instead of building upon Variable , we developed a new\\nkey-value HashTable for our sparse parameters.\\nOurHashTable utilizes Cuckoo Hashmap [ 16] under the hood,\\nwhich supports inserting new keys without colliding with existing\\nones. Cuckoo Hashing achieves worst-case 𝑂(1)time complexity\\nfor lookups and deletions, and an expected amortized 𝑂(1)time for\\ninsertions. As illustrated in Figure 3 it maintains two tables 𝑇0,𝑇1\\nwith different hash functions ℎ0(𝑥),ℎ1(𝑥), and an element would\\nbe stored in either one of them. When trying to insert an element\\n𝐴into𝑇0, it first attempts to place 𝐴atℎ0(𝐴); Ifℎ0(𝐴)is occupied\\nby another element 𝐵, it would evict 𝐵from𝑇0and try inserting 𝐵'),\n",
       " Document(page_content='into𝑇1with the same logic. This process will be repeated until all\\nelements stabilize, or rehash happens when insertion runs into a\\ncycle.\\nMemory footprint reduction is also an important consideration\\nin our design. A naive approach of inserting every new ID into\\ntheHashTable will deplete memory quickly. Observation of real\\nproduction models lead to two conclusions:\\n(1)IDs that appears only a handful of times have limited con-\\ntribution to improving model quality. An important obser-\\nvation is that IDs are long-tail distributed, where popular\\nIDs may occur millions of times while the unpopular ones\\nappear no more than ten times. Embeddings corresponding\\nto these infrequent IDs are underfit due to lack of training\\ndata and the model will not be able to make a good estima-\\ntion based on them. At the end of the day these IDs are not\\nlikely to affect the result, so model quality will not suffer\\nfrom removal of these IDs with low occurrences;'),\n",
       " Document(page_content='(2)Stale IDs from a distant history seldom contribute to the\\ncurrent model as many of them are never visited. This could\\npossibly due to a user that is no longer active, or a short-\\nvideo that is out-of-date. Storing embeddings for these IDs\\ncould not help model in any way but to drain our PS memory\\nin vain.\\nBased on these observation, we designed several feature ID fil-\\ntering heuristics for a more memory-efficient implementation of\\nHashTable :\\n(1)IDs are filtered before they are admitted into embedding\\ntables. We have two filtering methods: First we filter by\\ntheir occurrences before they are inserted as keys, where the\\nthreshold of occurrences is a tunable hyperparameter that\\nvaries for each model; In addition we utilize a probabilistic\\nfilter which helps further reduce memory usage;ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nUser \\n Log Kafka \\nActions \\n(Click, Like, \\nConvert …)'),\n",
       " Document(page_content='Convert …) \\nFeature Kafka \\n Model Server \\n Training Worker \\nTraining PS \\n Serving PS \\nJoiner \\nFlink Job \\nTraining Example \\nKafka \\nBatch Training Data \\nHDFS \\nGenerate \\nFeatures \\nParameter Synchronization \\nDump Batch Data \\nOnline \\nTraining \\nBatch Training \\nFigure 4: Streaming Engine.\\nThe information feedback loop from [User →Model Server→Training Worker→Model Server→User] would spend a long time when taking the Batch Training\\npath, while the Online Training will close the loop more instantly.\\n(2)IDs are timed and set to expire after being inactive for a\\npredefined period of time. The expire time is also tunable for\\neach embedding table to allow for distinguishing features\\nwith different sensitivity to historical information.\\nIn our implementation, HashTable is implemented as a Tensor-\\nFlow resource operation. Similar to Variable , look-ups and updates\\nare also implemented as native TensorFlow operations for easier\\nintegration and better compatibility.\\n2.2 Online Training'),\n",
       " Document(page_content='2.2 Online Training\\nIn Monolith, training is divided into two stages (Figure 1):\\n(1)Batch training stage. This stage works as an ordinary Tensor-\\nFlow training loop: In each training step, a training worker\\nreads one mini-batch of training examples from the stor-\\nage, requests parameters from PS, computes a forward and\\na backward pass, and finally push updated parameters to\\nthe training PS. Slightly different from other common deep\\nlearning tasks, we only train our dataset for one pass. Batch\\ntraining is useful for training historical data when we modify\\nour model architecture and retrain the model;\\n(2)Online training stage. After a model is deployed to online\\nserving, the training does not stop but enters the online train-\\ning stage. Instead of reading mini-batch examples from the\\nstorage, a training worker consumes realtime data on-the-fly\\nand updates the training PS. The training PS periodically syn-\\nchronizes its parameters to the serving PS, which will take'),\n",
       " Document(page_content='effect on the user side immediately. This enables our model\\nto interactively adapt itself according to a user’s feedback in\\nrealtime.2.2.1 Streaming Engine. Monolith is built with the capability of\\nseamlessly switching between batch training and online training.\\nThis is enabled by our design of streaming engine as illustrated by\\nFigure 4.\\nIn our design, we use one Kafka [ 13] queue to log actions of users\\n(E.g. Click on an item or like an item etc.) and another Kafka queue\\nfor features. At the core of the engine is a Flink [ 4] streaming job for\\nonline feature Joiner. The online joiner concatenates features with\\nlabels from user actions and produces training examples, which are\\nthen written to a Kafka queue. The queue for training examples is\\nconsumed by both online training and batch training:\\n•For online training, the training worker directly reads data\\nfrom the Kafka queue;\\n•For batch training, a data dumping job will first dump data'),\n",
       " Document(page_content='to HDFS [ 18]; After data in HDFS accumulated to certain\\namount, training worker will retrieve data from HDFS and\\nperform batch training.\\nUpdated parameters in training PS will be pushed to serving PS\\naccording to the parameter synchronization schedule.\\n2.2.2 Online Joiner. In real-world applications, user actions log\\nand features are streamed into the online joiner (Figure 5) without\\nguarantee in time order. Therefore we use a unique key for each\\nrequest so that user action and features could correctly pair up.\\nThe lag of user action could also be a problem. For example, a\\nuser may take a few days before they decide to buy an item they\\nwere presented days ago. This is a challenge for the joiner because\\nif all features are kept in cache, it would simply not fit in memory.\\nIn our system, an on-disk key-value storage is utilized to store\\nfeatures that are waiting for over certain time period. When a user\\naction log arrives, it first looks up the in-memory cache, and then'),\n",
       " Document(page_content='looks up the key-value storage in case of a missing cache.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nLog Kafka \\n(User Actions) \\nFeature Kafka \\nIn-memory \\nCache \\nOn-disk \\nKV-Store \\nJoin \\nNegative \\nSampling \\nTraining Example \\nKafka \\nFound in Cache \\nRead from KV-Store \\nFigure 5: Online Joiner.\\nAnother problem that arise in real-world application is that\\nthe distribution of negative and positive examples are highly un-\\neven, where number of the former could be magnitudes of order\\nhigher than the latter. To prevent positive examples from being\\noverwhelmed by negative ones, a common strategy is to do negative\\nsampling. This would certainly change the underlying distribution\\nof the trained model, tweaking it towards higher probability of\\nmaking positive predictions. As a remedy, we apply log odds cor-\\nrection [ 19] during serving, making sure that the online model is'),\n",
       " Document(page_content='an unbiased estimator of the original distribution.\\n2.2.3 Parameter Synchronization. During online training, the Mono-\\nlith training cluster keeps receiving data from the online serving\\nmodule and updates parameters on the training PS. A crucial step\\nto enable the online serving PS to benefit from these newly trained\\nparameters is the synchronization of updated model parameters. In\\nproduction environment, we are encountered by several challenges:\\n•Models on the online serving PS must not stop serving when\\nupdating. Our models in production is usually several ter-\\nabytes in size, and as a result replacing all parameters takes a\\nwhile. It would be intolerable to stop an online PS from serv-\\ning the model during the replacement process, and updates\\nmust be made on-the-fly;\\n•Transferring a multi-terabyte model of its entirety from train-\\ning PS to the online serving PS would pose huge pressure\\nto both the network bandwidth and memory on PS, since it'),\n",
       " Document(page_content='requires doubled model size of memory to accept the newly\\narriving model.\\nFor the online training to scale up to the size of our business\\nscenario, we designed an incremental on-the-fly periodic parameter\\nsynchronization mechanism in Monolith based on several notice-\\nable characteristic of our models:(1)Sparse parameters are dominating the size of recommenda-\\ntion models;\\n(2)Given a short range of time window, only a small subset of\\nIDs gets trained and their embeddings updated;\\n(3)Dense variables move much slower than sparse embeddings.\\nThis is because in momentum-based optimizers, the accumu-\\nlation of momentum for dense variables is magnified by the\\ngigantic size of recommendation training data, while only\\na few sparse embeddings receives updates in a single data\\nbatch.\\n(1) and (2) allows us to exploit the sparse updates across all feature\\nIDs. In Monolith, we maintain a hash set of touched keys, repre-\\nsenting IDs whose embeddings get trained since the last parameter'),\n",
       " Document(page_content='synchronization. We push the subset of sparse parameters whose\\nkeys are in the touched-keys set with a minute-level time interval\\nfrom the training PS to the online serving PS. This relatively small\\npack of incremental parameter update is lightweight for network\\ntransmission and will not cause a sharp memory spike during the\\nsynchronization.\\nWe also exploit (3) to further reduce network I/O and memory\\nusage by setting a more aggressive sync schedule for sparse pa-\\nrameters, while updating dense parameters less frequently. This\\ncould render us a situation where the dense parameters we serve is\\na relatively stale version compared to sparse part. However, such\\ninconsistency could be tolerated due to the reason mentioned in (3)\\nas no conspicuous loss has been observed.\\n2.3 Fault Tolerance\\nAs a system in production, Monolith is designed with the ability to\\nrecover a PS in case it fails. A common choice for fault tolerance is'),\n",
       " Document(page_content='to snapshot the state of a model periodically, and recover from theORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\nlatest snapshot when PS failure is detected. The choice of snapshot\\nfrequency has two major impacts:\\n(1)Model quality. Intuitively, model quality suffers less from\\nloss of recent history with increased snapshot frequency.\\n(2)Computation overhead. Snapshotting a multi-terabyte model\\nis not free. It incurs large chunks of memory copy and disk\\nI/O.\\nAs a trade-off between model quality and computation overhead,\\nMonolith snapshots all training PS every day. Though a PS will lose\\none day’s worth of update in case of a failure, we discover that the\\nperformance degradation is tolerable through our experiments. We\\nwill analyze the effect of PS reliability in the next section.\\n. . .\\n. . .\\nIDsEmbeddings FMMLP Output \\nFigure 6: DeepFM model architecture.\\n3 EVALUATION'),\n",
       " Document(page_content='3 EVALUATION\\nFor a better understanding of benefits and trade-offs brought about\\nby our proposed design, we conducted several experiments at pro-\\nduction scale and A/B test with live serving traffic to evaluate and\\nverify Monolith from different aspects. We aim to answer the fol-\\nlowing questions by our experiments:\\n(1) How much can we benefit from a collisionless HashTable ?\\n(2) How important is realtime online training?\\n(3)Is Monolith’s design of parameter synchronization robust\\nenough in a large-scale production scenario?\\nIn this section, we first present our experimental settings and\\nthen discuss results and our findings in detail.\\n3.1 Experimental Setup\\n3.1.1 Embedding Table. As described in Section 2.1, embedding\\ntables in Monolith are implemented as collisionless HashTable s.\\nTo prove the necessity of avoiding collisions in embedding tables\\nand to quantify gains from our collisionless implementation, we\\nperformed two groups of experiments on the Movielens dataset'),\n",
       " Document(page_content='and on our internal production dataset respectively:\\n(1)MovieLens ml-25m dataset [ 11]. This is a standard public\\ndataset for movie ratings, containing 25 million ratings that\\ninvolves approximately 162000 users and 62000 movies.\\n•Preprocessing of labels . The original labels are ratings from\\n0.5 to 5.0, while in production our tasks are mostly re-\\nceiving binary signals from users. To better simulate our\\nproduction models, we convert scale labels to binary labelsby treating scores≥3.5as positive samples and the rest\\nas negative samples.\\n•Model and metrics . We implemented a standard DeepFM\\n[9] model, a commonly used model architecture for recom-\\nmendation problems. It consist of an FM component and a\\ndense component (Figure 6). Predictions are evaluated by\\nAUC [ 2] as this is the major measurement for real models.\\n•Embedding collisions . This dataset contains approximately\\n160K user IDs and 60K movie IDs. To compare with the\\ncollisionless version of embedding table implementation,'),\n",
       " Document(page_content='we performed another group of experiment where IDs are\\npreprocessed with MD5 hashing and then mapped to a\\nsmaller ID space. As a result, some IDs will share their\\nembedding with others. Table 1 shows detailed statistics\\nof user and movie IDs before and after hashing.\\nUser IDs Movie IDs\\n# Before Hashing 162541 59047\\n# After Hashing 149970 57361\\nCollision rate 7.73% 2 .86%\\nTable 1: Statistics of IDs Before and After Hashing.\\n(2)Internal Recommendation dataset .\\nWe also performed experiments on a recommendation model\\nin production environment. This model generally follows a\\nmulti-tower architecture, with each tower responsible for\\nlearning to predict a specialized kind of user behavior.\\n•Each model has around 1000 embedding tables, and distri-\\nbution of size of embedding tables are very uneven;\\n•The original ID space of embedding table was 248. In our\\nbaseline, we applied a hashing trick by decomposing to\\ncurb the size of embedding table. To be more specific, we'),\n",
       " Document(page_content='use two smaller embedding tables instead of a gigantic\\none to generate a unique embedding for each ID by vector\\ncombination:\\n𝐼𝐷𝑟=𝐼𝐷%224\\n𝐼𝐷𝑞=𝐼𝐷÷224\\n𝐸=𝐸𝑟+𝐸𝑞,\\nwhere𝐸𝑟,𝐸𝑞are embeddings corresponding to 𝐼𝐷𝑟,𝐼𝐷𝑞.\\nThis effectively reduces embedding table sizes from 248\\nto225;\\n•This model is serving in real production, and the perfor-\\nmance of this experiment is measured by online AUC with\\nreal serving traffic.\\n3.1.2 Online Training. During online training, we update our on-\\nline serving PS with the latest set of parameters with minute-level\\nintervals. We designed two groups of experiments to verify model\\nquality and system robustness.\\n(1)Update frequency . To investigate the necessity of minute-\\nlevel update frequency, we conducted experiments that syn-\\nchronize parameters from training model to prediction model\\nwith different intervals.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA'),\n",
       " Document(page_content='Algorithm 1 Simulated Online Training.\\n1:Input:𝐷𝑏𝑎𝑡𝑐ℎ; /* Data for batch training. */\\n2:Input:𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖=1···𝑁; /* Data for online training, split into 𝑁shards. */\\n3:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑏𝑎𝑡𝑐ℎ,𝜃𝑡𝑟𝑎𝑖𝑛); /* Batch training. */\\n/* Online training. */\\n4:for𝑖=1···𝑁do\\n5:𝜃𝑠𝑒𝑟𝑣𝑒←𝜃𝑡𝑟𝑎𝑖𝑛 ; /* Sync training parameters to serving model. */\\n6:𝐴𝑈𝐶𝑖=Evaluate(𝜃𝑠𝑒𝑟𝑣𝑒,𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖); /* Evaluate online prediction on new data. */\\n7:𝜃𝑡𝑟𝑎𝑖𝑛←𝑇𝑟𝑎𝑖𝑛(𝐷𝑜𝑛𝑙𝑖𝑛𝑒\\n𝑖,𝜃𝑡𝑟𝑎𝑖𝑛); /* Train with new data. */\\n8:end for\\nThe dataset we use is the Criteo Display Ads Challenge\\ndataset3, a large-scale standard dataset for benchmarking\\nCTR models. It contains 7 days of chronologically ordered\\ndata recording features and click actions. For this experiment,\\nwe use a standard DeepFM [9] model as described in 6.\\nTo simulate online training, we did the following preprocess-\\ning for the dataset. We take 7 days of data from the dataset,\\nand split it to two parts: 5 days of data for batch training, and'),\n",
       " Document(page_content='2 days for online training. We further split the 2 days of data\\ninto𝑁shards chronologically. Online training is simulated\\nby algorithm 1.\\nAs such, we simulate synchronizing trained parameters to\\nonline serving PS with an interval determined by number of\\ndata shards. We experimented with 𝑁=10,50,100, which\\nroughly correspond to update interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛.\\n(2)Live experiment . In addition, we also performed a live ex-\\nperiment with real serving traffic to further demonstrate the\\nimportance of online training in real-world application. This\\nA/B experiment compares online training to batch training\\none one of our Ads model in production.\\n3.2 Results and Analysis\\n3.2.1 The Effect of Embedding Collision. Results from MovieLens\\ndataset and the Internal recommedation dataset both show that\\nembedding collisions will jeopardize model quality.\\n(1)Models with collisionless HashTable consistently outper-\\nforms those with collision. This conclusion holds true re-\\ngardless of'),\n",
       " Document(page_content='gardless of\\n•Increase of number of training epochs. As shown in Figure\\n7, the model with collisionless embedding table has higher\\nAUC from the first epoch and converges at higher value;\\n•Change of distribution with passage of time due to Con-\\ncept Drift. As shown in Figure 8, models with collision-\\nless embedding table is also robust as time passes by and\\nusers/items context changes.\\n(2)Data sparsity caused by collisionless embedding table will\\nnot lead to model overfitting. As shown in Figure 7, a model\\nwith collisionless embedding table does not overfit after it\\nconverges.\\n3https://www.kaggle.com/competitions/criteo-display-ad-challenge/data\\n1 2 3 4 5 6 7 8 9 10\\nepoch0.8050.8100.8150.8200.825test auccollision-free hash\\nhash w/ collisionFigure 7: Effect of Embedding Collision On DeepFM,\\nMovieLens\\n2 4 6 8 10 12\\nDay0.7700.7750.7800.7850.790Online serving AUChash w/ collision\\ncollisionless hash table\\nFigure 8: Effect of Embedding Collision On A\\nRecommendation Model In Production'),\n",
       " Document(page_content='We measure performance of this recommendation model by online serving AUC,\\nwhich is fluctuating across different days due to concept-drift.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n0 10 20 30 40 50\\nHours0.7920.7940.7960.798AUCw/ online training\\nw/o online training\\n(a) Online training with 5hrs sync interval\\n0 10 20 30 40 50\\nHours0.79000.79250.79500.79750.80000.80250.80500.8075AUCw/ online training\\nw/o online training (b) Online training with 1hr sync interval\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCw/ online training\\nw/o online training\\n(c) Online training with 30min sync interval\\nFigure 9: Online training v.s. Batch training on Criteo dataset.\\nBlue lines: AUC of models with online training; Yellow lines: AUC of batch training models evaluated against streaming data.\\n3.2.2 Online Training: Trading-off Reliability For Realtime. We dis-'),\n",
       " Document(page_content='covered that a higher parameter synchronization frequency is al-\\nways conducive to improving online serving AUC, and that online\\nserving models are more tolerant with loss of a few shard of PS\\nthan we expect.\\n(1)The Effect of Parameter Synchronization Frequency .\\nIn our online streaming training experiment (1) with Criteo\\nDisplay Ads Challenge dataset, model quality consistently\\nimproves with the increase of parameter synchronization\\nfrequency, as is evident by comparison from two perspec-\\ntives:\\n•Models with online training performs better than models\\nwithout. Figure 9a, 9b, 9c compares AUC of online training\\nmodels evaluated by the following shard of data versus\\nbatch training models evaluated by each shard of data;•Models with smaller parameter synchronization interval\\nperforms better that those with larger interval. Figure 10\\nand Table 2 compares online serving AUC for models with\\nsync interval of 5ℎ𝑟,1ℎ𝑟, and 30𝑚𝑖𝑛 respectively.'),\n",
       " Document(page_content='Sync Interval Average AUC (online) Average AUC (batch)\\n5 hr 79.66±0.020 79 .42±0.026\\n1 hr 79.78±0.005 79 .44±0.030\\n30 min 79.80±0.008 79 .43±0.025\\nTable 2: Average AUC comparison for DeepFM model on\\nCriteo dataset.Monolith: Real Time Recommendation System With Collisionless Embedding Table ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\nDay 1 2 3 4 5 6 7\\nAUC Improvement % 14.443 16.871 17.068 14.028 18.081 16.404 15.202\\nTable 3: Improvement of Online Training Over Batch Training from Live A/B Experiment on an Ads Model.\\n0 10 20 30 40 50\\nHours0.7900.7950.8000.8050.810AUCSync Interval 5 hr\\nSync Interval 1 hr\\nSync Interval 30 min\\nFigure 10: Comparison of different sync intervals for\\nonline training.\\nThe live A/B experiment between online training and batch\\ntraining on an Ads model in production also show that there\\nis a significant bump in online serving AUC (Table 3).\\nInspired by this observation, we synchronize sparse parame-'),\n",
       " Document(page_content='ters to serving PS of our production models as frequent as\\npossible (currently at minute-level), to the extent that the\\ncomputation overhead and system reliability could endure.\\nRecall that dense variables requires a less frequent update as\\ndiscussed in 2.2.3, we update them at day-level. By doing so,\\nwe can bring down our computation overhead to a very low\\nlevel. Suppose 100,000 IDs gets updated in a minute, and the\\ndimension of embedding is 1024, the total size of data need\\nto be transferred is 4𝐾𝐵×100,000≈400𝑀𝐵per minute.\\nFor dense parameters, since they are synchronized daily, we\\nchoose to schedule the synchronization when the traffic is\\nlowest (e.g. midnight).\\n(2)The Effect of PS reliability.\\nWith a minute-level parameter synchronization, we initially\\nexpect a more frequent snapshot of training PS to match the\\nrealtime update. To our surprise, we enlarged the snapshot\\ninterval to 1 day and still observed nearly no loss of model\\nquality.'),\n",
       " Document(page_content='quality.\\nFinding the right trade-off between model quality and com-\\nputation overhead is difficult for personalized ranking sys-\\ntems since users are extremely sensitive on recommendation\\nquality. Traditionally, large-scale systems tend to set a fre-\\nquent snapshot schedule for their models, which sacrifices\\ncomputation resources in exchange for minimized loss inmodel quality. We also did quite some exploration in this\\nregard and to our surprise, model quality is more robust than\\nexpected. With a 0.01% failure rate of PS machine per day,\\nwe find a model from the previous day works embarrassingly\\nwell. This is explicable by the following calculation: Suppose\\na model’s parameters are sharded across 1000 PS, and they\\nsnapshot every day. Given 0.01% failure rate, one of them\\nwill go down every 10 days and we lose all updates on this\\nPS for 1 day. Assuming a DAU of 15 Million and an even\\ndistribution of user IDs on each PS, we lose 1 day’s feedback'),\n",
       " Document(page_content='from 15000 users every 10 days. This is acceptable because\\n(a) For sparse features which is user-specific, this is equiv-\\nalent to losing a tiny fraction of 0.01% DAU; (b) For dense\\nvariables, since they are updated slowly as we discussed in\\n2.2.3, losing 1 day’s update out of 1000 PS is negligible.\\nBased on the above observation and calculation, we radically\\nlowered our snapshot frequency and thereby saved quite a\\nbit in computation overhead.\\n4 RELATED WORK\\nEver since some earliest successful application of deep learning to\\nindustry-level recommendation systems [ 6,10], researchers and\\nengineers have been employing various techniques to ameliorate\\nissues mentioned in Section 1.\\nTo tackle the issue of sparse feature representation, [ 3,6] uses\\nfixed-size embedding table with hash-trick. There are also attempts\\nin improving hashing to reduce collision [ 3,7]. Other works directly\\nutilize native key-value hash table to allow dynamic growth of table'),\n",
       " Document(page_content='size [ 12,15,20,21]. These implementations builds upon TensorFlow\\nbut relies either on specially designed software mechanism [ 14,\\n15,20] or hardware [ 21] to access and manage their hash-tables.\\nCompared to these solutions, Monolith’s hash-table is yet another\\nnative TensorFlow operation. It is developer friendly and has higher\\ncross-platform interoperability, which is suitable for ToB scenarios.\\nAn organic and tight integration with TensorFlow also enables\\neasier optimizations of computation performance.\\nBridging the gap between training and serving and alleviation\\nof Concept Drift [ 8] is another topic of interest. To support online\\nupdate and avoid memory issues, both [ 12] and [ 20] designed fea-\\nture eviction mechanisms to flexibly adjust the size of embedding\\ntables. Both [ 12] and [ 14] support some form of online training,\\nwhere learned parameters are synced to serving with a relatively\\nshort interval compared to traditional batch training, with fault tol-'),\n",
       " Document(page_content='erance mechanisms. Monolith took similar approach to elastically\\nadmit and evict features, while it has a more lightweight parameter\\nsynchronization mechanism to guarantee model quality.ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA Zhuoran, Leqi, Xuan, Caihua, Biao, Da, Bolin, Yijie, Peng, Ke, and Youlong\\n5 CONCLUSION\\nIn this work, we reviewed several most important challenges for\\nindustrial-level recommendation systems and present our system\\nin production, Monolith, to address them and achieved best perfor-\\nmance compared to existing solutions.\\nWe proved that a collisionless embedding table is essential for\\nmodel quality, and demonstrated that our implementation of Cuckoo\\nHashMap based embedding table is both memory efficient and help-\\nful for improving online serving metrics.\\nWe also proved that realtime serving is crucial in recommenda-\\ntion systems, and that parameter synchronization interval should\\nbe as short as possible for an ultimate model performance. Our'),\n",
       " Document(page_content='solution for online realtime serving in Monolith has a delicately\\ndesigned parameter synchronization and a fault tolerance mecha-\\nnism: In our parameter synchronization algorithm, we showed that\\nconsistency of version across different parts of parameters could be\\ntraded-off for reducing network bandwidth consumption; In fault\\ntolerance design, we demonstrated that our strategy of trading-off\\nPS reliability for realtime-ness is a robust solution.\\nTo conclude, Monolith succeeded in providing a general solution\\nfor production scale recommendation systems.\\nACKNOWLEDGMENTS\\nHanzhi Zhou provided useful suggestions on revision of this paper.\\nREFERENCES\\n[1]Martín Abadi, Paul Barham, Jianmin Chen, Z. Chen, Andy Davis, Jeffrey Dean,\\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath\\nKudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray,\\nBenoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke,'),\n",
       " Document(page_content='Yuan Yu, and Xiaoqiang Zhang. 2016. TensorFlow: A system for large-scale\\nmachine learning. ArXiv abs/1605.08695 (2016).\\n[2]Andrew P. Bradley. 1997. The use of the area under the ROC curve in the\\nevaluation of machine learning algorithms. Pattern Recognit. 30 (1997), 1145–\\n1159.\\n[3]Thomas Bredillet. 2019. Core modeling at Instagram. https://instagram-\\nengineering.com/core-modeling-at-instagram-a51e0158aa48\\n[4]Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif Haridi,\\nand Kostas Tzoumas. 2015. Apache Flink ™: Stream and Batch Processing in a\\nSingle Engine. IEEE Data Eng. Bull. 38 (2015), 28–38.\\n[5]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishikesh B. Aradhye, Glen Anderson, Gregory S. Corrado, Wei Chai, Mustafa\\nIspir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and\\nHemal Shah. 2016. Wide & Deep Learning for Recommender Systems. Proceedings\\nof the 1st Workshop on Deep Learning for Recommender Systems (2016).'),\n",
       " Document(page_content='[6]Paul Covington, Jay K. Adams, and Emre Sargin. 2016. Deep Neural Networks\\nfor YouTube Recommendations. Proceedings of the 10th ACM Conference on\\nRecommender Systems (2016).\\n[7]Alexandra Egg. 2021. Online Learning for Recommendations at Grubhub. Fif-\\nteenth ACM Conference on Recommender Systems (2021).\\n[8]João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and A. Bouchachia.\\n2014. A survey on concept drift adaptation. ACM Computing Surveys (CSUR) 46\\n(2014), 1 – 37.\\n[9]Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In\\nIJCAI .\\n[10] Udit Gupta, Xiaodong Wang, Maxim Naumov, Carole-Jean Wu, Brandon Reagen,\\nDavid M. Brooks, Bradford Cottel, Kim M. Hazelwood, Bill Jia, Hsien-Hsin S. Lee,\\nAndrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, and\\nXuan Zhang. 2020. The Architectural Implications of Facebook’s DNN-Based'),\n",
       " Document(page_content='Personalized Recommendation. 2020 IEEE International Symposium on High\\nPerformance Computer Architecture (HPCA) (2020), 488–501.\\n[11] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History\\nand Context. ACM Trans. Interact. Intell. Syst. 5 (2015), 19:1–19:19.\\n[12] Biye Jiang, Chao Deng, Huimin Yi, Zelin Hu, Guorui Zhou, Yang Zheng, Sui\\nHuang, Xinyang Guo, Dongyue Wang, Yue Song, Liqin Zhao, Zhi Wang, Peng\\nSun, Yu Zhang, Di Zhang, Jinhui Li, Jian Xu, Xiaoqiang Zhu, and Kun Gai. 2019.XDL: an industrial deep learning framework for high-dimensional sparse data.\\nProceedings of the 1st International Workshop on Deep Learning Practice for High-\\nDimensional Sparse Data (2019).\\n[13] Jay Kreps. 2011. Kafka : a Distributed Messaging System for Log Processing.\\n[14] Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan\\nWu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan'),\n",
       " Document(page_content='Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying\\nLin, Chengchun Shu, Xue-Bo Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai\\nbo Yu, Sen Yang, Ce Zhang, and Ji Liu. 2021. Persia: An Open, Hybrid System\\nScaling Deep Learning-based Recommenders up to 100 Trillion Parameters. ArXiv\\nabs/2111.05897 (2021).\\n[15] Meituan. 2021. Distributed Training Optimization for TensorFlow in Recom-\\nmender Systems (in Chinese). https://tech.meituan.com/2021/12/09/meituan-\\ntensorflow-in-recommender-systems.html\\n[16] R. Pagh and Flemming Friche Rodler. 2001. Cuckoo Hashing. In ESA.\\n[17] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban\\nDesmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith\\nChintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning\\nLibrary. In NeurIPS .'),\n",
       " Document(page_content='Library. In NeurIPS .\\n[18] Konstantin V. Shvachko, Hairong Kuang, Sanjay R. Radia, and Robert J. Chansler.\\n2010. The Hadoop Distributed File System. 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) (2010), 1–10.\\n[19] HaiYing Wang, Aonan Zhang, and Chong Wang. 2021. Nonuniform Negative\\nSampling and Log Odds Correction with Rare Events Data. In Advances in Neural\\nInformation Processing Systems .\\n[20] Minhui Xie, Kai Ren, Youyou Lu, Guangxu Yang, Qingxing Xu, Bihai Wu, Jiazhen\\nLin, Hongbo Ao, Wanhong Xu, and Jiwu Shu. 2020. Kraken: Memory-Efficient\\nContinual Learning for Large-Scale Real-Time Recommendations. SC20: Inter-\\nnational Conference for High Performance Computing, Networking, Storage and\\nAnalysis (2020), 1–17.\\n[21] Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li.\\n2019. AIBox: CTR Prediction Model Training on a Single Node. Proceedings of the\\n28th ACM International Conference on Information and Knowledge Management\\n(2019).')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap= 25)\n",
    "doc_chunks=Text_splitter.create_documents([content_of_pdf])\n",
    "doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such'),\n",
       " Document(page_content='frameworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Monolith: Real Time Recommendation System With\n",
      "Collisionless Embedding Table\n",
      "Zhuoran Liu\n",
      "Bytedance Inc.Leqi Zou\n",
      "Bytedance Inc.Xuan Zou\n",
      "Bytedance Inc.\n",
      "Caihua Wang\n",
      "Bytedance Inc.Biao Zhang\n",
      "Bytedance Inc.Da Tang\n",
      "Bytedance Inc.\n",
      "Bolin Zhu∗\n",
      "Fudan UniversityYijie Zhu\n",
      "Bytedance Inc.Peng Wu\n",
      "Bytedance Inc.\n",
      "Ke Wang\n",
      "Bytedance Inc.Youlong Cheng†\n",
      "Bytedance Inc.\n",
      "youlong.cheng@bytedance.com\n",
      "ABSTRACT\n",
      "Building a scalable and real-time recommendation system is vital\n",
      "for many businesses driven by time-sensitive customer feedback,\n",
      "such as short-videos ranking or online ads. Despite the ubiquitous\n",
      "adoption of production-scale deep learning frameworks like Ten-\n",
      "sorFlow or PyTorch, these general-purpose frameworks fall short\n",
      "of business demands in recommendation scenarios for various rea-\n",
      "sons: on one hand, tweaking systems based on static parameters and\n",
      "dense computations for recommendation with dynamic and sparse\n",
      "features is detrimental to model quality; on the other hand, such\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"frameworks are designed with batch-training stage and serving\n",
      "stage completely separated, preventing the model from interacting\n",
      "with customer feedback in real-time. These issues led us to reex-\n",
      "amine traditional approaches and explore radically different design\n",
      "choices. In this paper, we present Monolith1, a system tailored\n",
      "for online training. Our design has been driven by observations\n",
      "of our application workloads and production environment that re-\n",
      "flects a marked departure from other recommendations systems.\n",
      "Our contributions are manifold: first, we crafted a collisionless em-\n",
      "bedding table with optimizations such as expirable embeddings\n",
      "and frequency filtering to reduce its memory footprint; second, we\n",
      "provide an production-ready online training architecture with high\n",
      "fault-tolerance; finally, we proved that system reliability could be\n",
      "traded-off for real-time learning. Monolith has successfully landed\n",
      "in the BytePlus Recommend2product.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"∗Work done during internship at Bytedance Inc.\n",
      "†Corresponding author.\n",
      "1Code to be released soon.\n",
      "2https://www.byteplus.com/en/product/recommend\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "©2022 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\n",
      "https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\n",
      "Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\n",
      "Real Time Recommendation System With Collisionless Embedding Table.\n",
      "InProceedings of 5th Workshop on Online Recommender Systems and User\n",
      "Modeling, in conjunction with the 16th ACM Conference on Recommender\n",
      "Systems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "1 INTRODUCTION\n",
      "The past decade witnessed a boom of businesses powered by recom-\n",
      "mendation techniques. In pursuit of a better customer experience,\n",
      "delivering personalized content for each individual user as real-time\n",
      "response is a common goal of these business applications. To this\n",
      "end, information from a user’s latest interaction is often used as\n",
      "the primary input for training a model, as it would best depict a\n",
      "user’s portrait and make predictions of user’s interest and future\n",
      "behaviors.\n",
      "Deep learning have been dominating recommendation models\n",
      "[5,6,10,12,20,21] as the gigantic amount of user data is a natural\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"fit for massively data-driven neural models. However, efforts to\n",
      "leverage the power of deep learning in industry-level recommen-\n",
      "dation systems are constantly encountered with problems arising\n",
      "from the unique characteristics of data derived from real-world\n",
      "user behavior. These data are drastically different from those used\n",
      "in conventional deep learning problems like language modeling or\n",
      "computer vision in two aspects:\n",
      "(1)The features are mostly sparse, categorical and dynamically\n",
      "changing;\n",
      "(2)The underlying distribution of training data is non-stationary,\n",
      "a.k.a. Concept Drift [8].\n",
      "Such differences have posed unique challenges to researchers\n",
      "and engineers working on recommendation systems.\n",
      "1.1 Sparsity and Dynamism\n",
      "The data for recommendation mostly contain sparse categorical\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the above pdf The paper \"Monolith: Real Time Recommendation System With Collisionless Embedding Table\" proposes a scalable and real-time recommendation system designed specifically for time-sensitive customer feedback-driven businesses. The authors address limitations of general-purpose deep learning frameworks (e.g., TensorFlow, PyTorch) in handling dynamic and sparse features common in recommendation scenarios.\n",
      "\n",
      "Here is a concise summary:\n",
      "\n",
      "The authors of Monolith present an online training system designed to address limitations in traditional framework designs, which separate batch-training and serving stages, preventing real-time interaction with customer feedback. The paper introduces a novel approach with a collisionless embedding table optimized for memory efficiency and fault-tolerance, enabling real-time learning while trading off system reliability.\n",
      "\n",
      "This appears to be the metadata for a research paper or presentation submitted to the ACM RecSys 2022 conference. The document includes:\n",
      "\n",
      "* Information about an internship at Bytedance Inc.\n",
      "* A notice about permissions to copy and distribute the work\n",
      "* Details about the publication, including the title, authors, date, location, and ISBN\n",
      "* A reference format citation for the paper\n",
      "\n",
      "Here is a concise summary:\n",
      "\n",
      "The paper \"Monolith: Real Time Recommendation System With Collisionless Embedding Table\" presents a new approach to real-time recommendation systems. The authors propose a model that leverages user interaction data in real-time, aiming to deliver personalized content for each individual user. This approach uses deep learning techniques and information from the latest user interactions as primary input for training models.\n",
      "\n",
      "Deep learning-based recommendation systems face challenges due to the unique characteristics of real-world user behavior data, which differ from those used in conventional deep learning problems. The main issues are:\n",
      "\n",
      "* Data features are mostly sparse and categorical, making it difficult to leverage traditional deep learning methods.\n",
      "* The underlying distribution of training data is non-stationary (Concept Drift), requiring adaptations to handle changing patterns.\n",
      "\n",
      "These challenges pose difficulties for researchers and engineers working on recommendation systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such'),\n",
       "  Document(page_content='frameworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.'),\n",
       "  Document(page_content='∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin'),\n",
       "  Document(page_content='Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural'),\n",
       "  Document(page_content='fit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical')],\n",
       " 'output_text': 'Here\\'s a summary of the PDF:\\n\\nThe paper \"Monolith: Real Time Recommendation System With Collisionless Embedding Table\" proposes a scalable and real-time recommendation system designed specifically for time-sensitive businesses that rely on customer feedback. The authors address the limitations of traditional deep learning frameworks (e.g., TensorFlow, PyTorch) in handling dynamic and sparse features common in recommendation scenarios.\\n\\nThe paper presents an online training system that optimizes memory efficiency and fault-tolerance, enabling real-time learning while trading off system reliability. The approach uses a novel collisionless embedding table to address the challenges of traditional deep learning-based recommendation systems, which face issues with sparse and categorical data features, as well as non-stationary underlying distributions (concept drift).\\n\\nThe authors aim to deliver personalized content for each individual user by leveraging user interaction data in real-time, using deep learning techniques as primary input for training models. The proposed system aims to overcome the challenges of traditional recommendation systems and provide a scalable and reliable solution for time-sensitive businesses that rely on customer feedback.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_template3=\"Provide summary of the above pdf {text}\"\n",
    "prompt_doc3=PromptTemplate(input_variables=[\"text\"],template=template3)\n",
    "Chain_doc=load_summarize_chain(llm=llm1, chain_type=\"map_reduce\",combine_prompt=prompt_doc3,verbose=True)\n",
    "\"\"\"ValueError: extra value(s) found in load_summarize_chain: {prompt=prompt_doc3} -- therefore should be ommitted since By defualt the the funcion implies summarizing of the doc chunk provided\"\"\"\n",
    "\"\"\"Limited the no of chunks(not of dos's pages to 5 in interest of time for model to parse the summary)\"\"\"\n",
    "Chain_doc.invoke(doc_chunks[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) MAP_REDUCE METHOD WITH Custom Prompt for both (Chunk & whole document) \n",
    "-- **NOTE-1:using (map_prompt=chunk-prompt1)& (combine_prompt=prompt_doc3). Where each of the above can be used isolation with each other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper Monolith: Real Time Recommendation System With\n",
      "Collisionless Embedding Table\n",
      "Zhuoran Liu\n",
      "Bytedance Inc.Leqi Zou\n",
      "Bytedance Inc.Xuan Zou\n",
      "Bytedance Inc.\n",
      "Caihua Wang\n",
      "Bytedance Inc.Biao Zhang\n",
      "Bytedance Inc.Da Tang\n",
      "Bytedance Inc.\n",
      "Bolin Zhu∗\n",
      "Fudan UniversityYijie Zhu\n",
      "Bytedance Inc.Peng Wu\n",
      "Bytedance Inc.\n",
      "Ke Wang\n",
      "Bytedance Inc.Youlong Cheng†\n",
      "Bytedance Inc.\n",
      "youlong.cheng@bytedance.com\n",
      "ABSTRACT\n",
      "Building a scalable and real-time recommendation system is vital\n",
      "for many businesses driven by time-sensitive customer feedback,\n",
      "such as short-videos ranking or online ads. Despite the ubiquitous\n",
      "adoption of production-scale deep learning frameworks like Ten-\n",
      "sorFlow or PyTorch, these general-purpose frameworks fall short\n",
      "of business demands in recommendation scenarios for various rea-\n",
      "sons: on one hand, tweaking systems based on static parameters and\n",
      "dense computations for recommendation with dynamic and sparse\n",
      "features is detrimental to model quality; on the other hand, such until now\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper frameworks are designed with batch-training stage and serving\n",
      "stage completely separated, preventing the model from interacting\n",
      "with customer feedback in real-time. These issues led us to reex-\n",
      "amine traditional approaches and explore radically different design\n",
      "choices. In this paper, we present Monolith1, a system tailored\n",
      "for online training. Our design has been driven by observations\n",
      "of our application workloads and production environment that re-\n",
      "flects a marked departure from other recommendations systems.\n",
      "Our contributions are manifold: first, we crafted a collisionless em-\n",
      "bedding table with optimizations such as expirable embeddings\n",
      "and frequency filtering to reduce its memory footprint; second, we\n",
      "provide an production-ready online training architecture with high\n",
      "fault-tolerance; finally, we proved that system reliability could be\n",
      "traded-off for real-time learning. Monolith has successfully landed\n",
      "in the BytePlus Recommend2product. until now\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper ∗Work done during internship at Bytedance Inc.\n",
      "†Corresponding author.\n",
      "1Code to be released soon.\n",
      "2https://www.byteplus.com/en/product/recommend\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "©2022 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\n",
      "https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\n",
      "Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin until now\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\n",
      "Real Time Recommendation System With Collisionless Embedding Table.\n",
      "InProceedings of 5th Workshop on Online Recommender Systems and User\n",
      "Modeling, in conjunction with the 16th ACM Conference on Recommender\n",
      "Systems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "1 INTRODUCTION\n",
      "The past decade witnessed a boom of businesses powered by recom-\n",
      "mendation techniques. In pursuit of a better customer experience,\n",
      "delivering personalized content for each individual user as real-time\n",
      "response is a common goal of these business applications. To this\n",
      "end, information from a user’s latest interaction is often used as\n",
      "the primary input for training a model, as it would best depict a\n",
      "user’s portrait and make predictions of user’s interest and future\n",
      "behaviors.\n",
      "Deep learning have been dominating recommendation models\n",
      "[5,6,10,12,20,21] as the gigantic amount of user data is a natural until now\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper fit for massively data-driven neural models. However, efforts to\n",
      "leverage the power of deep learning in industry-level recommen-\n",
      "dation systems are constantly encountered with problems arising\n",
      "from the unique characteristics of data derived from real-world\n",
      "user behavior. These data are drastically different from those used\n",
      "in conventional deep learning problems like language modeling or\n",
      "computer vision in two aspects:\n",
      "(1)The features are mostly sparse, categorical and dynamically\n",
      "changing;\n",
      "(2)The underlying distribution of training data is non-stationary,\n",
      "a.k.a. Concept Drift [8].\n",
      "Such differences have posed unique challenges to researchers\n",
      "and engineers working on recommendation systems.\n",
      "1.1 Sparsity and Dynamism\n",
      "The data for recommendation mostly contain sparse categorical until now\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide information in bulleted list format for each of the above chunk summaries from pdf The paper \"Monolith: Real-Time Recommendation System With Collisionless Embedding Table\" proposes a novel approach to building a scalable and real-time recommendation system. The authors identify two major limitations of general-purpose deep learning frameworks like TensorFlow or PyTorch in recommendation scenarios:\n",
      "\n",
      "1. **Static parameters**: These frameworks are designed for general-purpose computations, which can lead to suboptimal performance when applied to recommendation tasks that require dynamic and sparse features.\n",
      "2. **Dense computations**: Recommendation systems often involve sparse and high-dimensional data, making dense computations inefficient.\n",
      "\n",
      "To address these limitations, the authors introduce Monolith, a real-time recommendation system with a novel \"collisionless embedding table\" (CET) architecture. The CET is designed to efficiently handle dynamic and sparse features while maintaining good performance.\n",
      "\n",
      "The key innovations of Monolith are:\n",
      "\n",
      "1. **Collisionless Embedding Table**: A data structure that maps high-dimensional and sparse user-item interactions to dense representations, enabling efficient computation and collision-free lookups.\n",
      "2. **Distributed Processing**: Monolith leverages distributed computing to handle large-scale recommendation tasks in real-time.\n",
      "\n",
      "The paper demonstrates the effectiveness of Monolith through experiments on several datasets, including a commercial short-video ranking scenario. The results show that Monolith outperforms existing methods while achieving faster inference times and better scalability.\n",
      "\n",
      "Overall, the authors' contributions aim to bridge the gap between production-scale deep learning frameworks and the specific requirements of real-time recommendation systems, making Monolith a promising solution for businesses driven by time-sensitive customer feedback.\n",
      "\n",
      "The paper presents a new approach to building recommendation systems, called Monolith, which is designed for online training and serving stages to be completely separate, allowing for real-time interactions with customer feedback. The authors re-examined traditional approaches and made radically different design choices based on observations of their application workloads and production environment.\n",
      "\n",
      "The contributions of the paper are:\n",
      "\n",
      "1. A collisionless embedding table with optimizations such as expirable embeddings and frequency filtering to reduce its memory footprint.\n",
      "2. A production-ready online training architecture with high fault-tolerance.\n",
      "3. The ability to trade off system reliability for real-time learning.\n",
      "\n",
      "Monolith has been successfully implemented in the BytePlus Recommend product, allowing for real-time learning and improvement of recommendation models.\n",
      "\n",
      "The paper is about the work done during an internship at ByteDance Inc., a Chinese technology company. The title of the paper is \"Work done during internship at Bytedance Inc.\".\n",
      "\n",
      "Here's a summary:\n",
      "\n",
      "* The authors, Zhuoran Liu and his colleagues, worked as interns at ByteDance Inc. in 2022.\n",
      "* During their internship, they focused on developing a recommendation system for ByteDance's product, Recommend.\n",
      "* The authors implemented the code using various algorithms and techniques, including collaborative filtering, matrix factorization, and attention-based models.\n",
      "* They also conducted experiments to evaluate the performance of their model and compared it with existing methods in the field.\n",
      "* The paper concludes by discussing the outcomes of the internship, including the development of a working prototype and insights gained from the project.\n",
      "\n",
      "Please note that this is just a brief summary, and you may want to read the full paper for more details.\n",
      "\n",
      "The paper \"Monolith: Real-Time Recommendation System with Collisionless Embedding Table\" by Zhu et al. presents a novel approach to real-time recommendation systems.\n",
      "\n",
      "**Summary**\n",
      "\n",
      "In recent years, there has been a surge in businesses that rely on recommendation techniques to provide personalized experiences for their users. To achieve this, models are trained using the user's most recent interactions as input, which helps capture the user's portrait and predict their future behaviors. Deep learning-based approaches have dominated recommendation models due to the vast amounts of user data available.\n",
      "\n",
      "The authors introduce Monolith, a real-time recommendation system that utilizes a collisionless embedding table (CET) to improve the efficiency and effectiveness of recommendations. The CET is designed to reduce collisions between embeddings, which can lead to slow query times and poor performance. This innovation allows for fast and accurate querying of the embedding space.\n",
      "\n",
      "**Key Points**\n",
      "\n",
      "1. **Real-time recommendation system**: Monolith aims to provide personalized content in real-time by leveraging user interactions.\n",
      "2. **Collisionless Embedding Table (CET)**: The CET is introduced to reduce collisions between embeddings, improving query times and performance.\n",
      "3. **Deep learning-based approach**: Monolith relies on deep learning techniques to learn complex patterns in user behavior.\n",
      "\n",
      "**Conference Information**\n",
      "\n",
      "The paper was presented at the 5th Workshop on Online Recommender Systems and User Modeling (ORSUM@ACM RecSys 2022), held in conjunction with the 16th ACM Conference on Recommender Systems.\n",
      "\n",
      "Here's a summary of the paper:\n",
      "\n",
      "Title: [Not Provided]\n",
      "\n",
      "Summary: Massively data-driven neural models are being used in industry-level recommendation systems, but they face unique challenges due to differences between real-world user behavior data and conventional deep learning problems. Specifically:\n",
      "\n",
      "1. **Sparsity and Dynamism**: The features in these datasets are mostly sparse (i.e., many missing values) and categorical, with dynamic changes over time. This makes it difficult to model the underlying patterns.\n",
      "2. **Non-Stationarity** (Concept Drift): The distribution of training data is non-stationary, meaning that the underlying patterns change over time. This poses a significant challenge for neural models, which are typically designed to learn from static datasets.\n",
      "\n",
      "These challenges have made it difficult for researchers and engineers working on recommendation systems to effectively leverage deep learning techniques.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such'),\n",
       "  Document(page_content='frameworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.'),\n",
       "  Document(page_content='∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin'),\n",
       "  Document(page_content='Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural'),\n",
       "  Document(page_content='fit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical')],\n",
       " 'output_text': 'Here are the bulleted list summaries for each chunk:\\n\\n**Chunk 1: Monolith**\\n\\n* **Static parameters**: General-purpose deep learning frameworks like TensorFlow or PyTorch can lead to suboptimal performance in recommendation tasks that require dynamic and sparse features.\\n* **Dense computations**: Recommendation systems often involve sparse and high-dimensional data, making dense computations inefficient.\\n* To address these limitations, the authors introduce Monolith, a real-time recommendation system with a novel \"collisionless embedding table\" (CET) architecture.\\n\\n**Chunk 2: Key Innovations of Monolith**\\n\\n* **Collisionless Embedding Table**: A data structure that maps high-dimensional and sparse user-item interactions to dense representations, enabling efficient computation and collision-free lookups.\\n* **Distributed Processing**: Monolith leverages distributed computing to handle large-scale recommendation tasks in real-time.\\n\\n**Chunk 3: Contributions of the Paper**\\n\\n* The authors\\' contributions aim to bridge the gap between production-cale deep learning frameworks and the specific requirements of real-time recommendation systems, making Monolith a promising solution for businesses driven by time-sensitive customer feedback.\\n* **Collisionless Embedding Table**: A data structure that maps high-dimensional and sparse user-item interactions to dense representations, enabling efficient computation and collision-free lookups.\\n* **Production-Ready Online Training Architecture**: A high-fault-tolerant architecture for online training.\\n\\n**Chunk 4: Summary of the Paper**\\n\\n* The authors worked as interns at ByteDance Inc. in 2022 and developed a recommendation system for ByteDance\\'s product, Recommend.\\n* They implemented code using various algorithms and techniques, including collaborative filtering, matrix factorization, and attention-based models.\\n* They conducted experiments to evaluate the performance of their model and compared it with existing methods in the field.\\n\\n**Chunk 5: Summary**\\n\\n* The paper presents a novel approach to real-time recommendation systems, called Monolith, which utilizes a collisionless embedding table (CET) to improve efficiency and effectiveness.\\n* Key points:\\n\\t+ Real-time recommendation system\\n\\t+ Collisionless Embedding Table (CET)\\n\\t+ Deep learning-based approach\\n\\n**Chunk 6: Conference Information**\\n\\n* The paper was presented at the 5th Workshop on Online Recommender Systems and User Modeling (ORSUM@ACM RecSys 2022), held in conjunction with the 16th ACM Conference on Recommender Systems.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"chunks prompt created\"\"\"\n",
    "chunk_template1=\"Provide summary of the paper {text} until now\"\n",
    "chunk_prompt1=PromptTemplate(input_variables=[\"text\"],template=chunk_template1)\n",
    "\n",
    "\"\"\"\"whole_doc prompt\"\"\"\n",
    "doc_template4=\"Provide information in bulleted list format for each of the above chunk summaries from pdf {text}\"\n",
    "prompt_doc4=PromptTemplate(input_variables=[\"text\"],template=doc_template4)\n",
    "\n",
    "Chain_doc2=load_summarize_chain(llm=llm1, chain_type=\"map_reduce\",map_prompt=chunk_prompt1,combine_prompt=prompt_doc4,verbose=True)\n",
    "\"\"\"ValueError: extra value(s) found in load_summarize_chain: {prompt=prompt_doc3} -- therefore should be ommitted since By defualt the the funcion implies summarizing of the doc chunk provided\"\"\"\n",
    "\"\"\"Limited the no of chunks(not of dos's pages to 5 in interest of time for model to parse the summary)\"\"\"\n",
    "Chain_doc2.invoke(doc_chunks[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) REFINE METHOD WITH Custom Prompt for both (Chunk & whole document)  [|||AR TO MAP_REDUCE METHOD]\n",
    "- **BUT EACH INPUT CHUNK IS PASSED TO SUBSEQUENT I/PS IN CDF FASHION(CUMULATIVE DISTRIBUTION FUNCTION) OR CUMULATIVE SUM**\n",
    "- **NOTE-2:using (refine_prompt=chunk-prompt2)& (question_prompt=prompt_doc5/whole_doc_prompt5). Where each of the above can be used isolation with each other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide information in bulleted list format for each of the above chunk summaries from pdf Monolith: Real Time Recommendation System With\n",
      "Collisionless Embedding Table\n",
      "Zhuoran Liu\n",
      "Bytedance Inc.Leqi Zou\n",
      "Bytedance Inc.Xuan Zou\n",
      "Bytedance Inc.\n",
      "Caihua Wang\n",
      "Bytedance Inc.Biao Zhang\n",
      "Bytedance Inc.Da Tang\n",
      "Bytedance Inc.\n",
      "Bolin Zhu∗\n",
      "Fudan UniversityYijie Zhu\n",
      "Bytedance Inc.Peng Wu\n",
      "Bytedance Inc.\n",
      "Ke Wang\n",
      "Bytedance Inc.Youlong Cheng†\n",
      "Bytedance Inc.\n",
      "youlong.cheng@bytedance.com\n",
      "ABSTRACT\n",
      "Building a scalable and real-time recommendation system is vital\n",
      "for many businesses driven by time-sensitive customer feedback,\n",
      "such as short-videos ranking or online ads. Despite the ubiquitous\n",
      "adoption of production-scale deep learning frameworks like Ten-\n",
      "sorFlow or PyTorch, these general-purpose frameworks fall short\n",
      "of business demands in recommendation scenarios for various rea-\n",
      "sons: on one hand, tweaking systems based on static parameters and\n",
      "dense computations for recommendation with dynamic and sparse\n",
      "features is detrimental to model quality; on the other hand, such\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper frameworks are designed with batch-training stage and serving\n",
      "stage completely separated, preventing the model from interacting\n",
      "with customer feedback in real-time. These issues led us to reex-\n",
      "amine traditional approaches and explore radically different design\n",
      "choices. In this paper, we present Monolith1, a system tailored\n",
      "for online training. Our design has been driven by observations\n",
      "of our application workloads and production environment that re-\n",
      "flects a marked departure from other recommendations systems.\n",
      "Our contributions are manifold: first, we crafted a collisionless em-\n",
      "bedding table with optimizations such as expirable embeddings\n",
      "and frequency filtering to reduce its memory footprint; second, we\n",
      "provide an production-ready online training architecture with high\n",
      "fault-tolerance; finally, we proved that system reliability could be\n",
      "traded-off for real-time learning. Monolith has successfully landed\n",
      "in the BytePlus Recommend2product. until now\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper ∗Work done during internship at Bytedance Inc.\n",
      "†Corresponding author.\n",
      "1Code to be released soon.\n",
      "2https://www.byteplus.com/en/product/recommend\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "ORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\n",
      "©2022 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\n",
      "https://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\n",
      "Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin until now\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\n",
      "Real Time Recommendation System With Collisionless Embedding Table.\n",
      "InProceedings of 5th Workshop on Online Recommender Systems and User\n",
      "Modeling, in conjunction with the 16th ACM Conference on Recommender\n",
      "Systems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\n",
      "https://doi.org/XXXXXXX.XXXXXXX\n",
      "1 INTRODUCTION\n",
      "The past decade witnessed a boom of businesses powered by recom-\n",
      "mendation techniques. In pursuit of a better customer experience,\n",
      "delivering personalized content for each individual user as real-time\n",
      "response is a common goal of these business applications. To this\n",
      "end, information from a user’s latest interaction is often used as\n",
      "the primary input for training a model, as it would best depict a\n",
      "user’s portrait and make predictions of user’s interest and future\n",
      "behaviors.\n",
      "Deep learning have been dominating recommendation models\n",
      "[5,6,10,12,20,21] as the gigantic amount of user data is a natural until now\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProvide summary of the paper fit for massively data-driven neural models. However, efforts to\n",
      "leverage the power of deep learning in industry-level recommen-\n",
      "dation systems are constantly encountered with problems arising\n",
      "from the unique characteristics of data derived from real-world\n",
      "user behavior. These data are drastically different from those used\n",
      "in conventional deep learning problems like language modeling or\n",
      "computer vision in two aspects:\n",
      "(1)The features are mostly sparse, categorical and dynamically\n",
      "changing;\n",
      "(2)The underlying distribution of training data is non-stationary,\n",
      "a.k.a. Concept Drift [8].\n",
      "Such differences have posed unique challenges to researchers\n",
      "and engineers working on recommendation systems.\n",
      "1.1 Sparsity and Dynamism\n",
      "The data for recommendation mostly contain sparse categorical until now\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Monolith: Real Time Recommendation System With\\nCollisionless Embedding Table\\nZhuoran Liu\\nBytedance Inc.Leqi Zou\\nBytedance Inc.Xuan Zou\\nBytedance Inc.\\nCaihua Wang\\nBytedance Inc.Biao Zhang\\nBytedance Inc.Da Tang\\nBytedance Inc.\\nBolin Zhu∗\\nFudan UniversityYijie Zhu\\nBytedance Inc.Peng Wu\\nBytedance Inc.\\nKe Wang\\nBytedance Inc.Youlong Cheng†\\nBytedance Inc.\\nyoulong.cheng@bytedance.com\\nABSTRACT\\nBuilding a scalable and real-time recommendation system is vital\\nfor many businesses driven by time-sensitive customer feedback,\\nsuch as short-videos ranking or online ads. Despite the ubiquitous\\nadoption of production-scale deep learning frameworks like Ten-\\nsorFlow or PyTorch, these general-purpose frameworks fall short\\nof business demands in recommendation scenarios for various rea-\\nsons: on one hand, tweaking systems based on static parameters and\\ndense computations for recommendation with dynamic and sparse\\nfeatures is detrimental to model quality; on the other hand, such'),\n",
       "  Document(page_content='frameworks are designed with batch-training stage and serving\\nstage completely separated, preventing the model from interacting\\nwith customer feedback in real-time. These issues led us to reex-\\namine traditional approaches and explore radically different design\\nchoices. In this paper, we present Monolith1, a system tailored\\nfor online training. Our design has been driven by observations\\nof our application workloads and production environment that re-\\nflects a marked departure from other recommendations systems.\\nOur contributions are manifold: first, we crafted a collisionless em-\\nbedding table with optimizations such as expirable embeddings\\nand frequency filtering to reduce its memory footprint; second, we\\nprovide an production-ready online training architecture with high\\nfault-tolerance; finally, we proved that system reliability could be\\ntraded-off for real-time learning. Monolith has successfully landed\\nin the BytePlus Recommend2product.'),\n",
       "  Document(page_content='∗Work done during internship at Bytedance Inc.\\n†Corresponding author.\\n1Code to be released soon.\\n2https://www.byteplus.com/en/product/recommend\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nORSUM@ACM RecSys 2022, September 23rd, 2022, Seattle, WA, USA\\n©2022 Association for Computing Machinery.\\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\\nhttps://doi.org/XXXXXXX.XXXXXXXACM Reference Format:\\nZhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin'),\n",
       "  Document(page_content='Zhu, Yijie Zhu, Peng Wu, Ke Wang, and Youlong Cheng. 2022. Monolith:\\nReal Time Recommendation System With Collisionless Embedding Table.\\nInProceedings of 5th Workshop on Online Recommender Systems and User\\nModeling, in conjunction with the 16th ACM Conference on Recommender\\nSystems (ORSUM@ACM RecSys 2022). ACM, New York, NY, USA, 10 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nThe past decade witnessed a boom of businesses powered by recom-\\nmendation techniques. In pursuit of a better customer experience,\\ndelivering personalized content for each individual user as real-time\\nresponse is a common goal of these business applications. To this\\nend, information from a user’s latest interaction is often used as\\nthe primary input for training a model, as it would best depict a\\nuser’s portrait and make predictions of user’s interest and future\\nbehaviors.\\nDeep learning have been dominating recommendation models\\n[5,6,10,12,20,21] as the gigantic amount of user data is a natural'),\n",
       "  Document(page_content='fit for massively data-driven neural models. However, efforts to\\nleverage the power of deep learning in industry-level recommen-\\ndation systems are constantly encountered with problems arising\\nfrom the unique characteristics of data derived from real-world\\nuser behavior. These data are drastically different from those used\\nin conventional deep learning problems like language modeling or\\ncomputer vision in two aspects:\\n(1)The features are mostly sparse, categorical and dynamically\\nchanging;\\n(2)The underlying distribution of training data is non-stationary,\\na.k.a. Concept Drift [8].\\nSuch differences have posed unique challenges to researchers\\nand engineers working on recommendation systems.\\n1.1 Sparsity and Dynamism\\nThe data for recommendation mostly contain sparse categorical')],\n",
       " 'output_text': 'Here is a summary of the paper:\\n\\nThe authors highlight the challenges faced when applying deep learning techniques to massive datasets in real-world recommendation systems. The main issues arise from two unique characteristics of these datasets: \\n\\n1. **Sparsity and Dynamism**: The features are mostly sparse (i.e., containing many zero values) and categorical, which makes it difficult to leverage traditional neural network architectures designed for dense numerical data. Additionally, the features are dynamically changing, meaning that new items or users can be added or removed over time.\\n\\n2. **Non-Stationarity** (Concept Drift): The underlying distribution of training data is non-stationary, implying that the patterns and relationships in the data change over time. This makes it challenging to develop models that can adapt to these changes and maintain their performance over time.\\n\\nThese differences pose significant challenges for researchers and engineers working on recommendation systems, requiring innovative approaches to effectively utilize deep learning techniques for massive datasets.'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"chunks prompt created\"\"\"\n",
    "chunk_template2=\"Provide summary of the paper {text} until now\"\n",
    "chunk_prompt2=PromptTemplate(input_variables=[\"text\"],template=chunk_template2)\n",
    "\n",
    "\"\"\"\"whole_doc prompt\"\"\"\n",
    "doc_template5=\"Provide information in bulleted list format for each of the above chunk summaries from pdf {text}\"\n",
    "prompt_doc5=PromptTemplate(input_variables=[\"text\"],template=doc_template5)\n",
    "\n",
    "Chain_doc3=load_summarize_chain(llm=llm1, chain_type=\"refine\",refine_prompt=chunk_prompt2,question_prompt=prompt_doc5,verbose=True)\n",
    "\"\"\"ValueError: extra value(s) found in load_summarize_chain: {prompt=prompt_doc3} -- therefore should be ommitted since By defualt the the funcion implies summarizing of the doc chunk provided\"\"\"\n",
    "\"\"\"Limited the no of chunks(not of dos's pages to 5 in interest of time for model to parse the summary)\"\"\"\n",
    "Chain_doc3.invoke(doc_chunks[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
